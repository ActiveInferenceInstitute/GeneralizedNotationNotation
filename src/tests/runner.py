"""
Test Runner for GNN Processing Pipeline.

This module provides comprehensive test execution capabilities for the GNN pipeline.
It implements the core test execution logic, following the thin orchestrator pattern
where 2_tests.py delegates all functionality to this module.

Architecture:
  The module provides multiple test execution modes:
  - run_tests() - Main entry point that routes to appropriate mode
  - run_fast_pipeline_tests() - Fast tests for quick pipeline validation (default)
  - run_comprehensive_tests() - All tests including slow/performance tests
  - run_fast_reliable_tests() - Essential tests fallback mode
  
  The ModularTestRunner class provides category-based execution with:
  - Resource monitoring (memory, CPU)
  - Timeout handling per category
  - Parallel execution support
  - Comprehensive error recovery

Key Features:
  - Staged test execution (fast, comprehensive, reliable)
  - Parallel test execution with resource monitoring
  - Comprehensive reporting and analytics (JSON, Markdown)
  - Graceful error handling and recovery
  - Performance regression detection
  - Memory usage tracking
  - Coverage analysis integration
  - Collection error detection (import/syntax errors)
  - Category-based test organization

Test Categories:
  Tests are organized into categories defined in MODULAR_TEST_CATEGORIES:
  - gnn, render, mcp, audio, visualization, pipeline, etc.
  Each category has its own timeout, max failures, and parallel execution settings.

Usage:
  from tests import run_tests
  from pathlib import Path
  import logging
  
  logger = logging.getLogger(__name__)
  success = run_tests(
      logger=logger,
      output_dir=Path("output/2_tests_output"),
      verbose=True,
      fast_only=True
  )

Dependencies:
  - pytest: Test framework
  - pytest-cov: Coverage analysis (optional)
  - pytest-timeout: Per-test timeouts (optional)
  - psutil: Resource monitoring (optional)
"""

import logging
import subprocess
import sys
import time
import json
# psutil is optional; tests should not fail to import if it's missing
try:
    import psutil as _psutil  # type: ignore
    PSUTIL_AVAILABLE = True
except Exception:
    _psutil = None  # type: ignore
    PSUTIL_AVAILABLE = False
import gc
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import signal
import re

# Import test categories from dedicated module
from .categories import MODULAR_TEST_CATEGORIES

# Import test utilities
from utils.test_utils import TEST_DIR, PROJECT_ROOT
from utils.pipeline_template import (
    setup_step_logging,
    log_step_start,
    log_step_success,
    log_step_error,
    log_step_warning
)

# Calculate project root (don't import from conftest as it's a pytest fixture)
project_root = Path(__file__).parent.parent.parent

@dataclass
class TestExecutionConfig:
    """Configuration for test execution."""
    timeout_seconds: int = 3600  # Increased to 60 minutes for comprehensive test suite
    max_failures: int = 10
    parallel: bool = True
    coverage: bool = True
    verbose: bool = False
    markers: List[str] = None
    memory_limit_mb: int = 2048
    cpu_limit_percent: int = 80

@dataclass
class TestExecutionResult:
    """Results from test execution."""
    success: bool
    tests_run: int
    tests_passed: int
    tests_failed: int
    tests_skipped: int
    execution_time: float
    memory_peak_mb: float
    coverage_percentage: Optional[float] = None
    error_message: Optional[str] = None
    stdout: str = ""
    stderr: str = ""

class ResourceMonitor:
    """Monitor system resources during test execution."""
    
    def __init__(self, memory_limit_mb: int = 2048, cpu_limit_percent: int = 80):
        self.memory_limit_mb = memory_limit_mb
        self.cpu_limit_percent = cpu_limit_percent
        self.peak_memory = 0.0
        self.peak_cpu = 0.0
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self):
        """Start resource monitoring in background thread."""
        if not PSUTIL_AVAILABLE:
            # No-op if psutil is not available
            self.monitoring = False
            return
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_resources)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """Stop resource monitoring."""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
            
    def _monitor_resources(self):
        """Monitor system resources in background."""
        if not PSUTIL_AVAILABLE:
            # Passive sleep loop to avoid busy spin when psutil missing
            while self.monitoring:
                try:
                    time.sleep(0.5)
                except Exception:
                    break
            return
        process = _psutil.Process()
        
        while self.monitoring:
            try:
                # Memory usage
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.peak_memory = max(self.peak_memory, memory_mb)
                
                # CPU usage
                cpu_percent = process.cpu_percent()
                self.peak_cpu = max(self.peak_cpu, cpu_percent)
                
                # Check limits
                if memory_mb > self.memory_limit_mb:
                    logging.warning(f"‚ö†Ô∏è Memory usage ({memory_mb:.1f}MB) exceeds limit ({self.memory_limit_mb}MB)")
                
                if cpu_percent > self.cpu_limit_percent:
                    logging.warning(f"‚ö†Ô∏è CPU usage ({cpu_percent:.1f}%) exceeds limit ({self.cpu_limit_percent}%)")
                    
                time.sleep(0.5)  # Monitor every 500ms
                
            except Exception as e:
                logging.warning(f"Resource monitoring error: {e}")
                break
                
    def get_stats(self) -> Dict[str, float]:
        """Get current resource statistics."""
        if not PSUTIL_AVAILABLE:
            return {
                "peak_memory_mb": self.peak_memory,
                "peak_cpu_percent": self.peak_cpu,
                "current_memory_mb": 0.0,
                "current_cpu_percent": 0.0,
            }
        return {
            "peak_memory_mb": self.peak_memory,
            "peak_cpu_percent": self.peak_cpu,
            "current_memory_mb": _psutil.Process().memory_info().rss / 1024 / 1024,
            "current_cpu_percent": _psutil.Process().cpu_percent(),
        }

class TestRunner:
    """Test runner with comprehensive monitoring and reporting."""
    
    def __init__(self, config: TestExecutionConfig):
        self.config = config
        self.logger = logging.getLogger("test_runner")
        self.resource_monitor = ResourceMonitor(
            memory_limit_mb=config.memory_limit_mb,
            cpu_limit_percent=config.cpu_limit_percent
        )
        self.execution_history: List[TestExecutionResult] = []
        
    def run_tests(self, test_paths: List[Path], output_dir: Path) -> TestExecutionResult:
        """Execute tests with comprehensive monitoring."""
        start_time = time.time()
        
        try:
            # Start resource monitoring
            self.resource_monitor.start_monitoring()
            
            # Build pytest command
            cmd = self._build_pytest_command(test_paths, output_dir)
            
            # Execute tests
            result = self._execute_pytest(cmd, output_dir)
            
            # Stop monitoring and get stats
            self.resource_monitor.stop_monitoring()
            resource_stats = self.resource_monitor.get_stats()
            
            # Create execution result
            execution_result = TestExecutionResult(
                success=result["success"],
                tests_run=result["tests_run"],
                tests_passed=result["tests_passed"],
                tests_failed=result["tests_failed"],
                tests_skipped=result["tests_skipped"],
                execution_time=time.time() - start_time,
                memory_peak_mb=resource_stats["peak_memory_mb"],
                coverage_percentage=result.get("coverage_percentage"),
                error_message=result.get("error_message"),
                stdout=result.get("stdout", ""),
                stderr=result.get("stderr", "")
            )
            
            # Store in history
            self.execution_history.append(execution_result)
            
            return execution_result
            
        except Exception as e:
            self.resource_monitor.stop_monitoring()
            return TestExecutionResult(
                success=False,
                tests_run=0,
                tests_passed=0,
                tests_failed=0,
                tests_skipped=0,
                execution_time=time.time() - start_time,
                memory_peak_mb=0.0,
                error_message=str(e)
            )
    
    def _build_pytest_command(self, test_paths: List[Path], output_dir: Path) -> List[str]:
        """Build pytest command with appropriate options."""
        cmd = [
            sys.executable, "-m", "pytest",
            "--verbose",
            "--tb=short",
            f"--maxfail={self.config.max_failures}",
            "--durations=10",
            "--disable-warnings"
        ]
        
        # Add markers
        if self.config.markers:
            for marker in self.config.markers:
                cmd.extend(["-m", marker])
        
        # Add coverage if enabled
        if self.config.coverage:
            cov_json = output_dir / "coverage.json"
            cov_html = output_dir / "htmlcov"
            cmd.extend([
                "--cov=src",
                f"--cov-report=json:{cov_json}",
                f"--cov-report=html:{cov_html}",
                "--cov-report=term-missing"
            ])
        
        # Add parallel execution if enabled (disabled for now to avoid hanging)
        # if self.config.parallel:
        #     cmd.extend(["-n", "auto"])
        
        # Add test paths
        cmd.extend([str(path) for path in test_paths])
        
        return cmd
    
    def _execute_pytest(self, cmd: List[str], output_dir: Path) -> Dict[str, Any]:
        """Execute pytest command and capture results."""
        try:
            # Debug: Log what we're trying to do
            self.logger.debug(f"Creating output directory: {output_dir}")
            self.logger.debug(f"Output dir type: {type(output_dir)}")
            
            # Create output files
            stdout_file = output_dir / "pytest_stdout.txt"
            stderr_file = output_dir / "pytest_stderr.txt"
            
            # Execute command with streaming
            from utils.execution_utils import execute_command_streaming
            
            result = execute_command_streaming(
                cmd,
                cwd=project_root,
                timeout=self.config.timeout_seconds,
                print_stdout=True,
                print_stderr=True,
                capture_output=True
            )
            
            stdout = result.get("stdout", "")
            stderr = result.get("stderr", "")
            
            # Save output to files
            with open(stdout_file, 'w') as f:
                f.write(stdout)
            with open(stderr_file, 'w') as f:
                f.write(stderr)
                
            if result["status"] == "TIMEOUT":
                 return {
                    "success": False,
                    "tests_run": 0,
                    "tests_passed": 0,
                    "tests_failed": 0,
                    "tests_skipped": 0,
                    "error_message": f"Test execution timed out after {self.config.timeout_seconds} seconds",
                    "stdout": stdout,
                    "stderr": stderr
                }
            
            # Parse results
            results = self._parse_pytest_output(stdout, stderr)
            results["stdout"] = stdout
            results["stderr"] = stderr
            
            return results
            
        except Exception as e:
            import traceback
            self.logger.error(f"Exception in _execute_pytest: {e}")
            self.logger.error(f"Full traceback:\n{traceback.format_exc()}")
            return {
                "success": False,
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 0,
                "tests_skipped": 0,
                "error_message": str(e),
                "stdout": "",
                "stderr": ""
            }
    
    def _parse_pytest_output(self, stdout: str, stderr: str) -> Dict[str, Any]:
        """Parse pytest output to extract test statistics."""
        try:
            # Extract test counts from output
            lines = stdout.split('\n')
            tests_run = 0
            tests_passed = 0
            tests_failed = 0
            tests_skipped = 0

            # Look for the summary line at the end (e.g., "60 passed, 20 skipped in 1.85s")
            for line in reversed(lines):
                if "passed" in line or "failed" in line or "skipped" in line:
                    # Try to extract numbers from the summary line
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if i > 0:  # Check previous part is a number
                            try:
                                num = int(parts[i-1])
                                if "passed" in part:
                                    tests_passed = num
                                elif "failed" in part:
                                    tests_failed = num
                                elif "skipped" in part:
                                    tests_skipped = num
                            except (ValueError, IndexError):
                                # Not a number, skip
                                continue
                    # Calculate total from summary
                    if tests_passed > 0 or tests_failed > 0:
                        tests_run = tests_passed + tests_failed + tests_skipped
                        break

            # Fallback: look for collected items line
            if tests_run == 0:
                for line in lines:
                    if "collected" in line:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if part == "collected" and i > 0:
                                try:
                                    tests_run = int(parts[i-1])
                                except ValueError:
                                    pass
                                break

            # Check for collection errors
            collection_errors = []
            for line in lines:
                if "ERROR collecting" in line or "ERROR: No tests collected" in line:
                    collection_errors.append(line)

            # Determine success - fail if no tests collected or collection errors
            success = tests_failed == 0 and tests_run > 0 and not collection_errors
            
            # Extract coverage if present
            coverage_percentage = None
            for line in lines:
                if "TOTAL" in line and "%" in line:
                    try:
                        coverage_percentage = float(line.split()[-1].replace('%', ''))
                    except (ValueError, IndexError):
                        pass
            
            return {
                "success": success,
                "tests_run": tests_run,
                "tests_passed": tests_passed,
                "tests_failed": tests_failed,
                "tests_skipped": tests_skipped,
                "coverage_percentage": coverage_percentage,
                "collection_errors": collection_errors
            }
            
        except Exception as e:
            import traceback
            self.logger.error(f"Failed to parse pytest output: {e}")
            self.logger.error(f"Traceback:\n{traceback.format_exc()}")
            return {
                "success": False,
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 0,
                "tests_skipped": 0,
                "error_message": f"Failed to parse pytest output: {e}"
            }
    
    def generate_report(self, output_dir: Path) -> Dict[str, Any]:
        """Generate comprehensive test execution report."""
        if not self.execution_history:
            return {"error": "No test execution history available"}
        
        latest_result = self.execution_history[-1]
        
        report = {
            "execution_summary": asdict(latest_result),
            "resource_usage": self.resource_monitor.get_stats(),
            "execution_history": [asdict(result) for result in self.execution_history],
            "performance_metrics": {
                "average_execution_time": sum(r.execution_time for r in self.execution_history) / len(self.execution_history),
                "peak_memory_usage": max(r.memory_peak_mb for r in self.execution_history),
                "success_rate": sum(1 for r in self.execution_history if r.success) / len(self.execution_history) * 100
            }
        }
        
        # Ensure output directory exists
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save report
        report_file = output_dir / "test_execution_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        return report

def check_test_dependencies(logger: logging.Logger) -> Dict[str, Any]:
    """
    Check if required test dependencies are available.
    
    Verifies that pytest and optional dependencies (pytest-cov, pytest-timeout)
    are installed and available.
    
    Args:
        logger: Logger instance for reporting
    
    Returns:
        Dictionary with dependency status:
        {
            'pytest': bool,
            'pytest_cov': bool,
            'pytest_timeout': bool,
            'all_required': bool
        }
    """
    dependencies = {
        "pytest": False,
        "pytest-cov": False,
        "pytest-xdist": False,
        "psutil": False,
        "coverage": False
    }
    
    try:
        import pytest
        dependencies["pytest"] = True
    except ImportError:
        pass
    
    try:
        import pytest_cov
        dependencies["pytest-cov"] = True
    except ImportError:
        pass
    
    try:
        import xdist
        dependencies["pytest-xdist"] = True
    except ImportError:
        pass
    
    try:
        import psutil
        dependencies["psutil"] = True
    except ImportError:
        pass

    try:
        import coverage
        dependencies["coverage"] = True
    except ImportError:
        pass
    
    # Log results
    missing_deps = [name for name, available in dependencies.items() if not available]
    if missing_deps:
        logger.warning(f"‚ö†Ô∏è Missing test dependencies: {missing_deps}")
    else:
        logger.info("‚úÖ All test dependencies available")
            
    return dependencies

def build_pytest_command(
    test_markers: List[str] = None,
    timeout_seconds: int = 600,
    max_failures: int = 20,
    parallel: bool = True,
    verbose: bool = False,
    generate_coverage: bool = True,
    fast_only: bool = False,
    include_slow: bool = False
) -> List[str]:
    """
    Build pytest command with appropriate options.
    
    Constructs a pytest command line with all necessary flags and options
    based on the provided parameters. Handles test filtering, timeout,
    coverage, and execution mode settings.
    
    Args:
        test_markers: List of pytest markers to include (e.g., ['fast', 'unit'])
        timeout_seconds: Maximum execution time per test (default: 600)
        max_failures: Maximum number of test failures before stopping (default: 20)
        parallel: Enable parallel test execution (default: True)
        verbose: Enable verbose output (default: False)
        generate_coverage: Generate coverage reports (default: True)
        fast_only: Run only fast tests, exclude slow tests (default: False)
        include_slow: Include slow tests (default: False)
    
    Returns:
        List of command arguments for subprocess.run()
    
    Example:
        cmd = build_pytest_command(
            test_markers=['fast'],
            timeout_seconds=120,
            max_failures=5,
            verbose=True,
            fast_only=True
        )
        # Returns: ['python', '-m', 'pytest', '--verbose', '--tb=short', ...]
    """
    cmd = [
        sys.executable, "-m", "pytest",
        "--verbose" if verbose else "--quiet",
        "--tb=short",
        f"--maxfail={max_failures}",
        "--durations=10",
        "--disable-warnings"
    ]
    
    # Add markers
    if test_markers:
        for marker in test_markers:
            cmd.extend(["-m", marker])
    
    # Add coverage if enabled
    if generate_coverage:
        cmd.extend([
            "--cov=src",
            "--cov-report=json",
            "--cov-report=html",
            "--cov-report=term-missing"
        ])
    
    # Add parallel execution if enabled
    if parallel:
        cmd.extend(["-n", "auto"])
    
    # Add test path
    cmd.append(str(TEST_DIR))
    
    return cmd

def run_tests(
    logger: logging.Logger,
    output_dir: Path,
    verbose: bool = False,
    include_slow: bool = False,
    fast_only: bool = True,  # Default to fast tests for pipeline integration
    comprehensive: bool = False,
    generate_coverage: bool = False  # Disable coverage by default for speed
) -> bool:
    """
    Run optimized test suite with improved performance and reliability.

    Args:
        logger: Logger instance
        output_dir: Output directory for test results
        verbose: Enable verbose output
        include_slow: Include slow tests (deprecated, use comprehensive)
        fast_only: Run only fast tests
        comprehensive: Run comprehensive test suite (all tests)
        generate_coverage: Generate coverage report

    Returns:
        True if tests pass, False otherwise
    """
    try:
        log_step_start(logger, "Running optimized test suite")

        # Check dependencies
        dependencies = check_test_dependencies(logger)
        if not all(dependencies.values()):
            log_step_warning(logger, "Some test dependencies missing - functionality may be limited")

        # For pipeline integration, run a focused subset of tests
        if fast_only and not comprehensive:
            logger.info("üèÉ Running fast pipeline test subset for quick validation")
            return run_fast_pipeline_tests(logger, output_dir, verbose)

        # For comprehensive mode, run all tests but with better timeout handling
        if comprehensive:
            logger.info("üî¨ Running comprehensive test suite with enhanced monitoring")
            return run_comprehensive_tests(logger, output_dir, verbose, generate_coverage)

        # Default to fast tests with improved reliability
        logger.info("‚ö° Running fast test suite with reliability improvements")
        return run_fast_reliable_tests(logger, output_dir, verbose)

    except Exception as e:
        log_step_error(logger, f"Test execution failed: {e}")
        return False

def run_fast_pipeline_tests(logger: logging.Logger, output_dir: Path, verbose: bool = False) -> bool:
    """
    Run FAST tests for quick pipeline validation.
    
    This runs only fast tests (marked with 'not slow') to keep pipeline execution efficient.
    
    Environment variables:
    - SKIP_TESTS_IN_PIPELINE: Set to any value to skip tests entirely
    - FAST_TESTS_TIMEOUT: Override the 600s timeout (in seconds)
    
    This function can be controlled via:
    - os.getenv("SKIP_TESTS_IN_PIPELINE") - Skip all tests
    - os.getenv("FAST_TESTS_TIMEOUT") - Custom timeout
    """
    import subprocess
    import sys
    import os

    # Check if tests should be skipped entirely for pipeline speed
    if os.getenv("SKIP_TESTS_IN_PIPELINE"):
        logger.info("‚è≠Ô∏è  Skipping tests (SKIP_TESTS_IN_PIPELINE set)")
        logger.info("üí° Set SKIP_TESTS_IN_PIPELINE='' or unset to run tests in pipeline")
        return True

    logger.info("‚ö° Running fast test subset for quick pipeline validation")
    logger.info("üí° To skip tests in pipeline: export SKIP_TESTS_IN_PIPELINE=1")
    logger.info("üí° To customize timeout: export FAST_TESTS_TIMEOUT=<seconds>")

    # Check if pytest-timeout is available
    try:
        import pytest_timeout
        has_timeout = True
    except ImportError:
        has_timeout = False

    # Build pytest command for FAST tests only
    cmd = [
        sys.executable, "-m", "pytest",
        "--tb=short",
        "--maxfail=5",  # Fail fast for quick feedback
        "--durations=10",  # Show slowest tests
        "-ra",  # Show summary of test outcomes (all)
    ]
    
    # Add timeout flags only if pytest-timeout is available
    if has_timeout:
        cmd.extend([
            "--timeout=30",  # 30-second per-test timeout (increased from 2s to handle subprocess/network tests)
            "--timeout-method=thread",  # Use thread-based timeout
        ])
    
    # Always use verbose mode (-v) for test-by-test logging visibility
    cmd.extend([
        "-m", "not slow",  # Skip slow tests
        "-v",  # Always verbose for test-by-test logging
    ])

    # Add the entire test directory to run ALL tests
    test_dir = Path(__file__).parent
    cmd.append(str(test_dir))

    logger.info(f"üöÄ Executing fast test suite: {' '.join(cmd)}")
    logger.info("üí° Skipping slow tests (-m 'not slow')")
    if has_timeout:
        logger.info("‚è±Ô∏è  Per-test timeout: 30s (pytest-timeout enabled)")
    else:
        logger.info("‚ö†Ô∏è  pytest-timeout not available - no per-test timeout enforcement")

    try:
        # Get timeout from environment or use default
        timeout_seconds = int(os.getenv("FAST_TESTS_TIMEOUT", "600"))
        
        logger.info(f"‚è±Ô∏è  Total timeout: {timeout_seconds} seconds ({timeout_seconds // 60}m)")
        
        # Run with strict timeout for pipeline integration and streaming
        from utils.execution_utils import execute_command_streaming
        
        result_dict = execute_command_streaming(
            cmd,
            cwd=Path(__file__).parent.parent.parent,
            timeout=timeout_seconds,
            print_stdout=True,
            print_stderr=True,
            capture_output=True
        )

        # Parse results
        stdout = result_dict.get("stdout", "")
        stderr = result_dict.get("stderr", "")

        # Save output with improved logging
        output_dir.mkdir(parents=True, exist_ok=True)
        with open(output_dir / "pytest_comprehensive_output.txt", "w") as f:
            f.write(f"STDOUT:\n{stdout}\n\nSTDERR:\n{stderr}")
        
        logger.info(f"üìÅ Test output saved to: {output_dir / 'pytest_comprehensive_output.txt'}")

        # Check for collection errors first (before parsing stats)
        collection_errors = _extract_collection_errors(stdout, stderr)
        if collection_errors:
            logger.error("‚ùå Test collection failed with errors:")
            for error in collection_errors:
                logger.error(f"  ‚Ä¢ {error}")
            logger.error("üí° Fix collection errors before tests can run")
            logger.error("üí° Common issues: missing imports, syntax errors, circular dependencies")
            return False

        # Parse test statistics from output
        test_stats = _parse_test_statistics(stdout)

        # Generate JSON report for orchestrator compatibility and fallback logic
        report = {
            "execution_summary": {
                "success": test_stats.get('tests_failed', 0) == 0 and test_stats.get('tests_run', 0) > 0,
                "tests_run": test_stats.get('tests_run', 0),
                "tests_passed": test_stats.get('tests_passed', 0),
                "tests_failed": test_stats.get('tests_failed', 0),
                "tests_skipped": test_stats.get('tests_skipped', 0),
                "execution_time": 0.0,  # Not tracked in fast mode
                "error_message": "No tests executed" if test_stats.get('tests_run', 0) == 0 else None
            }
        }
        try:
            with open(output_dir / "test_execution_report.json", "w") as f:
                json.dump(report, f, indent=2)
        except Exception as e:
            logger.warning(f"Failed to write test report JSON: {e}")

        # Log comprehensive results
        logger.info("üìä Test Results Summary:")
        logger.info(f"  üìà Tests run: {test_stats.get('tests_run', 0)}")
        logger.info(f"  ‚úÖ Passed: {test_stats.get('tests_passed', 0)}")
        logger.info(f"  ‚ùå Failed: {test_stats.get('tests_failed', 0)}")
        logger.info(f"  ‚è≠Ô∏è Skipped: {test_stats.get('tests_skipped', 0)}")

        # Determine success - we need to check if tests actually ran
        tests_run = test_stats.get('tests_run', 0)
        tests_failed = test_stats.get('tests_failed', 0)
        tests_passed = test_stats.get('tests_passed', 0)
        
        # Success if: tests ran, zero failures, or minimal failures (< 5% failure rate) with significant pass rate
        if tests_run == 0:
            # No tests were collected or all were skipped
            logger.error("‚ùå No tests were executed - all tests were skipped or not collected")
            logger.error("üí° Check that test files exist and follow pytest naming conventions (test_*.py)")
            logger.error("üí° Ensure test functions are named with 'test_' prefix")
            return False
        elif tests_failed == 0:
            logger.info(f"‚úÖ Complete test suite passed ({tests_passed}/{tests_run} tests passed)")
            return True
        elif tests_passed > 0 and (tests_failed / tests_run) < 0.05:
            # Allow up to 5% failure rate
            pass_rate = (tests_passed / tests_run) * 100
            logger.warning(f"‚ö†Ô∏è Test suite passed with minor failures ({tests_passed} passed, {tests_failed} failed, {pass_rate:.1f}% pass rate)")
            return True
        else:
            logger.error(f"‚ùå Test suite failed ({tests_passed} passed, {tests_failed} failed out of {tests_run} total)")
            return False

    except subprocess.TimeoutExpired:
        logger.error(f"‚è∞ Complete test execution timed out after {timeout_seconds} seconds ({timeout_seconds // 60}m)")
        logger.error("üí° To increase timeout, set FAST_TESTS_TIMEOUT environment variable")
        logger.error("üí° Example: export FAST_TESTS_TIMEOUT=900  # 15 minutes")
        logger.error("üí° Or skip tests in pipeline: export SKIP_TESTS_IN_PIPELINE=1")
        _generate_timeout_report(output_dir, cmd, timeout_seconds)
        return False
    except Exception as e:
        logger.error(f"‚ùå Complete test execution failed: {e}")
        import traceback
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        return False

def run_comprehensive_tests(logger: logging.Logger, output_dir: Path, verbose: bool = False, generate_coverage: bool = False) -> bool:
    """
    Run comprehensive test suite with all tests enabled.
    
    This function executes all test categories from MODULAR_TEST_CATEGORIES, including
    slow tests and performance tests. It uses ModularTestRunner for category-based
    execution with resource monitoring.
    
    Args:
        logger: Logger instance for progress reporting
        output_dir: Output directory for test results and reports
        verbose: Enable verbose output (default: False)
        generate_coverage: Generate coverage reports (default: False)
    
    Returns:
        True if tests passed, False otherwise
    
    Features:
        - Executes all test categories sequentially
        - Includes slow and performance tests
        - Generates comprehensive coverage reports if enabled
        - Resource monitoring per category
        - Comprehensive error recovery
    
    Example:
        from tests.runner import run_comprehensive_tests
        from pathlib import Path
        import logging
        
        logger = logging.getLogger(__name__)
        success = run_comprehensive_tests(
            logger=logger,
            output_dir=Path("output/2_tests_output"),
            verbose=True,
            generate_coverage=True
        )
    """
    logger.info("üî¨ Running comprehensive test suite with enhanced monitoring")

    # Use the ModularTestRunner for comprehensive testing
    # Create a simple args object for the runner
    class SimpleArgs:
        def __init__(self):
            self.output_dir = str(output_dir)
            self.comprehensive = True
            self.fast_only = False
            self.include_slow = True
            self.include_performance = False

    args = SimpleArgs()
    runner = ModularTestRunner(args, logger)

    # Run all categories but with better timeout handling
    results = runner.run_all_categories()

    success = results.get("overall_success", False)
    if success:
        logger.info("‚úÖ Comprehensive tests completed successfully")
    else:
        logger.warning("‚ö†Ô∏è Comprehensive tests had some failures")

    return success

def run_fast_reliable_tests(logger: logging.Logger, output_dir: Path, verbose: bool = False) -> bool:
    """
    Run a reliable subset of fast tests with improved error handling.
    
    This function runs only essential test files that should always pass:
    - test_core_modules.py
    - test_fast_suite.py
    - test_main_orchestrator.py
    
    Used as a fallback when fast pipeline tests are not suitable, or for
    quick validation of core functionality.
    
    Args:
        logger: Logger instance for progress reporting
        output_dir: Output directory for test results
        verbose: Enable verbose output (default: False)
    
    Returns:
        True if tests passed, False otherwise
    
    Features:
        - 90-second timeout for reliability
        - Focuses on essential tests only
        - Improved error handling
        - Fast execution (< 2 minutes typically)
    
    Example:
        from tests.runner import run_fast_reliable_tests
        from pathlib import Path
        import logging
        
        logger = logging.getLogger(__name__)
        success = run_fast_reliable_tests(
            logger=logger,
            output_dir=Path("output/2_tests_output"),
            verbose=True
        )
    """
    import subprocess
    import sys

    logger.info("‚ö° Running reliable fast test subset")

    # Focus on essential tests that should always pass
    reliable_tests = [
        "test_core_modules.py",
        "test_fast_suite.py",
        "test_main_orchestrator.py"
    ]

    # Build pytest command
    cmd = [
        sys.executable, "-m", "pytest",
        "--tb=short",
        "--maxfail=3",  # Stop after 3 failures
        "--durations=3",  # Show only top 3 slowest tests
        "-v" if verbose else "-q"
    ]

    # Add test files
    test_dir = Path(__file__).parent
    for test_file in reliable_tests:
        test_path = test_dir / test_file
        if test_path.exists():
            cmd.append(str(test_path))

    logger.info(f"üöÄ Executing reliable tests: {' '.join(cmd)}")

    try:
        # Run with timeout
        result = subprocess.run(
            cmd,
            cwd=Path(__file__).parent.parent.parent,
            capture_output=True,
            text=True,
            timeout=90  # 90 second timeout for reliability
        )

        # Save output
        output_dir.mkdir(parents=True, exist_ok=True)
        with open(output_dir / "pytest_reliable_output.txt", "w") as f:
            f.write(f"STDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}")

        success = result.returncode == 0
        if success:
            logger.info("‚úÖ Reliable fast tests completed successfully")
        else:
            logger.warning("‚ö†Ô∏è Reliable fast tests had some failures")

        return success

    except subprocess.TimeoutExpired:
        logger.error("‚è∞ Reliable test execution timed out")
        return False
    except Exception as e:
        logger.error(f"‚ùå Reliable test execution failed: {e}")
        return False

def _extract_collection_errors(stdout: str, stderr: str) -> List[str]:
    """
    Extract and parse collection errors from pytest output.
    
    Detects import errors, syntax errors, and other collection failures that
    prevent tests from being collected. Returns a list of unique error messages
    with actionable information.
    
    Args:
        stdout: Standard output from pytest execution
        stderr: Standard error from pytest execution
    
    Returns:
        List of unique error messages (strings)
    
    Error Types Detected:
        - ERROR collecting: Test file collection failures
        - NameError: Missing variable/import names
        - ImportError: Module import failures
        - SyntaxError: Code syntax issues
    
    Example:
        errors = _extract_collection_errors(pytest_stdout, pytest_stderr)
        # Returns: ["test_file.py: ImportError: No module named 'missing_module'"]
    """
    errors = []
    combined_output = stdout + "\n" + stderr
    
    # Look for ERROR collecting patterns
    import re
    error_patterns = [
        r'ERROR collecting ([^\n]+)\n([^\n]+: [^\n]+)',
        r'NameError: name \'([^\']+)\' is not defined',
        r'ImportError: ([^\n]+)',
        r'SyntaxError: ([^\n]+)',
    ]
    
    for pattern in error_patterns:
        matches = re.finditer(pattern, combined_output, re.MULTILINE)
        for match in matches:
            error_msg = match.group(0)
            # Extract the key part of the error
            if 'ERROR collecting' in error_msg:
                # Extract the file and error message
                lines = error_msg.split('\n')
                if len(lines) >= 2:
                    file_line = lines[0].replace('ERROR collecting ', '').strip()
                    error_line = lines[1].strip()
                    errors.append(f"{file_line}: {error_line}")
            elif 'NameError' in error_msg:
                var_name = match.group(1) if match.groups() else 'unknown'
                errors.append(f"NameError: '{var_name}' is not defined (missing import?)")
            elif 'ImportError' in error_msg:
                import_name = match.group(1) if match.groups() else 'unknown'
                errors.append(f"ImportError: {import_name}")
            elif 'SyntaxError' in error_msg:
                syntax_error = match.group(1) if match.groups() else 'unknown'
                errors.append(f"SyntaxError: {syntax_error}")
    
    # Also check for "ERRORS" section
    if "ERRORS" in combined_output or "ERROR collecting" in combined_output:
        # Extract all unique error messages
        error_section = re.search(r'=+\s+ERRORS\s+=+(.*?)(?=\n=+|\Z)', combined_output, re.DOTALL)
        if error_section:
            error_text = error_section.group(1)
            # Extract individual error blocks
            error_blocks = re.findall(r'ERROR collecting ([^\n]+)\n([^\n]+: [^\n]+)', error_text)
            for file_path, error_msg in error_blocks:
                errors.append(f"{file_path}: {error_msg}")
    
    # Remove duplicates while preserving order
    seen = set()
    unique_errors = []
    for error in errors:
        if error not in seen:
            seen.add(error)
            unique_errors.append(error)
    
    return unique_errors

def _parse_test_statistics(pytest_output: str) -> Dict[str, int]:
    """Parse pytest output to extract test statistics."""
    stats = {
        "tests_run": 0,
        "tests_passed": 0,
        "tests_failed": 0,
        "tests_skipped": 0
    }

    try:
        lines = pytest_output.split('\n')
        import re

        # Look for the summary line at the end (e.g., "534 passed, 12 skipped in 3.45s")
        for line in reversed(lines):
            line = line.strip()
            # Check if this line contains test results (has passed/failed/skipped + " in ")
            if (" passed" in line or " failed" in line or " skipped" in line) and " in " in line:
                # Parse patterns like: "534 passed, 12 skipped in 3.45s"
                # or "22 passed, 5 failed, 3 skipped in 1.23s"
                
                # Extract passed count
                passed_match = re.search(r'(\d+)\s+passed', line)
                if passed_match:
                    stats["tests_passed"] = int(passed_match.group(1))
                    stats["tests_run"] += int(passed_match.group(1))
                
                # Extract failed count
                failed_match = re.search(r'(\d+)\s+failed', line)
                if failed_match:
                    stats["tests_failed"] = int(failed_match.group(1))
                    stats["tests_run"] += int(failed_match.group(1))
                
                # Extract skipped count
                skipped_match = re.search(r'(\d+)\s+skipped', line)
                if skipped_match:
                    stats["tests_skipped"] = int(skipped_match.group(1))
                    stats["tests_run"] += int(skipped_match.group(1))
                
                # If we found any stats, break
                if stats["tests_run"] > 0:
                    break

        # Fallback: count individual test results if summary line not found
        # Look for lines like "test_foo.py::test_bar PASSED [  1%]"
        if stats["tests_passed"] == 0 and stats["tests_failed"] == 0:
            for line in lines:
                line_stripped = line.strip()
                # Match pytest verbose output format: "test_file.py::TestClass::test_method PASSED"
                if " PASSED" in line_stripped:
                    stats["tests_passed"] += 1
                elif " FAILED" in line_stripped:
                    stats["tests_failed"] += 1
                elif " SKIPPED" in line_stripped:
                    stats["tests_skipped"] += 1
            
            # Update tests_run from counted results
            if stats["tests_passed"] > 0 or stats["tests_failed"] > 0 or stats["tests_skipped"] > 0:
                stats["tests_run"] = stats["tests_passed"] + stats["tests_failed"] + stats["tests_skipped"]

        # Also look for collected items line if no results found yet
        if stats["tests_run"] == 0:
            for line in lines:
                if "collected" in line and ("item" in line or "test" in line):
                    # Extract number before "collected"
                    match = re.search(r'(\d+)\s+(?:item|test)', line)
                    if match:
                        stats["tests_run"] = int(match.group(1))
                        break

    except Exception as e:
        logging.warning(f"Failed to parse test statistics: {e}")

    return stats

def _parse_coverage_statistics(coverage_json_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """Parse coverage JSON file to extract coverage statistics."""
    try:
        if not coverage_json_path.exists():
            return {"error": "Coverage file not found"}
        
        with open(coverage_json_path) as f:
            coverage_data = json.load(f)
        
        # Extract overall coverage
        total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0.0)
        
        # Extract per-file coverage
        files_coverage = {}
        for file_path, file_data in coverage_data.get("files", {}).items():
            files_coverage[file_path] = file_data.get("summary", {}).get("percent_covered", 0.0)
        
        return {
            "total_coverage": total_coverage,
            "files_coverage": files_coverage,
            "files_count": len(files_coverage)
        }
        
    except Exception as e:
        logger.warning(f"Failed to parse coverage statistics: {e}")
        return {"error": str(e)}

def _generate_markdown_report(report_path: Path, summary: Dict[str, Any]):
    """Generate markdown test report."""
    try:
        with open(report_path, 'w') as f:
            f.write("# Test Execution Report\n\n")
            
            # Summary section
            f.write("## Summary\n\n")
            f.write(f"- **Total Tests**: {summary.get('total_tests_run', 0)}\n")
            f.write(f"- **Passed**: {summary.get('total_tests_passed', 0)}\n")
            f.write(f"- **Failed**: {summary.get('total_tests_failed', 0)}\n")
            f.write(f"- **Skipped**: {summary.get('total_tests_skipped', 0)}\n")
            f.write(f"- **Success Rate**: {summary.get('success_rate', 0):.1f}%\n")
            f.write(f"- **Execution Time**: {summary.get('total_execution_time', 0):.2f}s\n\n")
        
            # Stage results
            if 'stage_results' in summary:
                f.write("## Stage Results\n\n")
                for stage_name, stage_data in summary['stage_results'].items():
                    f.write(f"### {stage_name.title()}\n")
                    f.write(f"- **Status**: {'‚úÖ Passed' if stage_data.get('success') else '‚ùå Failed'}\n")
                    f.write(f"- **Duration**: {stage_data.get('duration_seconds', 0):.2f}s\n")
                    f.write(f"- **Tests**: {stage_data.get('tests_passed', 0)}/{stage_data.get('tests_run', 0)} passed\n\n")
        
            # Performance metrics
            if 'performance_metrics' in summary:
                f.write("## Performance Metrics\n\n")
                perf = summary['performance_metrics']
                f.write(f"- **Peak Memory**: {perf.get('peak_memory_mb', 0):.1f}MB\n")
                f.write(f"- **Average Execution Time**: {perf.get('average_execution_time', 0):.2f}s\n")
                f.write(f"- **Success Rate**: {perf.get('success_rate', 0):.1f}%\n\n")
        
        logging.info(f"‚úÖ Markdown report generated: {report_path}")
        
    except Exception as e:
        logging.warning(f"Failed to generate markdown report: {e}")

def _generate_fallback_report(output_dir: Path, summary: Dict[str, Any]):
    """Generate fallback report when main report generation fails."""
    try:
        fallback_file = output_dir / "test_report_fallback.txt"
        with open(fallback_file, 'w') as f:
            f.write("Test Execution Summary\n")
            f.write("=====================\n\n")
            f.write(f"Total Tests: {summary.get('total_tests_run', 0)}\n")
            f.write(f"Passed: {summary.get('total_tests_passed', 0)}\n")
            f.write(f"Failed: {summary.get('total_tests_failed', 0)}\n")
            f.write(f"Skipped: {summary.get('total_tests_skipped', 0)}\n")
            f.write(f"Execution Time: {summary.get('total_execution_time', 0):.2f}s\n")
        
        logging.info(f"‚úÖ Fallback report generated: {fallback_file}")
        
    except Exception as e:
        logging.warning(f"Failed to generate fallback report: {e}")

def _generate_timeout_report(output_dir: Path, cmd: List[str], timeout: int):
    """Generate report for timeout scenarios."""
    try:
        timeout_file = output_dir / "test_timeout_report.txt"
        with open(timeout_file, 'w') as f:
            f.write("Test Execution Timeout\n")
            f.write("=====================\n\n")
            f.write(f"Timeout: {timeout} seconds\n")
            f.write(f"Command: {' '.join(cmd)}\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        logging.warning(f"‚ö†Ô∏è Timeout report generated: {timeout_file}")
        
    except Exception as e:
        logging.warning(f"Failed to generate timeout report: {e}")

def _generate_error_report(output_dir: Path, cmd: List[str], error_msg: str):
    """Generate report for error scenarios."""
    try:
        error_file = output_dir / "test_error_report.txt"
        with open(error_file, 'w') as f:
            f.write("Test Execution Error\n")
            f.write("===================\n\n")
            f.write(f"Error: {error_msg}\n")
            f.write(f"Command: {' '.join(cmd)}\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        logging.info(f"‚ùå Error report generated: {error_file}")
        
    except Exception as e:
        logging.warning(f"Failed to generate error report: {e}")

class ModularTestRunner:
    """Test runner with comprehensive error handling and reporting."""
    
    def __init__(self, args, logger: logging.Logger):
        self.args = args
        self.logger = logger
        self.project_root = Path(__file__).parent.parent.parent
        self.test_dir = self.project_root / "src" / "tests"
        self.results = {}
        self.start_time = time.time()
        
        # Error tracking
        self.total_tests_run = 0
        self.total_tests_passed = 0
        self.total_tests_failed = 0
        self.total_tests_skipped = 0
        self.categories_run = 0
        self.categories_successful = 0
        self.categories_failed = 0
        
        # Performance tracking
        self.category_times = {}
        self.slowest_tests = []
        
        # Resource monitoring
        self.resource_usage = {}
        
        # Error categorization
        self.import_errors = []
        self.runtime_errors = []
        self.pathlib_errors = []
        self.sapf_errors = []
        
        self.logger.info(f"Initialized ModularTestRunner with {len(MODULAR_TEST_CATEGORIES)} categories")

    def _monitor_resources_during_test(self):
        """Monitor system resources during test execution."""
        try:
            import psutil
            process = psutil.Process()
            return {
                "memory_mb": process.memory_info().rss / 1024 / 1024,
                "cpu_percent": process.cpu_percent(),
                "threads": process.num_threads()
            }
        except ImportError:
            return {"memory_mb": 0, "cpu_percent": 0, "threads": 0}
            
    def should_run_category(self, category: str) -> bool:
        """Determine if a test category should be run based on arguments."""
        # Always run comprehensive category when requested
        if hasattr(self.args, 'comprehensive') and self.args.comprehensive:
            return True
            
        if hasattr(self.args, 'fast_only') and self.args.fast_only:
            # In fast mode, run core, pipeline, validation, utilities, and fast_suite
            # This gives better coverage while still being fast
            return category in ["core", "pipeline", "validation", "utilities", "fast_suite"]
        
        # Skip performance tests if not explicitly included
        if category == "performance" and hasattr(self.args, 'include_performance') and not self.args.include_performance:
            return False
            
        # Skip slow tests if not explicitly included
        if category in ["specialized", "integration"] and hasattr(self.args, 'include_slow') and not self.args.include_slow:
            return False
        
        # By default, run all categories except performance and slow tests
        return category not in ["performance", "specialized", "integration"]

    def _run_fallback_tests(self, category: str, test_files: List[str], python_executable: str, start_time: float) -> Dict[str, Any]:
        """Fallback test execution when pytest fails with internal errors."""
        self.logger.info(f"Running fallback test execution for category '{category}'")
        
        # Simple test discovery and execution without pytest
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for test_file in test_files:
            try:
                # Run basic Python syntax check
                syntax_result = subprocess.run(
                    [python_executable, "-m", "py_compile", test_file],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                if syntax_result.returncode == 0:
                    total_tests += 1
                    passed_tests += 1
                    self.logger.info(f"  ‚úÖ {Path(test_file).name}: Syntax valid")
                else:
                    total_tests += 1
                    failed_tests += 1
                    self.logger.warning(f"  ‚ùå {Path(test_file).name}: Syntax error")
                    
            except Exception as e:
                total_tests += 1
                failed_tests += 1
                self.logger.warning(f"  ‚ùå {Path(test_file).name}: Error {e}")
        
        duration = time.time() - start_time
        self.category_times[category] = duration
        
        # Update global counters
        self.total_tests_run += total_tests
        self.total_tests_passed += passed_tests
        self.total_tests_failed += failed_tests
        
        return {
            "success": failed_tests == 0,
            "tests_run": total_tests,
            "tests_passed": passed_tests,
            "tests_failed": failed_tests,
            "tests_skipped": 0,
            "duration": duration,
            "stdout": f"Fallback execution: {passed_tests}/{total_tests} files passed syntax check",
            "stderr": "",
            "returncode": 1 if failed_tests > 0 else 0,
            "resource_usage": {"memory_mb": 0, "cpu_percent": 0, "threads": 0}
        }

    def discover_test_files(self, category: str, config: Dict[str, Any]) -> List[str]:
        """Discover test files for a category with enhanced error handling."""
        test_files = []
        
        for file_pattern in config.get("files", []):
            # Look for exact file matches first
            exact_file = self.test_dir / file_pattern
            if exact_file.exists():
                test_files.append(str(exact_file))
                continue
            
            # Look for pattern matches
            pattern_files = list(self.test_dir.glob(file_pattern))
            test_files.extend([str(f) for f in pattern_files])
        
        # Remove duplicates and sort
        test_files = sorted(list(set(test_files)))
        
        self.logger.info(f"Discovered {len(test_files)} test files for category '{category}': {test_files}")
        return test_files
        
    def _parse_pytest_output(self, stdout: str, stderr: str) -> Dict[str, int]:
        """Parse pytest output to extract test counts."""
        tests_run = 0
        tests_passed = 0
        tests_failed = 0
        tests_skipped = 0

        # Parse the output for test results
        lines = stdout.split('\n')

        # Look for the summary line at the end (most reliable)
        # Format: "========================= 41 passed, 12 skipped in 1.76s ==========================="
        for line in reversed(lines):
            line = line.strip()
            if "passed" in line and ("failed" in line or "skipped" in line or "error" in line):
                # Extract numbers from patterns like "X passed, Y failed" or "X passed, Y skipped"
                import re
                # Match patterns like "41 passed, 12 skipped" or "41 passed, 12 failed"
                match = re.search(r'(\d+)\s+passed.*?(\d+)\s+(failed|skipped|error)', line)
                if match:
                    tests_passed = int(match.group(1))
                    count = int(match.group(2))
                    status = match.group(3)

                    if status == "failed" or status == "error":
                        tests_failed = count
                    elif status == "skipped":
                        tests_skipped = count

                    tests_run = tests_passed + tests_failed + tests_skipped
                    break

        # If no summary found, try to count from individual test results
        if tests_run == 0:
            # Count PASSED, FAILED, SKIPPED lines
            passed_count = stdout.count(" PASSED")
            failed_count = stdout.count(" FAILED") + stdout.count(" ERROR")
            skipped_count = stdout.count(" SKIPPED")

            if passed_count > 0 or failed_count > 0 or skipped_count > 0:
                tests_passed = passed_count
                tests_failed = failed_count
                tests_skipped = skipped_count
                tests_run = passed_count + failed_count + skipped_count

        # If still no tests found, try to parse from the collected line
        if tests_run == 0:
            for line in lines:
                if "collected" in line and "items" in line:
                    # Example: "collected 52 items"
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if part == "collected" and i+1 < len(parts):
                            try:
                                tests_run = int(parts[i+1])
                                # If we found collected tests but no summary, assume they all passed
                                tests_passed = tests_run
                                break
                            except (ValueError, IndexError):
                                pass

        # Return with both canonical and legacy key names used elsewhere in the runner
        return {
            "tests_run": tests_run,
            "tests_passed": tests_passed,
            "tests_failed": tests_failed,
            "tests_skipped": tests_skipped,
            # Legacy/shorthand keys referenced by callers
            "total": tests_run,
            "passed": tests_passed,
            "failed": tests_failed,
            "skipped": tests_skipped
        }
                        
    def run_test_category(self, category: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Run tests for a specific category with comprehensive error handling and enhanced logging."""
        category_start_time = time.time()
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"üß™ Running category: {config['name']}")
        self.logger.info(f"üìù Description: {config['description']}")
        effective_timeout_seconds = int(config.get('timeout_seconds', 120))
        self.logger.info(f"‚è±Ô∏è Category timeout budget: {effective_timeout_seconds} seconds")
        self.logger.info(f"üîÑ Parallel: {config.get('parallel', True)}")
        self.logger.info(f"{'='*60}")
        
        # Discover test files
        test_files = self.discover_test_files(category, config)
        
        if not test_files:
            self.logger.info(f"‚è≠Ô∏è No test files found for category '{category}', marking as skipped/success")
            return {
                "success": True,
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 0,
                "tests_skipped": 0,
                "duration": 0,
                "stdout": "",
                "stderr": "",
                "return_code": 0
            }
        
        self.logger.info(f"üìÅ Found {len(test_files)} test files:")
        for test_file in test_files:
            self.logger.info(f"  üî∏ {test_file}")
        
        # Use virtual environment Python if available
        venv_python = self.project_root / ".venv" / "bin" / "python"
        python_executable = str(venv_python) if venv_python.exists() else sys.executable
        
        self.logger.info(f"üêç Using Python: {python_executable}")
        
        # Detect optional plugins
        has_xdist = False
        has_pytest_cov = False
        has_jsonreport = False
        try:
            import xdist  # type: ignore
            has_xdist = True
        except Exception:
            has_xdist = False
        try:
            import pytest_cov  # type: ignore
            has_pytest_cov = True
        except Exception:
            has_pytest_cov = False
        try:
            import pytest_jsonreport  # type: ignore
            has_jsonreport = True
        except Exception:
            has_jsonreport = False
        try:
            import pytest_timeout  # type: ignore
            has_pytest_timeout = True
        except Exception:
            has_pytest_timeout = False

        # Ensure per-category output directory exists (for coverage artifacts)
        category_output_dir = Path(self.args.output_dir) / "test_reports" / f"category_{category}"
        category_output_dir.mkdir(parents=True, exist_ok=True)

        # Build pytest command with options and plugin isolation
        # Always use direct python executable to avoid UV dependency resolution issues
        cmd = [
            python_executable, "-m", "pytest",
            "--tb=short",  # Shorter traceback
            "--maxfail=10",  # Stop after 10 failures
            "--durations=10",  # Show 10 slowest tests
            "--no-header",  # Skip pytest header
            "-p", "no:randomly",  # Disable pytest-randomly plugin
            "-p", "no:sugar",    # Disable pytest-sugar plugin
            "-p", "no:cacheprovider",  # Disable cache to avoid corruption
        ]
        
        # Load and enable optional plugins explicitly since autoload is disabled
        # Ensure timeout plugin is enabled with 10s per-test limit (only if available)
        if has_pytest_timeout:
            cmd.extend(["-p", "pytest_timeout", "--timeout=10", "--timeout-method=thread"])  
            self.logger.info("‚è≥ Per-test timeout enforced (10s)")
        else:
            self.logger.info("‚ö†Ô∏è  pytest-timeout not available - no per-test timeout enforcement")

        # Explicitly enable asyncio plugin for async tests (do not gate on parent import)
        cmd.extend(["-p", "pytest_asyncio"])  
        self.logger.info("‚úÖ pytest-asyncio requested")
        if has_jsonreport:
            cmd.extend(["-p", "pytest_jsonreport", "--json-report", "--json-report-file=none"])
            self.logger.info("üìä JSON reporting enabled")
        else:
            self.logger.info("JSON reporting plugin not available")

        if has_pytest_cov:
            cmd.extend(["-p", "pytest_cov", "--cov=src",
                        f"--cov-report=term-missing",
                        f"--cov-report=json:{category_output_dir}/coverage.json",
                        f"--cov-report=html:{category_output_dir}/htmlcov"])        
        
        # Add markers if specified
        if config.get("markers"):
            for marker in config["markers"]:
                cmd.extend(["-m", marker])
            self.logger.info(f"üè∑Ô∏è Using markers: {config['markers']}")
        
        # Add test files
        cmd.extend(test_files)
        
        # Add parallel execution if enabled and xdist is available (disable for pipeline tests to avoid timeout)
        if (config.get("parallel", True)
            and not (hasattr(self.args, 'fast_only') and self.args.fast_only)
            and category != "pipeline"
            and has_xdist):
            cmd.extend(["-p", "xdist", "-n", "auto"])
            self.logger.info("‚ö° Parallel execution enabled (xdist)")
        else:
            if config.get("parallel", True) and not has_xdist:
                self.logger.info("Parallel requested but xdist not available; running sequentially")
            self.logger.info("üîÑ Sequential execution (parallel disabled)")
        
        self.logger.info(f"üöÄ Executing command: {' '.join(cmd[:5])}...")
        
        try:
            # Monitor resources before test execution
            initial_resources = self._monitor_resources_during_test()
            self.logger.info(f"üíæ Initial resource usage: {initial_resources}")
            
            # Avoid global SIGALRM; rely on per-test timeouts and inactivity-based aborts
            old_handler = None
            
            try:
                # Set environment variables to avoid dependency conflicts and promote live streaming
                env = os.environ.copy()
                env["PYTHONPATH"] = str(self.project_root / "src")
                env["PYTEST_DISABLE_PLUGIN_AUTOLOAD"] = "1"  # Disable automatic plugin loading
                # Encourage live, detailed progress output and disable color to avoid termcap issues
                desired_addopts = ["-vv", "-rA", "-s", "--durations=15", "-o", "console_output_style=classic", "--color=no"]
                existing_addopts = env.get("PYTEST_ADDOPTS", "").split()
                for opt in desired_addopts:
                    if opt not in existing_addopts:
                        existing_addopts.append(opt)
                env["PYTEST_ADDOPTS"] = " ".join(existing_addopts).strip()
                env.setdefault("PYTHONUNBUFFERED", "1")
                # Force no-color and dumb terminal to avoid termcap/terminfo warnings in headless runs
                env.setdefault("TERM", "dumb")
                env.setdefault("NO_COLOR", "1")
                env.setdefault("PY_COLORS", "0")
                # Set default terminal dimensions to placate some TTY-dependent libs
                env.setdefault("COLUMNS", "120")
                env.setdefault("LINES", "40")
                # Disable Ollama provider in tests unless explicitly enabled to avoid CLI timeouts
                env.setdefault("OLLAMA_DISABLED", "1")

                self.logger.info(f"‚è±Ô∏è Starting test execution at {time.strftime('%H:%M:%S')}")

                # Stream output live and tee to files
                category_output_dir = Path(self.args.output_dir) / "test_reports" / f"category_{category}"
                stdout_path = category_output_dir / "pytest_stdout.txt"
                stderr_path = category_output_dir / "pytest_stderr.txt"

                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    bufsize=1,
                    cwd=self.project_root,
                    env=env
                )

                collected_stdout: List[str] = []
                collected_stderr: List[str] = []
                progress_counts = {"passed": 0, "failed": 0, "skipped": 0}
                last_output_time = time.time()
                collected_seen: bool = False

                def _log_progress_line(line: str):
                    nonlocal progress_counts
                    lower = line.lower()
                    # Update quick counters on common keywords
                    if "passed" in lower:
                        progress_counts["passed"] += lower.count("passed")
                    if "failed" in lower or "error" in lower:
                        progress_counts["failed"] += lower.count("failed") + lower.count("error")
                    if "skipped" in lower:
                        progress_counts["skipped"] += lower.count("skipped")
                    if ("collected" in lower) and ("items" in lower):
                        collected_seen = True

                f_out = open(stdout_path, "w")
                f_err = open(stderr_path, "w")
                try:
                    def _stream(pipe, sink, is_err: bool):
                        nonlocal last_output_time
                        try:
                            for raw in iter(pipe.readline, ""):
                                line = raw.rstrip("\n")
                                try:
                                    sink.write(raw)
                                    sink.flush()
                                except Exception:
                                    # Sink might be closing; stop streaming
                                    break
                                if is_err:
                                    # Classify stderr lines to avoid error-level logs for non-errors
                                    lower_line = line.lower()
                                    if (
                                        "traceback" in lower_line
                                        or lower_line.startswith("e   ")
                                        or "= fail" in lower_line
                                        or "failed" in lower_line
                                        or re.search(r"\b(error|exception)\b", lower_line)
                                    ):
                                        self.logger.error(line)
                                    elif "warn" in lower_line:
                                        self.logger.warning(line)
                                    else:
                                        self.logger.info(line)
                                    collected_stderr.append(line)
                                else:
                                    self.logger.info(line)
                                    collected_stdout.append(line)
                                    _log_progress_line(line)
                                last_output_time = time.time()
                        finally:
                            try:
                                pipe.close()
                            except Exception:
                                pass

                    t_out = threading.Thread(target=_stream, args=(process.stdout, f_out, False))
                    t_err = threading.Thread(target=_stream, args=(process.stderr, f_err, True))
                    t_out.start(); t_err.start()

                    # Emit heartbeat if quiet too long; abort only on true inactivity
                    collection_stall_limit = 25
                    while process.poll() is None:
                        if time.time() - last_output_time > 10:
                            elapsed = time.time() - category_start_time
                            self.logger.info(
                                f"‚Ä¶ pytest running [{category}] ‚Äî elapsed {int(elapsed)}s; "
                                f"progress: {progress_counts['passed']} passed, {progress_counts['failed']} failed, {progress_counts['skipped']} skipped"
                            )
                            last_output_time = time.time()
                        # Abort only if no output whatsoever for stall_limit seconds
                        if time.time() - last_output_time > collection_stall_limit:
                            self.logger.warning(f"‚õî Aborting category '{category}' due to inactivity > {collection_stall_limit}s")
                            try:
                                process.kill()
                            except Exception:
                                pass
                            break
                        time.sleep(2)

                    # ensure threads finish completely before closing sinks
                    t_out.join()
                    t_err.join()
                finally:
                    try:
                        f_out.close()
                    except Exception:
                        pass
                    try:
                        f_err.close()
                    except Exception:
                        pass

                # Restore any previous handler if one was set
                if old_handler is not None:
                    try:
                        signal.signal(signal.SIGALRM, old_handler)
                    except Exception:
                        pass

                execution_time = time.time() - category_start_time
                self.logger.info(f"‚è±Ô∏è Test execution completed in {execution_time:.2f} seconds")

                # Monitor resources after test execution
                final_resources = self._monitor_resources_during_test()
                self.logger.info(f"üíæ Final resource usage: {final_resources}")

                # Store resource usage (absolute values only)
                resource_usage = {
                    "memory_mb": final_resources.get("memory_mb", 0),
                    "cpu_percent": final_resources.get("cpu_percent", 0),
                    "threads": final_resources.get("threads", 0)
                }
                self.resource_usage[category] = resource_usage

                # Join collected output
                stdout_text = "\n".join(collected_stdout)
                stderr_text = "\n".join(collected_stderr)

                # Handle pytest internal errors with fallback
                if process.returncode == 3 and ("INTERNALERROR" in stdout_text or "INTERNALERROR" in stderr_text):
                    self.logger.warning(f"‚ö†Ô∏è Pytest internal error detected for category '{category}', attempting fallback...")
                    return self._run_fallback_tests(category, test_files, python_executable, category_start_time)

                # Parse test results
                test_stats = self._parse_pytest_output(stdout_text, stderr_text)

                # Log detailed results
                self.logger.info(f"üìä Test Results for {category}:")
                self.logger.info(f"  ‚úÖ Passed: {test_stats.get('passed', 0)}")
                self.logger.info(f"  ‚ùå Failed: {test_stats.get('failed', 0)}")
                self.logger.info(f"  ‚è≠Ô∏è Skipped: {test_stats.get('skipped', 0)}")
                self.logger.info(f"  üìà Total: {test_stats.get('total', 0)}")

                # Log slowest tests if available
                if "slowest" in stdout_text.lower():
                    slowest_lines = [line for line in stdout_text.split('\n') if "slowest" in line.lower()]
                    if slowest_lines:
                        self.logger.info("üêå Slowest tests:")
                        for line in slowest_lines[:3]:  # Show top 3
                            self.logger.info(f"  {line.strip()}")

                # Determine success based on test results
                total_tests = test_stats.get('total', 0)
                failed_tests = test_stats.get('failed', 0)
                passed_tests = test_stats.get('passed', 0)

                # Strict: category succeeds only if there are zero failures when any tests ran
                success = (total_tests > 0 and failed_tests == 0)

                if success:
                    self.logger.info(f"‚úÖ Category '{category}' completed successfully")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Category '{category}' completed with failures")

                return {
                    "success": success,
                    "tests_run": total_tests,
                    "tests_passed": passed_tests,
                    "tests_failed": failed_tests,
                    "tests_skipped": test_stats.get('skipped', 0),
                    "duration": execution_time,
                    "resource_usage": resource_usage,
                    "stdout": stdout_text,
                    "stderr": stderr_text,
                    "return_code": process.returncode
                }

            except subprocess.TimeoutExpired:
                if old_handler is not None:
                    try:
                        signal.signal(signal.SIGALRM, old_handler)
                    except Exception:
                        pass
                self.logger.error(f"‚è∞ Test category '{category}' reached subprocess timeout")
                return {
                    "success": False,
                    "error": "Subprocess timeout",
                    "tests_run": 0,
                    "tests_passed": 0,
                    "tests_failed": 0,
                    "tests_skipped": 0,
                    "duration": time.time() - category_start_time
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error in category '{category}': {e}")
            return {
                "success": False,
                "error": str(e),
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 0,
                "tests_skipped": 0,
                "duration": time.time() - category_start_time
            }
            
    def run_all_categories(self) -> Dict[str, Any]:
        """Run all test categories and generate comprehensive report with enhanced logging."""
        self.logger.info(f"\n{'='*80}")
        self.logger.info("üöÄ STARTING COMPREHENSIVE TEST SUITE EXECUTION")
        self.logger.info(f"{'='*80}")
        
        # Calculate total categories to run
        total_categories = len([cat for cat in MODULAR_TEST_CATEGORIES.keys() if self.should_run_category(cat)])
        self.logger.info(f"üìä Total categories to run: {total_categories}")
        
        overall_success = True
        start_time = time.time()
        
        for i, (category, config) in enumerate(MODULAR_TEST_CATEGORIES.items(), 1):
            if not self.should_run_category(category):
                self.logger.info(f"‚è≠Ô∏è Skipping category '{category}' based on arguments")
                continue
            
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"üìã Category {i}/{total_categories}: {category}")
            self.logger.info(f"üìù {config['name']}")
            self.logger.info(f"{'='*60}")
            
            self.categories_run += 1
            
            try:
                result = self.run_test_category(category, config)
                self.results[category] = result
                
                # Update global counters
                self.total_tests_run += result.get("tests_run", 0)
                self.total_tests_passed += result.get("tests_passed", 0)
                self.total_tests_failed += result.get("tests_failed", 0)
                self.total_tests_skipped += result.get("tests_skipped", 0)
                
                if result.get("success", False):
                    self.categories_successful += 1
                    self.logger.info(f"‚úÖ Category '{category}' completed successfully")
                else:
                    self.categories_failed += 1
                    overall_success = False
                    self.logger.warning(f"‚ö†Ô∏è Category '{category}' completed with issues")
                
                # Log category summary
                duration = result.get("duration", 0)
                tests_run = result.get("tests_run", 0)
                tests_passed = result.get("tests_passed", 0)
                tests_failed = result.get("tests_failed", 0)
                
                self.logger.info(f"üìä Category '{category}' Summary:")
                self.logger.info(f"  ‚è±Ô∏è Duration: {duration:.2f}s")
                self.logger.info(f"  üìà Tests: {tests_run} total, {tests_passed} passed, {tests_failed} failed")
                
                if tests_run > 0:
                    success_rate = (tests_passed / tests_run) * 100
                    self.logger.info(f"  üìä Success Rate: {success_rate:.1f}%")
                
                # Log resource usage if available
                resource_usage = result.get("resource_usage", {})
                if resource_usage:
                    memory_mb = resource_usage.get("memory_mb", 0)
                    cpu_percent = resource_usage.get("cpu_percent", 0)
                    self.logger.info(f"  üíæ Memory: {memory_mb:.1f}MB, CPU: {cpu_percent:.1f}%")
                    
            except Exception as e:
                self.logger.error(f"‚ùå Unexpected error in category '{category}': {e}")
                self.results[category] = {
                    "success": False,
                    "error": str(e),
                    "tests_run": 0,
                    "tests_passed": 0,
                    "tests_failed": 0,
                    "tests_skipped": 0,
                    "duration": 0
                }
                self.categories_failed += 1
                overall_success = False
        
        # Calculate overall statistics
        total_duration = time.time() - start_time
        total_categories_run = self.categories_run
        total_categories_successful = self.categories_successful
        total_categories_failed = self.categories_failed
        
        # Log overall summary
        self.logger.info(f"\n{'='*80}")
        self.logger.info("üìä COMPREHENSIVE TEST SUITE SUMMARY")
        self.logger.info(f"{'='*80}")
        self.logger.info(f"‚è±Ô∏è Total Duration: {total_duration:.2f}s")
        self.logger.info(f"üìã Categories: {total_categories_run} run, {total_categories_successful} successful, {total_categories_failed} failed")
        self.logger.info(f"üß™ Tests: {self.total_tests_run} total, {self.total_tests_passed} passed, {self.total_tests_failed} failed, {self.total_tests_skipped} skipped")
        
        if self.total_tests_run > 0:
            overall_success_rate = (self.total_tests_passed / self.total_tests_run) * 100
            self.logger.info(f"üìä Overall Success Rate: {overall_success_rate:.1f}%")
        
        if total_categories_run > 0:
            category_success_rate = (total_categories_successful / total_categories_run) * 100
            self.logger.info(f"üìä Category Success Rate: {category_success_rate:.1f}%")
        
        # Generate comprehensive report
        self.logger.info(f"\nüíæ Generating comprehensive test reports...")
        self._save_intermediate_results()
        self._generate_final_report()
        self._log_final_summary(overall_success)
        
        # Return comprehensive results
        return {
            "overall_success": overall_success,
            "total_tests_run": self.total_tests_run,
            "total_tests_passed": self.total_tests_passed,
            "total_tests_failed": self.total_tests_failed,
            "total_tests_skipped": self.total_tests_skipped,
            "categories_run": self.categories_run,
            "categories_successful": self.categories_successful,
            "categories_failed": self.categories_failed,
            "total_duration": total_duration,
            "success_rate": (self.total_tests_passed / self.total_tests_run * 100) if self.total_tests_run > 0 else 0
        }
    
    def _save_intermediate_results(self):
        """Save intermediate results to JSON files."""
        output_dir = Path(self.args.output_dir) / "test_reports"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save detailed results for each category
        for category, result in self.results.items():
            category_dir = output_dir / f"category_{category}"
            category_dir.mkdir(exist_ok=True)
            
            # Save stdout and stderr
            if "stdout" in result:
                with open(category_dir / "pytest_stdout.txt", "w") as f:
                    f.write(result["stdout"])
            
            if "stderr" in result:
                with open(category_dir / "pytest_stderr.txt", "w") as f:
                    f.write(result["stderr"])
            
            # Save command info
            with open(category_dir / "pytest_command.txt", "w") as f:
                f.write(f"Category: {category}\n")
                f.write(f"Success: {result.get('success', False)}\n")
                f.write(f"Duration: {result.get('duration', 0):.2f}s\n")
                if "error" in result:
                    f.write(f"Error: {result['error']}\n")
                
                # Add resource usage if available
                if "resource_usage" in result:
                    resource_data = result["resource_usage"]
                    f.write(f"Peak Memory: {resource_data.get('memory_mb', 0):.2f} MB\n")
                    f.write(f"CPU Usage: {resource_data.get('cpu_percent', 0):.1f}%\n")
    
    def _generate_final_report(self):
        """Generate comprehensive final test report."""
        output_dir = Path(self.args.output_dir) / "test_reports"
        
        # Calculate success rate
        success_rate = (self.total_tests_passed / max(self.total_tests_run, 1)) * 100
        
        # Generate summary
        summary = {
            "total_categories": self.categories_run,
            "successful_categories": self.categories_successful,
            "failed_categories": self.categories_failed,
            "total_tests_run": self.total_tests_run,
            "total_tests_passed": self.total_tests_passed,
            "total_tests_failed": self.total_tests_failed,
            "total_tests_skipped": self.total_tests_skipped,
            "success_rate": success_rate,
            "total_duration": time.time() - self.start_time,
            "category_times": self.category_times,
            "resource_usage": self.resource_usage,
            "error_categories": {
                "pathlib_errors": self.pathlib_errors,
                "sapf_errors": self.sapf_errors,
                "import_errors": self.import_errors,
                "runtime_errors": self.runtime_errors
            }
        }
        
        # Save summary
        with open(output_dir / "modular_test_summary.json", "w") as f:
            json.dump(summary, f, indent=2)
        
        # Save detailed results
        with open(output_dir / "modular_test_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
    
    def _log_final_summary(self, success: bool):
        """Log comprehensive final summary."""
        self.logger.info(f"\n{'='*80}")
        self.logger.info("COMPREHENSIVE TEST SUITE EXECUTION COMPLETE")
        self.logger.info(f"{'='*80}")
        
        # Overall statistics
        self.logger.info(f"Overall Result: {'‚úÖ SUCCESS' if success else '‚ùå FAILURE'}")
        self.logger.info(f"Total Duration: {time.time() - self.start_time:.2f}s")
        self.logger.info(f"Categories Run: {self.categories_run}")
        self.logger.info(f"Categories Successful: {self.categories_successful}")
        self.logger.info(f"Categories Failed: {self.categories_failed}")
        
        # Test statistics
        self.logger.info(f"Total Tests Run: {self.total_tests_run}")
        self.logger.info(f"Total Tests Passed: {self.total_tests_passed}")
        self.logger.info(f"Total Tests Failed: {self.total_tests_failed}")
        self.logger.info(f"Total Tests Skipped: {self.total_tests_skipped}")
        
        if self.total_tests_run > 0:
            success_rate = (self.total_tests_passed / self.total_tests_run) * 100
            self.logger.info(f"Success Rate: {success_rate:.1f}%")
        
        # Error analysis
        if self.pathlib_errors:
            self.logger.warning(f"Pathlib errors detected in categories: {self.pathlib_errors}")
        
        if self.sapf_errors:
            self.logger.warning(f"SAPF errors detected in categories: {self.sapf_errors}")
        
        if self.import_errors:
            self.logger.warning(f"Import errors detected in categories: {self.import_errors}")
        
        # Performance analysis
        if self.category_times:
            slowest_category = max(self.category_times.items(), key=lambda x: x[1])
            self.logger.info(f"Slowest category: {slowest_category[0]} ({slowest_category[1]:.2f}s)")
        
        # Resource usage summary
        if self.resource_usage:
            peak_memory = max([usage.get("memory_mb", 0) for usage in self.resource_usage.values()])
            self.logger.info(f"Peak Memory Usage: {peak_memory:.2f} MB")
        
        self.logger.info(f"{'='*80}")

def create_test_runner(args, logger: logging.Logger) -> ModularTestRunner:
    """Factory function to create a ModularTestRunner instance."""
    return ModularTestRunner(args, logger)

def monitor_memory(logger: logging.Logger, threshold_mb: int = 2000):
    """Monitor memory usage and log warnings if threshold exceeded."""
    try:
        import psutil as _ps  # local import to avoid top-level dependency
        process = _ps.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        if memory_mb > threshold_mb:
            logger.warning(f"‚ö†Ô∏è High memory usage: {memory_mb:.1f}MB (threshold: {threshold_mb}MB)")
            
            # Suggest garbage collection
            gc.collect()
            
            # Check again after GC
            memory_info = process.memory_info()
            memory_mb_after = memory_info.rss / 1024 / 1024
            
            if memory_mb_after < memory_mb:
                logger.info(f"‚úÖ Memory usage reduced to {memory_mb_after:.1f}MB after garbage collection")
            else:
                logger.warning(f"‚ö†Ô∏è Memory usage still high: {memory_mb_after:.1f}MB")
        
        return memory_mb
        
    except Exception as e:
        logger.warning(f"Failed to monitor memory: {e}")
        return 0.0