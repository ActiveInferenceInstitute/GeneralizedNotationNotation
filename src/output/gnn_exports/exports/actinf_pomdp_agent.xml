<?xml version="1.0" encoding="UTF-8"?>
<gnn>
  <gnnsection>
    <line>ActInfPOMDP</line>
  </gnnsection>
  <gnnversionandflags>
    <line>GNN v1</line>
  </gnnversionandflags>
  <modelname>
    <line>Classic Active Inference POMDP Agent v1</line>
  </modelname>
  <modelannotation>
    <line>This model describes a classic Active Inference agent for a discrete POMDP:</line>
    <line>- One observation modality ("state_observation") with 3 possible outcomes.</line>
    <line>- One hidden state factor ("location") with 3 possible states.</line>
    <line>- The hidden state is fully controllable via 3 discrete actions.</line>
    <line>- The agent's preferences are encoded as log-probabilities over observations.</line>
    <line>- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.</line>
  </modelannotation>
  <statespaceblock>
    <line># Likelihood matrix: A[observation_outcomes, hidden_states]</line>
    <line>A[3,3,type=float]   # Likelihood mapping hidden states to observations</line>
    <line># Transition matrix: B[states_next, states_previous, actions]</line>
    <line>B[3,3,3,type=float]   # State transitions given previous state and action</line>
    <line># Preference vector: C[observation_outcomes]</line>
    <line>C[3,type=float]       # Log-preferences over observations</line>
    <line># Prior vector: D[states]</line>
    <line>D[3,type=float]       # Prior over initial hidden states</line>
    <line># Habit vector: E[actions]</line>
    <line>E[3,type=float]       # Initial policy prior (habit) over actions</line>
    <line># Hidden State</line>
    <line>s[3,1,type=float]     # Current hidden state distribution</line>
    <line>s_prime[3,1,type=float] # Next hidden state distribution</line>
    <line># Observation</line>
    <line>o[3,1,type=int]     # Current observation (integer index)</line>
    <line># Policy and Control</line>
    <line>π[3,type=float]       # Policy (distribution over actions), no planning</line>
    <line>u[1,type=int]         # Action taken</line>
    <line>G[π,type=float]       # Expected Free Energy (per policy)</line>
    <line># Time</line>
    <line>t[1,type=int]         # Discrete time step</line>
  </statespaceblock>
  <connections>
    <line>D>s</line>
    <line>s-A</line>
    <line>s>s_prime</line>
    <line>A-o</line>
    <line>s-B</line>
    <line>C>G</line>
    <line>E>π</line>
    <line>G>π</line>
    <line>π>u</line>
    <line>B>u</line>
    <line>u>s_prime</line>
  </connections>
  <initialparameterization>
    <line># A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.</line>
    <line>A={</line>
    <line>(0.9, 0.05, 0.05),</line>
    <line>(0.05, 0.9, 0.05),</line>
    <line>(0.05, 0.05, 0.9)</line>
    <line>}</line>
    <line># B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.</line>
    <line>B={</line>
    <line>( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),</line>
    <line>( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),</line>
    <line>( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )</line>
    <line>}</line>
    <line># C: 3 observations. Preference in terms of log-probabilities over observations.</line>
    <line>C={(0.1, 0.1, 1.0)}</line>
    <line># D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.</line>
    <line>D={(0.33333, 0.33333, 0.33333)}</line>
    <line># E: 3 actions. Uniform habit used as initial policy prior.</line>
    <line>E={(0.33333, 0.33333, 0.33333)}</line>
  </initialparameterization>
  <equations>
    <line># Standard Active Inference update equations for POMDPs:</line>
    <line># - State inference using Variational Free Energy with infer_states()</line>
    <line># - Policy inference using Expected Free Energy = with infer_policies()</line>
    <line># - Action selection from policy posterior: action = sample_action()</line>
  </equations>
  <time>
    <line>Time=t</line>
    <line>Dynamic</line>
    <line>Discrete</line>
    <line>ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.</line>
  </time>
  <actinfontologyannotation>
    <line>A=LikelihoodMatrix</line>
    <line>B=TransitionMatrix</line>
    <line>C=LogPreferenceVector</line>
    <line>D=PriorOverHiddenStates</line>
    <line>E=Habit</line>
    <line>F=VariationalFreeEnergy</line>
    <line>G=ExpectedFreeEnergy</line>
    <line>s=HiddenState</line>
    <line>s_prime=NextHiddenState</line>
    <line>o=Observation</line>
    <line>π=PolicyVector # Distribution over actions</line>
    <line>u=Action       # Chosen action</line>
    <line>t=Time</line>
  </actinfontologyannotation>
  <modelparameters>
    <line>num_hidden_states: 3  # s[3]</line>
    <line>num_obs: 3           # o[3]</line>
    <line>num_actions: 3       # B actions_dim=3 (controlled by π)</line>
  </modelparameters>
  <footer>
    <line>Active Inference POMDP Agent v1 - GNN Representation.</line>
    <line>Currently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting.</line>
  </footer>
  <signature>
    <line>Cryptographic signature goes here</line>
  </signature>
</gnn>