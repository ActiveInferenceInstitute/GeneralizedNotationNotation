#!/usr/bin/env python3
"""
Execute Processor module for GNN Processing Pipeline.

This module provides execute processing capabilities for rendered implementations.
"""

from pathlib import Path
from typing import Dict, Any, List
import logging
import subprocess
import json
import os
import stat
from datetime import datetime

# Import logging helpers with fallback
try:
    from utils.pipeline_template import (
        log_step_start,
        log_step_success,
        log_step_error,
        log_step_warning
    )
except Exception:
    def log_step_start(logger, msg): logger.info(f"ðŸš€ {msg}")
    def log_step_success(logger, msg): logger.info(f"âœ… {msg}")
    def log_step_error(logger, msg): logger.error(f"âŒ {msg}")
    def log_step_warning(logger, msg): logger.warning(f"âš ï¸ {msg}")

logger = logging.getLogger(__name__)


def determine_script_framework(script_path: Path, render_output_dir: Path, framework_dirs: Dict[str, str]) -> str:
    """
    Determine the framework for a script based on its directory path.

    Args:
        script_path: Path to the script
        render_output_dir: Base render output directory
        framework_dirs: Mapping of directory names to framework names

    Returns:
        Framework name or 'unknown'
    """
    try:
        # Get relative path from render output directory
        relative_path = script_path.relative_to(render_output_dir)

        # Check each part of the path for framework indicators
        for part in relative_path.parts:
            # Check if this part matches a known framework directory
            if part.lower() in framework_dirs:
                return framework_dirs[part.lower()]

            # Check for framework names in the directory name
            for framework_name in framework_dirs.values():
                if framework_name.lower() in part.lower():
                    return framework_name

        # Default fallback
        return "unknown"

    except Exception:
        return "unknown"


def parse_frameworks_parameter(frameworks: str, logger) -> List[str]:
    """
    Parse the frameworks parameter into a list of framework names.

    Args:
        frameworks: Comma-separated string of framework names or preset
        logger: Logger instance

    Returns:
        List of framework names to include
    """
    if not frameworks or frameworks.lower() == "all":
        return ["pymdp", "jax", "discopy", "rxinfer", "activeinference_jl"]

    if frameworks.lower() == "lite":
        return ["pymdp", "jax", "discopy"]

    # Parse comma-separated list
    framework_list = [f.strip() for f in frameworks.split(",")]
    valid_frameworks = ["pymdp", "jax", "discopy", "rxinfer", "activeinference_jl"]

    # Filter out invalid frameworks
    valid_list = [f for f in framework_list if f in valid_frameworks]

    if len(valid_list) != len(framework_list):
        invalid = [f for f in framework_list if f not in valid_frameworks]
        logger.warning(f"Invalid frameworks specified: {invalid}. Valid options: {valid_frameworks}")

    return valid_list if valid_list else ["pymdp"]  # Default to pymdp if nothing valid

def process_execute(
    target_dir: Path,
    output_dir: Path,
    verbose: bool = False,
    frameworks: str = "all",
    **kwargs
) -> bool:
    """
    Execute rendered implementations from 11_render_output directory.
    
    This function searches for executable scripts generated by 11_render.py
    and executes them using subprocess, capturing their outputs and results.
    
    Args:
        target_dir: Directory containing rendered executable scripts (typically 11_render_output)
        output_dir: Directory to save execution results
        verbose: Enable verbose output
        **kwargs: Additional arguments
        
    Returns:
        True if processing successful, False otherwise
    """
    logger = logging.getLogger("execute")
    
    try:
        log_step_start(logger, "Processing execute - searching for rendered implementations")

        # Parse frameworks parameter
        requested_frameworks = parse_frameworks_parameter(frameworks, logger)
        logger.info(f"Requested frameworks: {requested_frameworks}")

        # Create results directory
        results_dir = output_dir / "execution_results"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize execution results
        execution_results = {
            "timestamp": datetime.now().isoformat(),
            "target_directory": str(target_dir),
            "output_directory": str(output_dir),
            "total_scripts_found": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "skipped_executions": 0,
            "execution_details": [],
            "framework_status": {},
            "success": True
        }
        
        # Look for rendered implementations from render output
        render_output_dir = None
        
        # Priority 1: Check if --render-output-dir was specified
        if kwargs.get('render_output_dir'):
            render_output_dir = Path(kwargs['render_output_dir'])
            
        # Priority 2: Check if target_dir is already a render output directory
        elif "11_render_output" in str(target_dir) or target_dir.name == "11_render_output":
            render_output_dir = target_dir
            
        # Priority 3: Search for render output directories in common locations
        else:
            potential_dirs = [
                # Standard pipeline location
                target_dir.parent / "output" / "11_render_output",
                target_dir / "11_render_output",
                
                # Recent test locations
                Path("output/test_render/11_render_output/11_render_output"),
                Path("output/test_render_improved/11_render_output/11_render_output"),
                
                # Search in all output directories
                *list(Path("output").glob("*/11_render_output/11_render_output")),
                *list(Path("output").glob("**/11_render_output")),
            ]
            
            for potential_dir in potential_dirs:
                if potential_dir.exists() and any(potential_dir.rglob("*")):
                    render_output_dir = potential_dir
                    logger.info(f"Found render output directory: {render_output_dir}")
                    break
        
        if verbose:
            logger.info(f"Searching for executable scripts in: {render_output_dir}")

        if not render_output_dir or not render_output_dir.exists():
            log_step_warning(logger, f"Render output directory not found: {render_output_dir}")
            execution_results["success"] = True  # Not an error, just no files to execute
            execution_results["message"] = "No rendered implementations found"
        else:
            # Find executable scripts, filtered by requested frameworks
            executable_scripts = find_executable_scripts(render_output_dir, verbose, logger, requested_frameworks)
            execution_results["total_scripts_found"] = len(executable_scripts)
            execution_results["requested_frameworks"] = requested_frameworks
            
            if not executable_scripts:
                log_step_warning(logger, "No executable scripts found in render output")
                execution_results["message"] = "No executable scripts found"
                execution_results["success"] = True
            else:
                logger.info(f"Found {len(executable_scripts)} executable scripts to run")
                
                # Execute each script
                for script_info in executable_scripts:
                    exec_result = execute_single_script(script_info, results_dir, verbose, logger)
                    execution_results["execution_details"].append(exec_result)
                    
                    # Update framework status
                    framework = exec_result.get("framework", "unknown")
                    if framework not in execution_results["framework_status"]:
                        execution_results["framework_status"][framework] = {"status": "unknown", "executions": 0}

                    execution_results["framework_status"][framework]["executions"] += 1

                    if exec_result["success"]:
                        execution_results["successful_executions"] += 1
                        execution_results["framework_status"][framework]["status"] = "success"
                    else:
                        execution_results["failed_executions"] += 1
                        execution_results["framework_status"][framework]["status"] = "failed"
                        if "error" in exec_result:
                            execution_results["framework_status"][framework]["error"] = exec_result["error"]
        
        # Save detailed results
        results_file = results_dir / "execution_summary.json"
        with open(results_file, 'w') as f:
            json.dump(execution_results, f, indent=2, default=str)
            
        # Generate execution report
        generate_execution_report(execution_results, results_dir, logger)
        
        # Determine overall success with improved logic
        total_scripts = execution_results["total_scripts_found"]
        failed_scripts = execution_results["failed_executions"]

        if total_scripts == 0:
            log_step_warning(logger, "No executable scripts found to run")
            return True
        elif failed_scripts == 0:
            log_step_success(logger, "Execute processing completed successfully")
        elif failed_scripts < total_scripts * 0.5:  # Less than 50% failures
            log_step_warning(logger, f"Execute processing completed with {failed_scripts}/{total_scripts} failures (partial success)")
        else:  # 50% or more failures
            log_step_error(logger, f"Execute processing completed with {failed_scripts}/{total_scripts} failures (critical failures)")

        # Return True if we found and attempted to run scripts (even if some failed)
        # Return False only if there was a critical error preventing execution
        return True
        
    except Exception as e:
        log_step_error(logger, f"Execute processing failed: {e}")
        return False

def find_executable_scripts(render_output_dir: Path, verbose: bool, logger, requested_frameworks: List[str]) -> List[Dict[str, Any]]:
    """
    Find executable scripts in the render output directory.
    
    Args:
        render_output_dir: Directory containing rendered scripts
        verbose: Enable verbose logging
        logger: Logger instance
        
    Returns:
        List of dictionaries with script information
    """
    executable_scripts = []
    
    # Define supported script types and their executors
    script_types = {
        '*.py': {'executor': 'python3', 'framework': 'python'},
        '*.jl': {'executor': 'julia', 'framework': 'julia'},
    }

    # Map framework directories to framework names
    framework_dirs = {
        'pymdp': 'pymdp',
        'jax': 'jax',
        'discopy': 'discopy',
        'rxinfer': 'rxinfer',
        'activeinference_jl': 'activeinference_jl',
        'activeinference.jl': 'activeinference_jl',
    }

    for pattern, config in script_types.items():
        scripts = list(render_output_dir.rglob(pattern))

        for script_path in scripts:
            # Skip test files and other non-executable scripts
            if any(skip in script_path.name.lower() for skip in ['test_', '__', 'readme']):
                continue

            # Determine framework from directory path
            framework = determine_script_framework(script_path, render_output_dir, framework_dirs)

            # Filter by requested frameworks
            if framework not in requested_frameworks:
                if verbose:
                    logger.debug(f"Skipping {framework} script: {script_path.name} (not in requested frameworks)")
                continue

            # Check if script is executable or can be made executable
            script_info = {
                'path': script_path,
                'name': script_path.name,
                'framework': framework,
                'executor': config['executor'],
                'relative_path': script_path.relative_to(render_output_dir),
                'size_bytes': script_path.stat().st_size if script_path.exists() else 0
            }
            
            executable_scripts.append(script_info)
            
            if verbose:
                logger.info(f"Found {config['framework']} script: {script_info['relative_path']}")
    
    return executable_scripts


def execute_single_script(script_info: Dict[str, Any], results_dir: Path, verbose: bool, logger) -> Dict[str, Any]:
    """
    Execute a single script using subprocess.
    
    Args:
        script_info: Dictionary containing script information
        results_dir: Directory to save execution results (will create implementation-specific subfolders)
        verbose: Enable verbose logging
        logger: Logger instance
        
    Returns:
        Dictionary with execution results
    """
    script_path = script_info['path']
    executor = script_info['executor']
    
    # Extract model name and framework from script path for organization
    # Expected path: .../11_render_output/model_name/framework/script.ext
    path_parts = script_path.parts
    if len(path_parts) >= 3:
        model_name = path_parts[-3]  # e.g., 'actinf_pomdp_agent' 
        framework = path_parts[-2]   # e.g., 'pymdp'
    else:
        model_name = 'unknown_model'
        framework = script_info['framework']
    
    # Prepare execution result
    exec_result = {
        'script_path': str(script_path),
        'script_name': script_info['name'],
        'framework': framework,
        'model_name': model_name,
        'executor': executor,
        'success': False,
        'return_code': None,
        'stdout': '',
        'stderr': '',
        'execution_time': 0,
        'timestamp': datetime.now().isoformat()
    }
    
    try:
        if verbose:
            logger.info(f"Executing {script_info['framework']} script: {script_info['name']}")
        
        start_time = datetime.now()
        
        # Check if the executor is available
        try:
            # For Python scripts, check if Python is available (most are Python scripts)
            if executor in ['python', 'python3']:
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
            # For Julia scripts, check if julia is available
            elif executor == 'julia':
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
            # For other executors, try a basic check
            else:
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
            exec_result['error'] = f"Executor '{executor}' is not available or not working: {e}"
            logger.warning(f"Executor '{executor}' is not available - skipping {script_info['name']}")
            return exec_result
        
        # Execute the script with improved error handling
        script_name = script_path.name
        result = None
        try:
            result = subprocess.run(
                [executor, script_name],
                capture_output=True,
                text=True,
                timeout=60,  # Reduced timeout for better responsiveness
                cwd=script_path.parent  # Run in the script's directory
            )

            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['return_code'] = result.returncode
            exec_result['stdout'] = result.stdout
            exec_result['stderr'] = result.stderr

            if result.returncode == 0:
                exec_result['success'] = True
                logger.info(f"âœ… Successfully executed {script_info['name']}")
                if verbose and result.stdout:
                    logger.info(f"Script output: {result.stdout[:200]}...")  # Show first 200 chars
            else:
                exec_result['error'] = f"Script failed with return code {result.returncode}"
                logger.warning(f"âš ï¸ Script {script_info['name']} failed with return code {result.returncode}")
                if result.stderr:
                    logger.warning(f"Error output: {result.stderr[:200]}...")  # Show first 200 chars

        except subprocess.TimeoutExpired:
            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['error'] = f"Script execution timed out after 60 seconds"
            exec_result['return_code'] = -1
            exec_result['stdout'] = ""
            exec_result['stderr'] = "Timeout"
            logger.warning(f"â° Script {script_info['name']} timed out after 60 seconds")
            # Create dummy result for consistency
            class DummyResult:
                returncode = -1
                stdout = ""
                stderr = "Timeout"
            result = DummyResult()

        except Exception as e:
            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['error'] = f"Script execution failed: {e}"
            exec_result['return_code'] = -2
            exec_result['stdout'] = ""
            exec_result['stderr'] = str(e)
            logger.warning(f"âŒ Script {script_info['name']} execution failed: {e}")
            # Create dummy result for consistency
            class DummyResult:
                returncode = -2
                stdout = ""
                stderr = str(e)
            result = DummyResult()
        
        # Ensure result is defined before using it
        if result is None:
            class DummyResult:
                returncode = -3
                stdout = ""
                stderr = "Unknown error"
            result = DummyResult()
        
        # Save individual script output in implementation-specific subdirectory
        # Create the implementation-specific directory structure
        impl_specific_dir = results_dir.parent / model_name / framework / "execution_logs"
        impl_specific_dir.mkdir(parents=True, exist_ok=True)
        
        # Also create other expected subdirectories for this framework
        if framework == "pymdp":
            (results_dir.parent / model_name / framework / "simulation_data").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "traces").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "visualizations").mkdir(parents=True, exist_ok=True)
        elif framework == "discopy":
            (results_dir.parent / model_name / framework / "diagram_outputs").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "analysis").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "visualizations").mkdir(parents=True, exist_ok=True)
        elif framework == "activeinference_jl":
            (results_dir.parent / model_name / framework / "simulation_data").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "free_energy_traces").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "visualizations").mkdir(parents=True, exist_ok=True)
        elif framework == "rxinfer":
            (results_dir.parent / model_name / framework / "inference_data").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "posterior_traces").mkdir(parents=True, exist_ok=True)
            (results_dir.parent / model_name / framework / "visualizations").mkdir(parents=True, exist_ok=True)
        
        # Extract simulation data from stdout/stderr
        simulation_data = _extract_simulation_data(result.stdout, result.stderr, framework, logger)
        exec_result['simulation_data'] = simulation_data
        
        # Save structured execution results in JSON format
        structured_result = {
            "framework": framework,
            "model_name": model_name,
            "script_name": script_info['name'],
            "script_path": str(script_path),
            "success": exec_result['success'],
            "return_code": exec_result.get('return_code'),
            "execution_time": exec_result.get('execution_time', 0),
            "timestamp": exec_result['timestamp'],
            "simulation_data": simulation_data,
            "execution_metadata": {
                "executor": executor,
                "stdout_length": len(result.stdout),
                "stderr_length": len(result.stderr),
                "output_directory": str(impl_specific_dir.parent)
            }
        }
        
        # Save structured JSON result
        json_output_file = impl_specific_dir / f"{script_info['name']}_results.json"
        with open(json_output_file, 'w') as f:
            json.dump(structured_result, f, indent=2, default=str)
        
        exec_result['structured_result_file'] = str(json_output_file)
        
        # Also save human-readable log
        output_file = impl_specific_dir / f"{script_info['name']}_execution.log"
        with open(output_file, 'w') as f:
            f.write(f"Execution Results for {script_info['name']}\n")
            f.write(f"Timestamp: {exec_result['timestamp']}\n")
            f.write(f"Return Code: {result.returncode}\n")
            f.write(f"Execution Time: {exec_result['execution_time']:.2f} seconds\n")
            f.write(f"Model: {model_name}\n")
            f.write(f"Framework: {framework}\n")
            f.write(f"Output Directory: {impl_specific_dir.parent}\n\n")
            f.write("STDOUT:\n")
            f.write(result.stdout)
            f.write("\n\nSTDERR:\n")
            f.write(result.stderr)
            
        # Also save to centralized location for backward compatibility
        script_output_dir = results_dir / 'individual_outputs' / framework
        script_output_dir.mkdir(parents=True, exist_ok=True)
        
        centralized_output = script_output_dir / f"{script_info['name']}_output.txt"
        with open(centralized_output, 'w') as f:
            f.write(f"Execution Results for {script_info['name']}\n")
            f.write(f"Timestamp: {exec_result['timestamp']}\n")
            f.write(f"Return Code: {result.returncode}\n")
            f.write(f"Execution Time: {exec_result['execution_time']:.2f} seconds\n\n")
            f.write("STDOUT:\n")
            f.write(result.stdout)
            f.write("\n\nSTDERR:\n")
            f.write(result.stderr)
            
        exec_result['output_file'] = str(output_file)
        exec_result['centralized_output_file'] = str(centralized_output)
        exec_result['implementation_directory'] = str(impl_specific_dir.parent)
        
    except subprocess.TimeoutExpired:
        exec_result['error'] = 'Script execution timed out (5 minutes)'
        logger.error(f"Script {script_info['name']} timed out")
        
    except Exception as e:
        exec_result['error'] = str(e)
        exec_result['error_type'] = type(e).__name__
        logger.error(f"Error executing {script_info['name']}: {e}")
    
    return exec_result


def _extract_simulation_data(stdout: str, stderr: str, framework: str, logger) -> Dict[str, Any]:
    """
    Extract simulation data from execution output.
    
    Args:
        stdout: Standard output from script execution
        stderr: Standard error from script execution
        framework: Framework name
        logger: Logger instance
        
    Returns:
        Dictionary with extracted simulation data
    """
    simulation_data = {
        "traces": [],
        "free_energy": [],
        "states": [],
        "observations": [],
        "actions": [],
        "policy": [],
        "raw_output": stdout[:10000] if stdout else "",  # Limit size
        "raw_error": stderr[:10000] if stderr else ""
    }
    
    try:
        # Framework-specific extraction
        if framework == "pymdp":
            simulation_data.update(_extract_pymdp_data(stdout, stderr))
        elif framework == "rxinfer":
            simulation_data.update(_extract_rxinfer_data(stdout, stderr))
        elif framework == "activeinference_jl":
            simulation_data.update(_extract_activeinference_jl_data(stdout, stderr))
        elif framework == "jax":
            simulation_data.update(_extract_jax_data(stdout, stderr))
        elif framework == "discopy":
            simulation_data.update(_extract_discopy_data(stdout, stderr))
        else:
            # Generic extraction - try to find common patterns
            simulation_data.update(_extract_generic_data(stdout, stderr))
            
    except Exception as e:
        logger.warning(f"Failed to extract simulation data for {framework}: {e}")
    
    return simulation_data


def _extract_pymdp_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract PyMDP-specific simulation data."""
    import re
    data = {}
    
    # Try to find state trajectories
    state_pattern = r'state[:\s]+\[([^\]]+)\]|states[:\s]+\[([^\]]+)\]'
    state_matches = re.findall(state_pattern, stdout, re.IGNORECASE)
    if state_matches:
        data["states"] = [match[0] or match[1] for match in state_matches]
    
    # Try to find observations
    obs_pattern = r'observation[:\s]+(\d+)|obs[:\s]+(\d+)'
    obs_matches = re.findall(obs_pattern, stdout, re.IGNORECASE)
    if obs_matches:
        data["observations"] = [int(match[0] or match[1]) for match in obs_matches]
    
    # Try to find actions
    action_pattern = r'action[:\s]+(\d+)|action_taken[:\s]+(\d+)'
    action_matches = re.findall(action_pattern, stdout, re.IGNORECASE)
    if action_matches:
        data["actions"] = [int(match[0] or match[1]) for match in action_matches]
    
    # Try to find free energy
    fe_pattern = r'free[_\s]?energy[:\s]+([\d.]+)|FE[:\s]+([\d.]+)'
    fe_matches = re.findall(fe_pattern, stdout, re.IGNORECASE)
    if fe_matches:
        data["free_energy"] = [float(match[0] or match[1]) for match in fe_matches]
    
    return data


def _extract_rxinfer_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract RxInfer.jl-specific simulation data."""
    import re
    data = {}
    
    # Try to find posterior distributions
    posterior_pattern = r'posterior[:\s]+\[([^\]]+)\]'
    posterior_matches = re.findall(posterior_pattern, stdout, re.IGNORECASE)
    if posterior_matches:
        data["posterior"] = posterior_matches
    
    return data


def _extract_activeinference_jl_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract ActiveInference.jl-specific simulation data."""
    import re
    data = {}
    
    # Try to find free energy traces
    fe_pattern = r'free[_\s]?energy[:\s]+([\d.]+)|FE[:\s]+([\d.]+)'
    fe_matches = re.findall(fe_pattern, stdout, re.IGNORECASE)
    if fe_matches:
        data["free_energy"] = [float(match[0] or match[1]) for match in fe_matches]
    
    # Try to find state beliefs
    belief_pattern = r'belief[:\s]+\[([^\]]+)\]|q\(s\)[:\s]+\[([^\]]+)\]'
    belief_matches = re.findall(belief_pattern, stdout, re.IGNORECASE)
    if belief_matches:
        data["beliefs"] = [match[0] or match[1] for match in belief_matches]
    
    return data


def _extract_jax_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract JAX-specific simulation data."""
    # Similar to PyMDP but may have different output format
    return _extract_pymdp_data(stdout, stderr)


def _extract_discopy_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract DisCoPy-specific simulation data."""
    import re
    data = {}
    
    # Try to find diagram information
    diagram_pattern = r'diagram[:\s]+(\w+)|circuit[:\s]+(\w+)'
    diagram_matches = re.findall(diagram_pattern, stdout, re.IGNORECASE)
    if diagram_matches:
        data["diagrams"] = [match[0] or match[1] for match in diagram_matches]
    
    return data


def _extract_generic_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Generic extraction for unknown frameworks."""
    import re
    data = {}
    
    # Try to find any numeric arrays or lists
    array_pattern = r'\[([\d.,\s]+)\]'
    array_matches = re.findall(array_pattern, stdout)
    if array_matches:
        data["arrays"] = array_matches[:10]  # Limit to first 10
    
    return data


def generate_execution_report(execution_results: Dict[str, Any], results_dir: Path, logger):
    """
    Generate a comprehensive execution report.
    
    Args:
        execution_results: Dictionary with execution results
        results_dir: Directory to save the report
        logger: Logger instance
    """
    report_file = results_dir / "execution_report.md"
    
    try:
        with open(report_file, 'w') as f:
            f.write("# GNN Script Execution Report\n\n")
            f.write(f"**Generated:** {execution_results['timestamp']}\n")
            f.write(f"**Target Directory:** {execution_results['target_directory']}\n")
            f.write(f"**Output Directory:** {execution_results['output_directory']}\n\n")
            
            f.write("## Summary\n\n")
            f.write(f"- **Total Scripts Found:** {execution_results['total_scripts_found']}\n")
            f.write(f"- **Successful Executions:** {execution_results['successful_executions']}\n")
            f.write(f"- **Failed Executions:** {execution_results['failed_executions']}\n\n")
            
            if execution_results['execution_details']:
                f.write("## Execution Details\n\n")
                
                for detail in execution_results['execution_details']:
                    status = "âœ… SUCCESS" if detail['success'] else "âŒ FAILED"
                    f.write(f"### {detail['script_name']} - {status}\n\n")
                    f.write(f"- **Framework:** {detail['framework']}\n")
                    f.write(f"- **Executor:** {detail['executor']}\n")
                    f.write(f"- **Path:** `{detail['script_path']}`\n")
                    f.write(f"- **Return Code:** {detail.get('return_code', 'N/A')}\n")
                    f.write(f"- **Execution Time:** {detail.get('execution_time', 0):.2f} seconds\n")
                    
                    if not detail['success'] and 'error' in detail:
                        f.write(f"- **Error:** {detail['error']}\n")
                    
                    if 'output_file' in detail:
                        f.write(f"- **Detailed Output:** {detail['output_file']}\n")
                    
                    f.write("\n")
            
            f.write("## Next Steps\n\n")
            if execution_results['failed_executions'] > 0:
                f.write("1. Review failed executions above\n")
                f.write("2. Check individual output files for detailed error information\n")
                f.write("3. Ensure required dependencies are installed\n")
                f.write("4. Verify script syntax and functionality\n\n")
            else:
                f.write("All scripts executed successfully! Check individual output files for results.\n\n")
            
        logger.info(f"Generated execution report: {report_file}")
        
    except Exception as e:
        logger.error(f"Failed to generate execution report: {e}")


def execute_simulation_from_gnn(gnn_file: Path, output_dir: Path) -> Dict[str, Any]:
    """
    Execute simulation from GNN file.
    
    Args:
        gnn_file: Path to GNN file
        output_dir: Output directory
        
    Returns:
        Dictionary with execution results
    """
    try:
        logger.info(f"Executing simulation for {gnn_file}")
        
        # Import execution engine
        from .executor import ExecutionEngine
        
        # Create execution engine
        engine = ExecutionEngine()
        
        # Execute simulation
        result = engine.execute_simulation_from_gnn(gnn_file, output_dir)
        
        return result
        
    except Exception as e:
        logger.error(f"Failed to execute simulation for {gnn_file}: {e}")
        return {
            "success": False,
            "error": str(e)
        }
