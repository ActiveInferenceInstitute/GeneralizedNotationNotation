#!/usr/bin/env python3
"""
Execute Processor module for GNN Processing Pipeline.

This module provides execute processing capabilities for rendered implementations.
"""

from pathlib import Path
from typing import Dict, Any, List
import logging
import subprocess
import json
import os
import sys
import stat
from datetime import datetime
from shutil import copy2

# Import logging helpers with fallback
try:
    from utils.step_logging import (
        log_step_start,
        log_step_success,
        log_step_error,
        log_step_warning
    )
    from utils.logging_utils import PipelineLogger
except ImportError:
    # Use fallback if imports fail
    PipelineLogger = None
    def log_step_start(logger, msg): logger.info(f"ðŸš€ {msg}")
    def log_step_success(logger, msg): logger.info(f"âœ… {msg}")
    def log_step_error(logger, msg): logger.error(f"âŒ {msg}")
    def log_step_warning(logger, msg): logger.warning(f"âš ï¸ {msg}")

logger = logging.getLogger(__name__)



def check_julia_dependencies(verbose: bool, logger) -> bool:
    """
    Check if required Julia packages are available.
    
    Args:
        verbose: Enable verbose logging
        logger: Logger instance
        
    Returns:
        True if dependencies ok, False otherwise
    """
    try:
        # check basic julia availability
        subprocess.run(['julia', '--version'], capture_output=True, check=True)
        
        # Check for key packages
        check_script = 'using Pkg; Pkg.status(["RxInfer", "ActiveInference", "GraphPPL"])'
        result = subprocess.run(
            ['julia', '-e', check_script],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            if verbose:
                logger.warning(f"Julia package check failed: {result.stderr}")
            return False
            
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False


def determine_script_framework(script_path: Path, render_output_dir: Path, framework_dirs: Dict[str, str]) -> str:
    """
    Determine the framework for a script based on its directory path.

    Args:
        script_path: Path to the script
        render_output_dir: Base render output directory
        framework_dirs: Mapping of directory names to framework names

    Returns:
        Framework name or 'unknown'
    """
    try:
        # Get relative path from render output directory
        relative_path = script_path.relative_to(render_output_dir)

        # Check each part of the path for framework indicators
        for part in relative_path.parts:
            # Check if this part matches a known framework directory
            if part.lower() in framework_dirs:
                return framework_dirs[part.lower()]

            # Check for framework names in the directory name
            for framework_name in framework_dirs.values():
                if framework_name.lower() in part.lower():
                    return framework_name

        # Default fallback
        return "unknown"

    except Exception:
        return "unknown"


def parse_frameworks_parameter(frameworks: str, logger) -> List[str]:
    """
    Parse the frameworks parameter into a list of framework names.

    Args:
        frameworks: Comma-separated string of framework names or preset
        logger: Logger instance

    Returns:
        List of framework names to include
    """
    if not frameworks or frameworks.lower() == "all":
        return ["pymdp", "jax", "discopy", "rxinfer", "activeinference_jl"]

    if frameworks.lower() == "lite":
        return ["pymdp", "jax", "discopy"]

    # Parse comma-separated list
    framework_list = [f.strip() for f in frameworks.split(",")]
    valid_frameworks = ["pymdp", "jax", "discopy", "rxinfer", "activeinference_jl"]

    # Filter out invalid frameworks
    valid_list = [f for f in framework_list if f in valid_frameworks]

    if len(valid_list) != len(framework_list):
        invalid = [f for f in framework_list if f not in valid_frameworks]
        logger.warning(f"Invalid frameworks specified: {invalid}. Valid options: {valid_frameworks}")

    return valid_list if valid_list else ["pymdp"]  # Default to pymdp if nothing valid

def process_execute(
    target_dir: Path,
    output_dir: Path,
    verbose: bool = False,
    frameworks: str = "all",
    **kwargs
) -> bool:
    """
    Execute rendered implementations from 11_render_output directory.
    
    This function searches for executable scripts generated by 11_render.py
    and executes them using subprocess, capturing their outputs and results.
    
    Args:
        target_dir: Directory containing rendered executable scripts (typically 11_render_output)
        output_dir: Directory to save execution results
        verbose: Enable verbose output
        **kwargs: Additional arguments
        
    Returns:
        True if processing successful, False otherwise
    """
    logger = logging.getLogger("execute")
    
    try:
        log_step_start(logger, "Processing execute - searching for rendered implementations")

        # Parse frameworks parameter
        requested_frameworks = parse_frameworks_parameter(frameworks, logger)
        logger.info(f"Requested frameworks: {requested_frameworks}")

        # Create results directory
        results_dir = output_dir
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize execution results
        execution_results = {
            "timestamp": datetime.now().isoformat(),
            "target_directory": str(target_dir),
            "output_directory": str(output_dir),
            "total_scripts_found": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "skipped_executions": 0,
            "execution_details": [],
            "framework_status": {},
            "success": True
        }
        
        # Look for rendered implementations from render output
        render_output_dir = None
        
        # Priority 1: Check if --render-output-dir was specified
        if kwargs.get('render_output_dir'):
            render_output_dir = Path(kwargs['render_output_dir'])
            
        # Priority 2: Check if target_dir is already a render output directory
        elif "11_render_output" in str(target_dir) or target_dir.name == "11_render_output":
            render_output_dir = target_dir
            
        # Priority 3: Search for render output directories in common locations
        else:
            potential_dirs = [
                # Standard pipeline location
                target_dir.parent / "output" / "11_render_output",
                target_dir / "11_render_output",
                
                # Recent test locations
                Path("output/test_render/11_render_output/11_render_output"),
                Path("output/test_render_improved/11_render_output/11_render_output"),
                
                # Search in all output directories
                *list(Path("output").glob("*/11_render_output/11_render_output")),
                *list(Path("output").glob("**/11_render_output")),
            ]
            
            for potential_dir in potential_dirs:
                if potential_dir.exists() and any(potential_dir.rglob("*")):
                    render_output_dir = potential_dir
                    logger.info(f"Found render output directory: {render_output_dir}")
                    break
        
        if verbose:
            logger.info(f"Searching for executable scripts in: {render_output_dir}")

        if not render_output_dir or not render_output_dir.exists():
            log_step_warning(logger, f"Render output directory not found: {render_output_dir}")
            execution_results["success"] = True  # Not an error, just no files to execute
            execution_results["message"] = "No rendered implementations found"
        else:
            # Find executable scripts, filtered by requested frameworks
            executable_scripts = find_executable_scripts(render_output_dir, verbose, logger, requested_frameworks)
            execution_results["total_scripts_found"] = len(executable_scripts)
            execution_results["requested_frameworks"] = requested_frameworks
            
            if not executable_scripts:
                log_step_warning(logger, "No executable scripts found in render output")
                execution_results["message"] = "No executable scripts found"
                execution_results["success"] = True
            else:
                logger.info(f"Found {len(executable_scripts)} executable scripts to run")
                
                # Execute each script
                for script_info in executable_scripts:
                    exec_result = execute_single_script(script_info, results_dir, verbose, logger)
                    execution_results["execution_details"].append(exec_result)
                    
                    # Update framework status
                    framework = exec_result.get("framework", "unknown")
                    if framework not in execution_results["framework_status"]:
                        execution_results["framework_status"][framework] = {"status": "unknown", "executions": 0}

                    execution_results["framework_status"][framework]["executions"] += 1

                    if exec_result["success"]:
                        execution_results["successful_executions"] += 1
                        execution_results["framework_status"][framework]["status"] = "success"
                    else:
                        execution_results["failed_executions"] += 1
                        execution_results["framework_status"][framework]["status"] = "failed"
                        if "error" in exec_result:
                            execution_results["framework_status"][framework]["error"] = exec_result["error"]
        
        # Save detailed results
        results_file = results_dir / "execution_summary.json"
        with open(results_file, 'w') as f:
            json.dump(execution_results, f, indent=2, default=str)
            
        # Generate execution report
        generate_execution_report(execution_results, results_dir, logger)
        
        # Determine overall success with improved logic
        total_scripts = execution_results["total_scripts_found"]
        failed_scripts = execution_results["failed_executions"]

        if total_scripts == 0:
            log_step_warning(logger, "No executable scripts found to run")
            return True
        elif failed_scripts == 0:
            log_step_success(logger, "Execute processing completed successfully")
        elif failed_scripts < total_scripts * 0.5:  # Less than 50% failures
            log_step_warning(logger, f"Execute processing completed with {failed_scripts}/{total_scripts} failures (partial success)")
        else:  # 50% or more failures
            log_step_error(logger, f"Execute processing completed with {failed_scripts}/{total_scripts} failures (critical failures)")

        # Return True if we found and attempted to run scripts (even if some failed)
        # Return False only if there was a critical error preventing execution
        return True
        
    except Exception as e:
        log_step_error(logger, f"Execute processing failed: {e}")
        return False

def find_executable_scripts(render_output_dir: Path, verbose: bool, logger, requested_frameworks: List[str]) -> List[Dict[str, Any]]:
    """
    Find executable scripts in the render output directory.
    
    Args:
        render_output_dir: Directory containing rendered scripts
        verbose: Enable verbose logging
        logger: Logger instance
        
    Returns:
        List of dictionaries with script information
    """
    executable_scripts = []
    
    # Define supported script types and their executors
    script_types = {
        '*.py': {'executor': sys.executable, 'framework': 'python'},
        '*.jl': {'executor': 'julia', 'framework': 'julia'},
    }

    # Map framework directories to framework names
    framework_dirs = {
        'pymdp': 'pymdp',
        'jax': 'jax',
        'discopy': 'discopy',
        'rxinfer': 'rxinfer',
        'activeinference_jl': 'activeinference_jl',
        'activeinference.jl': 'activeinference_jl',
    }

    for pattern, config in script_types.items():
        scripts = list(render_output_dir.rglob(pattern))

        for script_path in scripts:
            # Skip test files and other non-executable scripts
            if any(skip in script_path.name.lower() for skip in ['test_', '__', 'readme']):
                continue

            # Determine framework from directory path
            framework = determine_script_framework(script_path, render_output_dir, framework_dirs)

            # Filter by requested frameworks
            if framework not in requested_frameworks:
                if verbose:
                    logger.debug(f"Skipping {framework} script: {script_path.name} (not in requested frameworks)")
                continue

            # Check if script is executable or can be made executable
            script_info = {
                'path': script_path,
                'name': script_path.name,
                'framework': framework,
                'executor': config['executor'],
                'relative_path': script_path.relative_to(render_output_dir),
                'size_bytes': script_path.stat().st_size if script_path.exists() else 0
            }
            
            executable_scripts.append(script_info)
            
            if verbose:
                logger.info(f"Found {config['framework']} script: {script_info['relative_path']}")
    
    return executable_scripts


def execute_single_script(script_info: Dict[str, Any], results_dir: Path, verbose: bool, logger) -> Dict[str, Any]:
    """
    Execute a single script using subprocess.
    
    Args:
        script_info: Dictionary containing script information
        results_dir: Directory to save execution results (will create implementation-specific subfolders)
        verbose: Enable verbose logging
        logger: Logger instance
        
    Returns:
        Dictionary with execution results
    """
    script_path = script_info['path']
    executor = script_info['executor']
    
    # Extract model name and framework from script path for organization
    # Expected path: .../11_render_output/model_name/framework/script.ext
    path_parts = script_path.parts
    if len(path_parts) >= 3:
        model_name = path_parts[-3]  # e.g., 'actinf_pomdp_agent' 
        framework = path_parts[-2]   # e.g., 'pymdp'
    else:
        model_name = 'unknown_model'
        framework = script_info['framework']
    
    # Prepare execution result
    exec_result = {
        'script_path': str(script_path),
        'script_name': script_info['name'],
        'framework': framework,
        'model_name': model_name,
        'executor': executor,
        'success': False,
        'return_code': None,
        'stdout': '',
        'stderr': '',
        'execution_time': 0,
        'timestamp': datetime.now().isoformat()
    }
    
    try:

        if verbose:
            logger.info(f"Executing {script_info['framework']} script: {script_info['name']}")
        
        start_time = datetime.now()
        
        # Check if the executor is available
        try:
            # For Python scripts, check if Python is available (most are Python scripts)
            if executor in ['python', 'python3']:
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
                             
                # For PyMDP, specifically check if it's importable
                if framework == "pymdp":
                    try:
                        import_check = subprocess.run(
                            [executor, '-c', 'import pymdp; print("ok")'],
                            capture_output=True,
                            text=True,
                            timeout=5
                        )
                        if import_check.returncode != 0:
                            logger.warning(f"PyMDP package appears missing or broken: {import_check.stderr}")
                            exec_result['error'] = f"PyMDP dependency missing: {import_check.stderr}"
                            # Continue anyway as it might be a local import, but log warning
                    except Exception:
                        pass
                        
            # For Julia scripts, check availability and dependencies
            elif executor == 'julia':
                if not check_julia_dependencies(verbose, logger):
                    logger.warning("Julia dependencies (RxInfer/ActiveInference) may be missing")
                    # Don't fail here, let the script try to run, but log warning
                    
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
            # For other executors, try a basic check
            else:
                subprocess.run([executor, '--version'],
                             capture_output=True,
                             text=True,
                             timeout=5,
                             check=True)
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
            exec_result['error'] = f"Executor '{executor}' is not available or not working: {e}"
            logger.warning(f"Executor '{executor}' is not available - skipping {script_info['name']}")
            return exec_result
        
        # Execute the script with improved error handling
        script_name = script_path.name
        result = None
        try:
            # Set environment variables if needed
            env = os.environ.copy()
            if framework == "pymdp":
                env["PYTHONPATH"] = str(script_path.parent) + os.pathsep + env.get("PYTHONPATH", "")
                
            result = subprocess.run(
                [executor, script_name],
                capture_output=True,
                text=True,
                timeout=300,  # Increased timeout to 5 minutes for complex simulations
                cwd=script_path.parent,  # Run in the script's directory
                env=env
            )

            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['return_code'] = result.returncode
            exec_result['stdout'] = result.stdout
            exec_result['stderr'] = result.stderr

            if result.returncode == 0:
                exec_result['success'] = True
                logger.info(f"âœ… Successfully executed {script_info['name']}")
                if verbose and result.stdout:
                    logger.info(f"Script output: {result.stdout[:200]}...")  # Show first 200 chars
            else:
                exec_result['error'] = f"Script failed with return code {result.returncode}"
                
                # Analyze stderr for common errors
                if "ModuleNotFoundError" in result.stderr:
                    exec_result['error_type'] = "DependencyError"
                    logger.error(f"Missing dependency in {script_info['name']}: {result.stderr.splitlines()[-1]}")
                elif "SyntaxError" in result.stderr:
                    exec_result['error_type'] = "SyntaxError"
                    logger.error(f"Syntax error in {script_info['name']}")
                else:
                    exec_result['error_type'] = "RuntimeError"
                    
                logger.warning(f"âš ï¸ Script {script_info['name']} failed with return code {result.returncode}")
                if result.stderr:
                    logger.warning(f"Error output: {result.stderr[:500]}...")  # Show first 500 chars

        except subprocess.TimeoutExpired:
            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['error'] = f"Script execution timed out after 300 seconds"
            exec_result['return_code'] = -1
            exec_result['stdout'] = ""
            exec_result['stderr'] = "Timeout"
            logger.warning(f"â° Script {script_info['name']} timed out after 300 seconds")
            # Create error result for consistency
            class ErrorResult:
                returncode = -1
                stdout = ""
                stderr = "Timeout"
            result = ErrorResult()

        except Exception as e:
            end_time = datetime.now()
            exec_result['execution_time'] = (end_time - start_time).total_seconds()
            exec_result['error'] = f"Script execution failed: {e}"
            exec_result['return_code'] = -2
            exec_result['stdout'] = ""
            exec_result['stderr'] = str(e)
            logger.warning(f"âŒ Script {script_info['name']} execution failed: {e}")
            # Create error result for consistency
            class ErrorResult:
                returncode = -2
                stdout = ""
                stderr = str(e)
            result = ErrorResult()
        
        # Ensure result is defined before using it
        if result is None:
            class ErrorResult:
                returncode = -3
                stdout = ""
                stderr = "Unknown error"
            result = ErrorResult()
        
        # Save individual script output in implementation-specific subdirectory
        # Create the implementation-specific directory structure
        impl_specific_dir = results_dir / model_name / framework / "execution_logs"
        impl_specific_dir.mkdir(parents=True, exist_ok=True)
        
        # Note: Framework-specific subdirectories (visualizations, simulation_data, etc.)
        # are created on-demand by collect_execution_outputs() only when actual content
        # is copied to them, avoiding empty folder creation.

        
        # Extract simulation data from stdout/stderr
        simulation_data = _extract_simulation_data(result.stdout, result.stderr, framework, logger)
        exec_result['simulation_data'] = simulation_data
        
        # Save structured execution results in JSON format
        structured_result = {
            "framework": framework,
            "model_name": model_name,
            "script_name": script_info['name'],
            "script_path": str(script_path),
            "success": exec_result['success'],
            "return_code": exec_result.get('return_code'),
            "execution_time": exec_result.get('execution_time', 0),
            "timestamp": exec_result['timestamp'],
            "simulation_data": simulation_data,
            "execution_metadata": {
                "executor": executor,
                "stdout_length": len(result.stdout),
                "stderr_length": len(result.stderr),
                "output_directory": str(impl_specific_dir.parent)
            }
        }
        
        # Save structured JSON result
        json_output_file = impl_specific_dir / f"{script_info['name']}_results.json"
        with open(json_output_file, 'w') as f:
            json.dump(structured_result, f, indent=2, default=str)
        
        exec_result['structured_result_file'] = str(json_output_file)
        
        # Also save human-readable log
        output_file = impl_specific_dir / f"{script_info['name']}_execution.log"
        with open(output_file, 'w') as f:
            f.write(f"Execution Results for {script_info['name']}\n")
            f.write(f"Timestamp: {exec_result['timestamp']}\n")
            f.write(f"Return Code: {result.returncode}\n")
            f.write(f"Execution Time: {exec_result['execution_time']:.2f} seconds\n")
            f.write(f"Model: {model_name}\n")
            f.write(f"Framework: {framework}\n")
            f.write(f"Output Directory: {impl_specific_dir.parent}\n\n")
            f.write("STDOUT:\n")
            f.write(result.stdout)
            f.write("\n\nSTDERR:\n")
            f.write(result.stderr)
            
        # Also save to centralized location for backward compatibility
        script_output_dir = results_dir / 'individual_outputs' / framework
        script_output_dir.mkdir(parents=True, exist_ok=True)
        
        centralized_output = script_output_dir / f"{script_info['name']}_output.txt"
        with open(centralized_output, 'w') as f:
            f.write(f"Execution Results for {script_info['name']}\n")
            f.write(f"Timestamp: {exec_result['timestamp']}\n")
            f.write(f"Return Code: {result.returncode}\n")
            f.write(f"Execution Time: {exec_result['execution_time']:.2f} seconds\n\n")
            f.write("STDOUT:\n")
            f.write(result.stdout)
            f.write("\n\nSTDERR:\n")
            f.write(result.stderr)
            
        exec_result['output_file'] = str(output_file)
        exec_result['centralized_output_file'] = str(centralized_output)
        exec_result['implementation_directory'] = str(impl_specific_dir.parent)
        
        # Collect execution outputs (visualizations, simulation data, traces)
        if exec_result['success']:
            try:
                logger.info(f"Collecting execution outputs for {framework} script {script_info['name']}")
                collected_outputs = collect_execution_outputs(
                    script_path, 
                    impl_specific_dir.parent, 
                    framework, 
                    logger
                )
                exec_result['collected_outputs'] = collected_outputs
                
                # Update structured result with collected file paths
                structured_result['collected_outputs'] = collected_outputs
                
                # Re-save structured result with collected outputs
                with open(json_output_file, 'w') as f:
                    json.dump(structured_result, f, indent=2, default=str)
                logger.debug(f"Updated results JSON with collected outputs")
                
                # Enhance simulation data extraction from collected files
                if collected_outputs:
                    logger.info(f"Extracting simulation data from collected files for {framework}")
                    enhanced_data = _extract_simulation_data_from_files(
                        impl_specific_dir.parent,
                        framework,
                        logger
                    )
                    if enhanced_data:
                        logger.info(f"Extracted {len(enhanced_data)} data fields from files")
                        simulation_data.update(enhanced_data)
                        exec_result['simulation_data'] = simulation_data
                        structured_result['simulation_data'] = simulation_data
                        
                        # Re-save again with enhanced data
                        with open(json_output_file, 'w') as f:
                            json.dump(structured_result, f, indent=2, default=str)
                        logger.debug(f"Updated results JSON with enhanced simulation data")
                    else:
                        logger.debug(f"No additional data extracted from files for {framework}")
                
            except Exception as e:
                logger.warning(f"Failed to collect execution outputs: {e}")
                import traceback
                logger.debug(traceback.format_exc())
        
    except subprocess.TimeoutExpired:
        exec_result['error'] = 'Script execution timed out (5 minutes)'
        logger.error(f"Script {script_info['name']} timed out")
        
    except Exception as e:
        exec_result['error'] = str(e)
        exec_result['error_type'] = type(e).__name__
        logger.error(f"Error executing {script_info['name']}: {e}")
    
    return exec_result


def _normalize_and_deduplicate_paths(found_files: List[Path], logger) -> List[Path]:
    """
    Normalize paths and remove duplicates/nested paths.
    
    Args:
        found_files: List of file paths to normalize
        logger: Logger instance for logging
        
    Returns:
        Deduplicated list of normalized paths
    """
    if not found_files:
        return []
    
    # Resolve all paths to absolute and remove duplicates
    normalized = {}
    for file_path in found_files:
        try:
            abs_path = file_path.resolve()
            # Use absolute path as key to detect duplicates
            if abs_path not in normalized:
                normalized[abs_path] = file_path
        except (OSError, RuntimeError) as e:
            logger.debug(f"Skipping invalid path {file_path}: {e}")
            continue
    
    # Filter out nested paths (if file in subdir is already found in parent)
    # Sort by path depth (shallow first) to process parent directories first
    sorted_paths = sorted(normalized.values(), key=lambda p: len(p.parts))
    deduplicated = []
    seen_names = set()
    
    for file_path in sorted_paths:
        file_name = file_path.name
        file_parent = file_path.parent
        
        # Check if we've already seen this filename from a parent directory
        # This prevents collecting files from nested visualizations/visualizations/ directories
        is_nested_duplicate = False
        for seen_path in deduplicated:
            seen_parent = seen_path.parent
            # Check if current file is in a nested subdirectory of an already-seen file
            try:
                if file_parent.is_relative_to(seen_parent) and file_name == seen_path.name:
                    is_nested_duplicate = True
                    logger.debug(f"Skipping nested duplicate: {file_path} (already have {seen_path})")
                    break
            except (ValueError, AttributeError):
                # Python < 3.9 doesn't have is_relative_to, use string comparison
                try:
                    if str(file_parent).startswith(str(seen_parent)) and file_name == seen_path.name:
                        is_nested_duplicate = True
                        logger.debug(f"Skipping nested duplicate: {file_path} (already have {seen_path})")
                        break
                except Exception:
                    pass
        
        if not is_nested_duplicate:
            deduplicated.append(file_path)
            seen_names.add(file_name)
    
    if len(found_files) != len(deduplicated):
        logger.info(f"Deduplicated paths: {len(found_files)} â†’ {len(deduplicated)} files")
    
    return deduplicated


def collect_execution_outputs(
    script_path: Path,
    output_dir: Path,
    framework: str,
    logger
) -> Dict[str, List[str]]:
    """
    Collect all outputs from executed script and copy to execute output directory.
    
    Args:
        script_path: Path to the executed script
        output_dir: Execute output directory for this model/framework
        framework: Framework name
        logger: Logger instance
        
    Returns:
        Dictionary with lists of copied file paths by category
    """
    collected = {
        "visualizations": [],
        "simulation_data": [],
        "traces": [],
        "other": []
    }
    
    try:
        # Find script's output directory (varies by framework)
        script_dir = script_path.parent
        
        # Framework-specific output location patterns (using Path.rglob)
        found_files = []
        
        if framework == "pymdp":
            # PyMDP saves to output/pymdp_simulations/{model_name}/
            pymdp_output = script_dir / "output" / "pymdp_simulations"
            if pymdp_output.exists():
                found_files.extend(pymdp_output.rglob("*.png"))
                found_files.extend(pymdp_output.rglob("*.svg"))
                found_files.extend(pymdp_output.rglob("*.json"))
                found_files.extend(pymdp_output.rglob("*.pkl"))
        elif framework == "discopy":
            # DisCoPy saves to discopy_diagrams/
            discopy_dir = script_dir / "discopy_diagrams"
            if discopy_dir.exists():
                found_files.extend(discopy_dir.rglob("*.png"))
                found_files.extend(discopy_dir.rglob("*.svg"))
                found_files.extend(discopy_dir.rglob("*.json"))
        elif framework == "activeinference_jl":
            # ActiveInference.jl saves to activeinference_outputs_*/
            # Use specific directory patterns to avoid nested visualizations/ directories
            for output_dir in script_dir.glob("activeinference_outputs_*"):
                if output_dir.is_dir():
                    # Look directly in visualizations/ directory (not recursively)
                    viz_dir = output_dir / "visualizations"
                    if viz_dir.exists() and viz_dir.is_dir():
                        found_files.extend(viz_dir.glob("*.png"))
                        found_files.extend(viz_dir.glob("*.svg"))
                    # Look in simulation_data/ directory
                    sim_data_dir = output_dir / "simulation_data"
                    if sim_data_dir.exists() and sim_data_dir.is_dir():
                        found_files.extend(sim_data_dir.glob("*.json"))
                        found_files.extend(sim_data_dir.glob("*.csv"))
                    # Look in free_energy_traces/ directory
                    traces_dir = output_dir / "free_energy_traces"
                    if traces_dir.exists() and traces_dir.is_dir():
                        found_files.extend(traces_dir.glob("*.json"))
                        found_files.extend(traces_dir.glob("*.csv"))
        elif framework == "rxinfer":
            # RxInfer saves to rxinfer_outputs/
            rxinfer_dir = script_dir / "rxinfer_outputs"
            if rxinfer_dir.exists():
                found_files.extend(rxinfer_dir.rglob("*.png"))
                found_files.extend(rxinfer_dir.rglob("*.json"))
                found_files.extend(rxinfer_dir.rglob("*.csv"))
        elif framework == "jax":
            # JAX saves to jax_outputs/
            jax_dir = script_dir / "jax_outputs"
            if jax_dir.exists():
                found_files.extend(jax_dir.rglob("*.png"))
                found_files.extend(jax_dir.rglob("*.json"))
        
        # Also search recursively in script directory for common output files
        if not found_files:
            found_files.extend(script_dir.rglob("*.png"))
            found_files.extend(script_dir.rglob("*.svg"))
            found_files.extend(script_dir.rglob("*.json"))
            found_files.extend(script_dir.rglob("*.pkl"))
            found_files.extend(script_dir.rglob("*.csv"))
        
        # Filter out the script itself and non-existent files
        found_files = [f for f in found_files if f != script_path and f.exists() and f.is_file()]
        
        # Normalize and deduplicate paths
        found_files = _normalize_and_deduplicate_paths(found_files, logger)
        
        if not found_files:
            logger.debug(f"No output files found for {framework} script {script_path.name}")
            return collected
        
        logger.info(f"Found {len(found_files)} output files to collect for {framework}")
        
        # Categorize and copy files
        for source_file in found_files:
            try:
                # Determine category
                ext = source_file.suffix.lower()
                if ext in ['.png', '.svg', '.jpg', '.jpeg']:
                    dest_dir = output_dir / "visualizations"
                    category = "visualizations"
                elif ext in ['.json', '.pkl', '.csv']:
                    # Check if it's a trace file
                    if 'trace' in source_file.name.lower() or 'posterior' in source_file.name.lower():
                        dest_dir = output_dir / "traces"
                        category = "traces"
                    else:
                        dest_dir = output_dir / "simulation_data"
                        category = "simulation_data"
                else:
                    dest_dir = output_dir / "other"
                    category = "other"
                
                # Create destination directory
                dest_dir.mkdir(parents=True, exist_ok=True)
                
                # Copy file (handle name conflicts)
                dest_file = dest_dir / source_file.name
                
                # Check if file already exists (avoid duplicate copies)
                if dest_file.exists():
                    # Check if it's the same file (by size and modification time)
                    try:
                        source_stat = source_file.stat()
                        dest_stat = dest_file.stat()
                        if (source_stat.st_size == dest_stat.st_size and 
                            abs(source_stat.st_mtime - dest_stat.st_mtime) < 1.0):
                            # Same file, skip copy
                            logger.debug(f"Skipping duplicate: {dest_file.name} already exists")
                            collected[category].append(str(dest_file))
                            continue
                    except OSError:
                        pass
                    
                    # Different file or can't compare - add parent directory name to avoid conflicts
                    parent_name = source_file.parent.name
                    # Avoid adding parent name if it's already in the filename
                    if not source_file.name.startswith(f"{parent_name}_"):
                        dest_file = dest_dir / f"{parent_name}_{source_file.name}"
                    else:
                        # Already has parent name, try adding a counter
                        counter = 1
                        base_name = source_file.stem
                        ext = source_file.suffix
                        while dest_file.exists():
                            dest_file = dest_dir / f"{base_name}_{counter}{ext}"
                            counter += 1
                
                copy2(source_file, dest_file)
                
                collected[category].append(str(dest_file))
                logger.info(f"Copied {source_file.name} â†’ {dest_file.relative_to(output_dir)}")
                
            except Exception as e:
                logger.warning(f"Failed to copy {source_file}: {e}")
        
        # Log summary
        total_copied = sum(len(files) for files in collected.values())
        if total_copied > 0:
            logger.info(f"Collected {total_copied} output files: "
                       f"{len(collected['visualizations'])} visualizations, "
                       f"{len(collected['simulation_data'])} data files, "
                       f"{len(collected['traces'])} traces")
        
    except Exception as e:
        logger.error(f"Error collecting execution outputs: {e}")
        import traceback
        logger.debug(traceback.format_exc())
    
    return collected


def _extract_simulation_data_from_files(
    output_dir: Path,
    framework: str,
    logger
) -> Dict[str, Any]:
    """
    Extract simulation data from collected files (not just stdout/stderr).
    
    Args:
        output_dir: Directory containing collected output files
        framework: Framework name
        logger: Logger instance
        
    Returns:
        Dictionary with extracted simulation data
    """
    enhanced_data = {}
    
    try:
        if framework == "pymdp":
            enhanced_data = _extract_pymdp_data_from_files(output_dir, logger)
        elif framework == "rxinfer":
            enhanced_data = _extract_rxinfer_data_from_files(output_dir, logger)
        elif framework == "activeinference_jl":
            enhanced_data = _extract_activeinference_jl_data_from_files(output_dir, logger)
        elif framework == "discopy":
            enhanced_data = _extract_discopy_data_from_files(output_dir, logger)
        elif framework == "jax":
            enhanced_data = _extract_jax_data_from_files(output_dir, logger)
            
    except Exception as e:
        logger.warning(f"Failed to extract simulation data from files for {framework}: {e}")
        import traceback
        logger.debug(traceback.format_exc())
    
    return enhanced_data


def _extract_pymdp_data_from_files(output_dir: Path, logger) -> Dict[str, Any]:
    """Extract PyMDP simulation data from saved files."""
    data = {}
    
    try:
        # Look for simulation_results.json
        sim_data_dir = output_dir / "simulation_data"
        if sim_data_dir.exists():
            results_files = list(sim_data_dir.glob("*simulation_results.json"))
            if results_files:
                results_file = results_files[0]
                try:
                    with open(results_file, 'r') as f:
                        results = json.load(f)
                    
                    # Extract beliefs, actions, observations
                    if "beliefs" in results:
                        data["beliefs"] = results["beliefs"]
                    if "actions" in results:
                        data["actions"] = results["actions"]
                    if "observations" in results:
                        data["observations"] = results["observations"]
                    if "num_timesteps" in results:
                        data["num_timesteps"] = results["num_timesteps"]
                    
                    logger.info(f"Extracted PyMDP data from {results_file.name}")
                except Exception as e:
                    logger.warning(f"Failed to parse {results_file}: {e}")
        
        # Count visualizations
        viz_dir = output_dir / "visualizations"
        if viz_dir.exists():
            viz_files = list(viz_dir.glob("*.png")) + list(viz_dir.glob("*.svg"))
            if viz_files:
                data["visualization_count"] = len(viz_files)
                data["visualization_files"] = [str(f.name) for f in viz_files]
        
    except Exception as e:
        logger.warning(f"Error extracting PyMDP data from files: {e}")
    
    return data


def _extract_rxinfer_data_from_files(output_dir: Path, logger) -> Dict[str, Any]:
    """Extract RxInfer.jl simulation data from saved files."""
    data = {}
    
    try:
        # Look for inference data JSON files
        data_dir = output_dir / "inference_data"
        if data_dir.exists():
            json_files = list(data_dir.glob("*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, 'r') as f:
                        inference_data = json.load(f)
                        if "posterior" in inference_data:
                            data["posterior"] = inference_data["posterior"]
                except Exception:
                    pass
        
        # Look for trace files
        trace_dir = output_dir / "posterior_traces"
        if trace_dir.exists():
            trace_files = list(trace_dir.glob("*.csv"))
            if trace_files:
                data["trace_files"] = [str(f.name) for f in trace_files]
                
    except Exception as e:
        logger.warning(f"Error extracting RxInfer data from files: {e}")
    
    return data


def _extract_activeinference_jl_data_from_files(output_dir: Path, logger) -> Dict[str, Any]:
    """Extract ActiveInference.jl simulation data from saved files."""
    data = {}
    
    try:
        # Look for simulation data JSON files
        data_dir = output_dir / "simulation_data"
        if data_dir.exists():
            json_files = list(data_dir.glob("*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, 'r') as f:
                        sim_data = json.load(f)
                        if "free_energy" in sim_data:
                            data["free_energy"] = sim_data["free_energy"]
                        if "beliefs" in sim_data:
                            data["beliefs"] = sim_data["beliefs"]
                except Exception:
                    pass
        
        # Look for free energy traces
        fe_dir = output_dir / "free_energy_traces"
        if fe_dir.exists():
            trace_files = list(fe_dir.glob("*.csv"))
            if trace_files:
                data["free_energy_trace_files"] = [str(f.name) for f in trace_files]
                
    except Exception as e:
        logger.warning(f"Error extracting ActiveInference.jl data from files: {e}")
    
    return data


def _extract_discopy_data_from_files(output_dir: Path, logger) -> Dict[str, Any]:
    """Extract DisCoPy simulation data from saved files."""
    data = {}
    
    try:
        # Look for circuit analysis JSON
        analysis_dir = output_dir / "analysis"
        if analysis_dir.exists():
            json_files = list(analysis_dir.glob("*circuit*.json")) + list(analysis_dir.glob("*analysis*.json"))
            for json_file in json_files:
                try:
                    with open(json_file, 'r') as f:
                        circuit_data = json.load(f)
                        if "circuit" in circuit_data:
                            data["circuit"] = circuit_data["circuit"]
                        if "components" in circuit_data:
                            data["components"] = circuit_data["components"]
                except Exception:
                    pass
        
        # Count diagram outputs
        diagram_dir = output_dir / "diagram_outputs"
        if diagram_dir.exists():
            diagram_files = list(diagram_dir.glob("*.png"))
            if diagram_files:
                data["diagram_count"] = len(diagram_files)
                data["diagram_files"] = [str(f.name) for f in diagram_files]
                
    except Exception as e:
        logger.warning(f"Error extracting DisCoPy data from files: {e}")
    
    return data


def _extract_jax_data_from_files(output_dir: Path, logger) -> Dict[str, Any]:
    """Extract JAX simulation data from saved files."""
    # Similar to PyMDP
    return _extract_pymdp_data_from_files(output_dir, logger)


def _extract_simulation_data(stdout: str, stderr: str, framework: str, logger) -> Dict[str, Any]:
    """
    Extract simulation data from execution output.
    
    Args:
        stdout: Standard output from script execution
        stderr: Standard error from script execution
        framework: Framework name
        logger: Logger instance
        
    Returns:
        Dictionary with extracted simulation data
    """
    simulation_data = {
        "traces": [],
        "free_energy": [],
        "states": [],
        "observations": [],
        "actions": [],
        "policy": [],
        "raw_output": stdout[:10000] if stdout else "",  # Limit size
        "raw_error": stderr[:10000] if stderr else ""
    }
    
    try:
        # Framework-specific extraction
        if framework == "pymdp":
            simulation_data.update(_extract_pymdp_data(stdout, stderr))
        elif framework == "rxinfer":
            simulation_data.update(_extract_rxinfer_data(stdout, stderr))
        elif framework == "activeinference_jl":
            simulation_data.update(_extract_activeinference_jl_data(stdout, stderr))
        elif framework == "jax":
            simulation_data.update(_extract_jax_data(stdout, stderr))
        elif framework == "discopy":
            simulation_data.update(_extract_discopy_data(stdout, stderr))
        else:
            # Generic extraction - try to find common patterns
            simulation_data.update(_extract_generic_data(stdout, stderr))
            
    except Exception as e:
        logger.warning(f"Failed to extract simulation data for {framework}: {e}")
    
    return simulation_data


def _extract_pymdp_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract PyMDP-specific simulation data from stdout/stderr."""
    import re
    data = {}
    
    # Combine stdout and stderr for parsing
    combined_output = stdout + "\n" + stderr
    
    # Try to find observations from log lines like "Step 0: obs=2, belief=[...], action=0.0"
    obs_pattern = r'Step\s+\d+:\s+obs=(\d+)'
    obs_matches = re.findall(obs_pattern, combined_output, re.IGNORECASE)
    if obs_matches:
        data["observations"] = [int(obs) for obs in obs_matches]
    
    # Try to find actions from log lines
    action_pattern = r'Step\s+\d+:\s+obs=\d+,\s+belief=[^\]]+,\s+action=([\d.]+)'
    action_matches = re.findall(action_pattern, combined_output, re.IGNORECASE)
    if action_matches:
        data["actions"] = [int(float(act)) for act in action_matches]
    
    # Try to find beliefs from log lines
    belief_pattern = r'belief=\[([^\]]+)\]'
    belief_matches = re.findall(belief_pattern, combined_output, re.IGNORECASE)
    if belief_matches:
        try:
            beliefs = []
            for match in belief_matches:
                # Parse array string like "0.05 0.05 0.9" or "0.05, 0.05, 0.9"
                values = re.findall(r'[\d.]+', match)
                if values:
                    beliefs.append([float(v) for v in values])
            if beliefs:
                data["beliefs"] = beliefs
        except Exception:
            pass
    
    # Try to find state trajectories (legacy pattern)
    state_pattern = r'state[:\s]+\[([^\]]+)\]|states[:\s]+\[([^\]]+)\]'
    state_matches = re.findall(state_pattern, combined_output, re.IGNORECASE)
    if state_matches and "states" not in data:
        data["states"] = [match[0] or match[1] for match in state_matches]
    
    # Try to find observations (legacy pattern)
    if "observations" not in data:
        obs_pattern_legacy = r'observation[:\s]+(\d+)|obs[:\s]+(\d+)'
        obs_matches_legacy = re.findall(obs_pattern_legacy, combined_output, re.IGNORECASE)
        if obs_matches_legacy:
            data["observations"] = [int(match[0] or match[1]) for match in obs_matches_legacy]
    
    # Try to find actions (legacy pattern)
    if "actions" not in data:
        action_pattern_legacy = r'action[:\s]+(\d+)|action_taken[:\s]+(\d+)'
        action_matches_legacy = re.findall(action_pattern_legacy, combined_output, re.IGNORECASE)
        if action_matches_legacy:
            data["actions"] = [int(match[0] or match[1]) for match in action_matches_legacy]
    
    # Try to find free energy
    fe_pattern = r'free[_\s]?energy[:\s]+([\d.]+)|FE[:\s]+([\d.]+)'
    fe_matches = re.findall(fe_pattern, combined_output, re.IGNORECASE)
    if fe_matches:
        data["free_energy"] = [float(match[0] or match[1]) for match in fe_matches]
    
    return data


def _extract_rxinfer_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract RxInfer.jl-specific simulation data."""
    import re
    data = {}
    
    # Try to find posterior distributions
    posterior_pattern = r'posterior[:\s]+\[([^\]]+)\]'
    posterior_matches = re.findall(posterior_pattern, stdout, re.IGNORECASE)
    if posterior_matches:
        data["posterior"] = posterior_matches
    
    return data


def _extract_activeinference_jl_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract ActiveInference.jl-specific simulation data."""
    import re
    data = {}
    
    # Try to find free energy traces
    fe_pattern = r'free[_\s]?energy[:\s]+([\d.]+)|FE[:\s]+([\d.]+)'
    fe_matches = re.findall(fe_pattern, stdout, re.IGNORECASE)
    if fe_matches:
        data["free_energy"] = [float(match[0] or match[1]) for match in fe_matches]
    
    # Try to find state beliefs
    belief_pattern = r'belief[:\s]+\[([^\]]+)\]|q\(s\)[:\s]+\[([^\]]+)\]'
    belief_matches = re.findall(belief_pattern, stdout, re.IGNORECASE)
    if belief_matches:
        data["beliefs"] = [match[0] or match[1] for match in belief_matches]
    
    return data


def _extract_jax_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract JAX-specific simulation data."""
    # Similar to PyMDP but may have different output format
    return _extract_pymdp_data(stdout, stderr)


def _extract_discopy_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Extract DisCoPy-specific simulation data."""
    import re
    data = {}
    
    # Try to find diagram information
    diagram_pattern = r'diagram[:\s]+(\w+)|circuit[:\s]+(\w+)'
    diagram_matches = re.findall(diagram_pattern, stdout, re.IGNORECASE)
    if diagram_matches:
        data["diagrams"] = [match[0] or match[1] for match in diagram_matches]
    
    return data


def _extract_generic_data(stdout: str, stderr: str) -> Dict[str, Any]:
    """Generic extraction for unknown frameworks."""
    import re
    data = {}
    
    # Try to find any numeric arrays or lists
    array_pattern = r'\[([\d.,\s]+)\]'
    array_matches = re.findall(array_pattern, stdout)
    if array_matches:
        data["arrays"] = array_matches[:10]  # Limit to first 10
    
    return data


def generate_execution_report(execution_results: Dict[str, Any], results_dir: Path, logger):
    """
    Generate a comprehensive execution report.
    
    Args:
        execution_results: Dictionary with execution results
        results_dir: Directory to save the report
        logger: Logger instance
    """
    report_file = results_dir / "execution_report.md"
    
    try:
        with open(report_file, 'w') as f:
            f.write("# GNN Script Execution Report\n\n")
            f.write(f"**Generated:** {execution_results['timestamp']}\n")
            f.write(f"**Target Directory:** {execution_results['target_directory']}\n")
            f.write(f"**Output Directory:** {execution_results['output_directory']}\n\n")
            
            f.write("## Summary\n\n")
            f.write(f"- **Total Scripts Found:** {execution_results['total_scripts_found']}\n")
            f.write(f"- **Successful Executions:** {execution_results['successful_executions']}\n")
            f.write(f"- **Failed Executions:** {execution_results['failed_executions']}\n\n")
            
            if execution_results['execution_details']:
                f.write("## Execution Details\n\n")
                
                for detail in execution_results['execution_details']:
                    status = "âœ… SUCCESS" if detail['success'] else "âŒ FAILED"
                    f.write(f"### {detail['script_name']} - {status}\n\n")
                    f.write(f"- **Framework:** {detail['framework']}\n")
                    f.write(f"- **Executor:** {detail['executor']}\n")
                    f.write(f"- **Path:** `{detail['script_path']}`\n")
                    f.write(f"- **Return Code:** {detail.get('return_code', 'N/A')}\n")
                    f.write(f"- **Execution Time:** {detail.get('execution_time', 0):.2f} seconds\n")
                    
                    if not detail['success'] and 'error' in detail:
                        f.write(f"- **Error:** {detail['error']}\n")
                    
                    if 'output_file' in detail:
                        f.write(f"- **Detailed Output:** {detail['output_file']}\n")
                    
                    f.write("\n")
            
            f.write("## Next Steps\n\n")
            if execution_results['failed_executions'] > 0:
                f.write("1. Review failed executions above\n")
                f.write("2. Check individual output files for detailed error information\n")
                f.write("3. Ensure required dependencies are installed\n")
                f.write("4. Verify script syntax and functionality\n\n")
            else:
                f.write("All scripts executed successfully! Check individual output files for results.\n\n")
            
        logger.info(f"Generated execution report: {report_file}")
        
    except Exception as e:
        logger.error(f"Failed to generate execution report: {e}")


def execute_simulation_from_gnn(gnn_file: Path, output_dir: Path) -> Dict[str, Any]:
    """
    Execute simulation from GNN file.
    
    Args:
        gnn_file: Path to GNN file
        output_dir: Output directory
        
    Returns:
        Dictionary with execution results
    """
    try:
        logger.info(f"Executing simulation for {gnn_file}")
        
        # Import execution engine
        from .executor import ExecutionEngine
        
        # Create execution engine
        engine = ExecutionEngine()
        
        # Execute simulation
        result = engine.execute_simulation_from_gnn(gnn_file, output_dir)
        
        return result
        
    except Exception as e:
        logger.error(f"Failed to execute simulation for {gnn_file}: {e}")
        return {
            "success": False,
            "error": str(e)
        }
