#!/usr/bin/env python3
"""
GNN Processing Pipeline - Step 8: Ontology Operations

This script handles ontology-specific operations, such as:
- Loading and parsing ontology files.
- Performing ontology-based analysis or validation.
- Integrating ontological information with GNN models.

Usage:
    python 8_ontology.py [options]
    
Options:
    Same as main.py (though many may not be relevant for this specific step)
"""

import os
import sys
import glob # For finding .md files
from pathlib import Path
import argparse
import datetime
import logging # Import logging

# Logger for this module
logger = logging.getLogger(__name__)

# Attempt to import MCP functionalities from the ontology module
# This assumes 8_ontology.py is in src/ and mcp.py is in src/ontology/
try:
    from ontology import mcp as ontology_mcp
except ImportError:
    # Fallback if the script is run in a context where src/ontology is not directly importable
    # This might happen if PYTHONPATH is not set up as expected for direct script execution.
    try:
        # Adjust path to include src directory if running from src/ or similar context
        sys.path.append(str(Path(__file__).parent.resolve()))
        from ontology import mcp as ontology_mcp
    except ImportError as e:
        logger.error(f"Error: Could not import 'mcp' from src/ontology/mcp.py: {e}")
        logger.error("Ensure src/ontology/mcp.py exists and src/ is discoverable.")
        ontology_mcp = None

def process_ontology_operations(target_dir_str: str, output_dir_str: str, ontology_terms_file: str = None, recursive: bool = False, verbose: bool = False):
    """Processes GNN files to extract, validate, and report on ontology annotations."""
    if not ontology_mcp:
        logger.error("‚ùåüß¨ MCP for ontology (ontology.mcp) not available. Cannot process ontology operations.")
        return False, 0

    # Log initial parameters at DEBUG level
    logger.info(f"  üîé Processing ontology related tasks...")
    logger.debug(f"    üéØ Target GNN files in: {Path(target_dir_str).resolve()}")
    logger.debug(f"    ’•’¨ Output directory for ontology report: {Path(output_dir_str).resolve()}")
    logger.debug(f"    üîÑ Recursive mode: {'Enabled' if recursive else 'Disabled'}")
    if ontology_terms_file:
        logger.debug(f"    üìñ Using ontology terms definition from: {Path(ontology_terms_file).resolve()}")
    else:
        logger.warning("    ‚ö†Ô∏è No ontology terms definition file provided. Validation will be skipped.")

    # Conceptual Ontology Logging (if verbose)
    logger.debug("    üß† Conceptual Note: Ontologies provide a formal way to represent knowledge.")
    logger.debug("      - Informal ontologies (like folksonomies or taxonomies) help organize concepts.")
    logger.debug("      - Formal ontologies (e.g., in OWL, RDF) allow for logical reasoning and consistency checks.")
    logger.debug("      - This script focuses on extracting and validating terms based on a predefined JSON schema.")
    logger.debug("      - Different ontology languages (OWL, RDF, SKOS) offer varying expressiveness.")

    target_dir = Path(target_dir_str)
    output_dir = Path(output_dir_str)
    ontology_output_path = output_dir / "ontology_processing"
    ontology_output_path.mkdir(parents=True, exist_ok=True)
    logger.debug(f"    ‚úçÔ∏è Ontology report will be saved in: {ontology_output_path.resolve()}")

    # Determine project root for relative paths in report
    project_root = Path(__file__).resolve().parent.parent

    defined_ontology_terms = {}
    if ontology_terms_file:
        logger.debug(f"    üßê Loading defined ontology terms from: {ontology_terms_file}")
        defined_ontology_terms = ontology_mcp.load_defined_ontology_terms(ontology_terms_file, verbose=verbose)
        if defined_ontology_terms:
            logger.debug(f"      üìö Loaded {len(defined_ontology_terms)} ontology terms successfully.")
        else:
            logger.warning(f"      ‚ö†Ô∏è Could not load or no terms found in {ontology_terms_file}. Validation may be limited.")

    report_title = "# üß¨ GNN Ontological Annotations Report"
    all_reports_parts = [report_title]
    all_reports_parts.append(f"ÔøΩÔøΩÔ∏è Report Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    # Make paths relative to project root for the report
    try:
        reported_target_dir = target_dir.relative_to(project_root)
    except ValueError:
        reported_target_dir = target_dir # Keep absolute if not under project_root (e.g. symlink or unusual setup)
    all_reports_parts.append(f"üéØ GNN Source Directory: `{reported_target_dir}`")
    
    if ontology_terms_file:
        try:
            reported_ontology_file = Path(ontology_terms_file).resolve().relative_to(project_root)
        except ValueError:
            reported_ontology_file = Path(ontology_terms_file).resolve() # Keep absolute if not under project root
        all_reports_parts.append(f"üìñ Ontology Terms Definition: `{reported_ontology_file}` (Loaded: {len(defined_ontology_terms)} terms)")
    else:
        all_reports_parts.append("‚ö†Ô∏è Ontology Terms Validation: Skipped (no definition file provided)")
    all_reports_parts.append("\n---\n")

    search_pattern = "**/*.md" if recursive else "*.md"
    gnn_files = list(target_dir.glob(search_pattern))

    if not gnn_files:
        logger.info(f"    ‚ÑπÔ∏è No .md files found in '{target_dir}' with search pattern '{search_pattern}'.")
        all_reports_parts.append("**No GNN (.md) files found to process in the specified target directory.**\n")
    else:
        logger.debug(f"    üìä Found {len(gnn_files)} GNN (.md) files to process.")

    processed_file_count = 0
    total_annotations_found = 0
    total_validations_passed = 0
    total_validations_failed = 0

    for gnn_file_path in gnn_files:
        logger.debug(f"    üìÑ Processing file: {gnn_file_path.name} ({gnn_file_path.stat().st_size} bytes)")
        try:
            with open(gnn_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Assuming parse_gnn_ontology_section returns a list/dict of annotations
            parsed_annotations = ontology_mcp.parse_gnn_ontology_section(content, verbose=verbose)
            num_file_annotations = len(parsed_annotations) if parsed_annotations else 0
            total_annotations_found += num_file_annotations
            if num_file_annotations > 0:
                logger.debug(f"      Found {num_file_annotations} ontology annotations in {gnn_file_path.name}.")
            
            validation_results = None
            file_valid = 0
            file_invalid = 0
            if defined_ontology_terms and parsed_annotations:
                validation_results = ontology_mcp.validate_annotations(parsed_annotations, defined_ontology_terms, verbose=verbose)
                if validation_results:
                    # Directly use the counts from the validation_results structure
                    file_valid = len(validation_results.get("valid_mappings", {}))
                    file_invalid = len(validation_results.get("invalid_terms", {}))
                    
                    total_validations_passed += file_valid
                    total_validations_failed += file_invalid
                    if file_valid > 0 or file_invalid > 0:
                        logger.debug(f"        Validated for {gnn_file_path.name}: {file_valid} passed, {file_invalid} failed.")
            
            # The path passed to generate_ontology_report_for_file needs to be relative to project root.
            # Original: str(gnn_file_path.relative_to(target_dir.parent if target_dir.is_dir() else target_dir.parent.parent))
            # gnn_file_path is absolute here. target_dir is also absolute.
            # We want path relative to project_root, e.g. src/gnn/examples/file.md
            try:
                report_file_display_path = gnn_file_path.resolve().relative_to(project_root)
            except ValueError:
                report_file_display_path = gnn_file_path.name # Fallback to just filename if not in project root

            file_report_str = ontology_mcp.generate_ontology_report_for_file(
                str(report_file_display_path),
                parsed_annotations, 
                validation_results
            )
            all_reports_parts.append(file_report_str)
            processed_file_count +=1
        except Exception as e:
            logger.error(f"    ‚ùå Error processing file {gnn_file_path.name}: {e}", exc_info=True)
            all_reports_parts.append(f"### Error processing `{gnn_file_path.name}`\n - {str(e)}\n\n---\n")

    # Add a summary section to the report
    all_reports_parts.insert(1, f"\n## üìä Summary of Ontology Processing\n")
    all_reports_parts.insert(2, f"- **Files Processed:** {processed_file_count} / {len(gnn_files)}")
    all_reports_parts.insert(3, f"- **Total Ontological Annotations Found:** {total_annotations_found}")
    if defined_ontology_terms:
        all_reports_parts.insert(4, f"- **Total Annotations Validated:** {total_validations_passed + total_validations_failed}")
        all_reports_parts.insert(5, f"  - ‚úÖ Passed: {total_validations_passed}")
        all_reports_parts.insert(6, f"  - ‚ùå Failed: {total_validations_failed}")
    all_reports_parts.insert(7, "\n---\n")
            
    report_file_path = ontology_output_path / "ontology_processing_report.md"
    try:
        with open(report_file_path, 'w', encoding='utf-8') as f_report:
            f_report.write("\n".join(all_reports_parts))
        report_size = report_file_path.stat().st_size
        logger.debug(f"  ‚úÖ Ontology processing report saved: {report_file_path.resolve()} ({report_size} bytes)")
    except Exception as e:
        logger.error(f"‚ùå Failed to write ontology report to {report_file_path}: {e}", exc_info=True)
        return False, 0
        
    return True, total_validations_failed

def main(args):
    """Main function for the ontology operations step (Step 8).

    This function is the entry point for ontology processing. It logs the start
    of the step and calls `process_ontology_operations` with the necessary arguments
    derived from the `args` Namespace object.

    Args:
        args (argparse.Namespace): 
            Parsed command-line arguments from `main.py` or standalone execution.
            Expected attributes include: target_dir, output_dir, ontology_terms_file,
            recursive, verbose.
    """
    # Set this script's logger level based on pipeline's args.verbose
    # This is typically handled by main.py for child modules.
    # The __main__ block handles it for standalone execution.
    # if args.verbose:
    #     logger.setLevel(logging.DEBUG)
    # else:
    #     logger.setLevel(logging.INFO)

    logger.info(f"‚ñ∂Ô∏è Starting Step 8: Ontology Operations ({Path(__file__).name})")
    logger.debug(f"  Parsed options (from main.py or standalone):")
    logger.debug(f"    Target GNN files directory: {args.target_dir}")
    logger.debug(f"    Output directory for report: {args.output_dir}")
    logger.debug(f"    Recursive: {args.recursive}")
    if args.ontology_terms_file:
        logger.debug(f"    Ontology terms definition file: {args.ontology_terms_file}")
    else:
        logger.debug(f"    Ontology terms file: Not provided (validation will be skipped)")
    logger.debug(f"    Verbose flag from args: {args.verbose}")

    success, num_failed_validations = process_ontology_operations(
        args.target_dir, 
        args.output_dir, 
        args.ontology_terms_file,
        args.recursive if hasattr(args, 'recursive') else False, 
        args.verbose
    )

    if not success:
        logger.error(f"‚ùå Step 8: Ontology Operations ({Path(__file__).name}) FAILED critically.")
        return 1 # Critical failure
    
    if num_failed_validations > 0:
        warning_message = (
            f"Ontology validation completed with {num_failed_validations} failed term(s). "
            f"Check '{Path(args.output_dir) / 'ontology_processing/ontology_processing_report.md'}' "
            f"for details."
        )
        logger.warning(f"‚ö†Ô∏è Step 8: {warning_message}")
        return {
            "status": "success_with_warnings",
            "summary": f"Ontology: {num_failed_validations} failed term(s).", # Shorter summary for main pipeline
            "warnings": [warning_message] # Detailed message
        }
        
    logger.info(f"‚úÖ Step 8: Ontology Operations ({Path(__file__).name}) - COMPLETED without validation errors.")
    return 0

if __name__ == "__main__":
    # Basic configuration for running this script standalone
    # In a pipeline, main.py should configure logging.
    log_level_str = os.environ.get("LOG_LEVEL", "INFO").upper()
    log_level = getattr(logging, log_level_str, logging.INFO)
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    # Simplified arg parsing for standalone run
    parser = argparse.ArgumentParser(description="Standalone Ontology Processing.")
    parser.add_argument("--target-dir", default="../output/gnn_exports", help="Directory with GNN exports.")
    parser.add_argument("--output-dir", default="../output", help="Output directory for reports.")
    parser.add_argument("--ontology-terms-file", default="act_inf_ontology_terms.json", help="JSON file with defined ontology terms.")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output.")
    parser.add_argument("--recursive", action="store_true", help="Recursively process GNN files in the target directory.")
    
    cli_args = parser.parse_args()
    
    # Update log level if --verbose is used in standalone mode
    if cli_args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        for handler in logging.getLogger().handlers:
            handler.setLevel(logging.DEBUG)
        logger.debug("Verbose logging enabled for standalone run.")

    # Construct the args object that the script's main() expects
    # It seems the script's main expects an object with attributes like args.output_dir, etc.
    # We'll pass the parsed cli_args directly as it should have the necessary attributes.
    result = main(cli_args) # Pass the parsed args directly 
    if isinstance(result, dict): # Handle success with warnings
        if result.get("status") == "success_with_warnings":
            sys.exit(2) # Exit code 2 for warnings
        else:
            sys.exit(1) # Generic error if dict is not as expected
    else:
        sys.exit(result) # Exit with the integer code (0 or 1) 