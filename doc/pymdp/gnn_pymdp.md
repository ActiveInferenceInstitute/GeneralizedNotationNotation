# GNN-PyMDP Integration Guide

## Overview

PyMDP is a Python package for building and simulating Active Inference agents using Partially Observable Markov Decision Processes (POMDPs). This guide explains how GNN models are translated to PyMDP code and how to work with the generated implementations.

## Table of Contents

1. [GNN to PyMDP Translation](#gnn-to-pymdp-translation)
2. [Working with Generated PyMDP Code](#working-with-generated-pymdp-code)
3. [Simulation Examples](#simulation-examples)
4. [Advanced PyMDP Features](#advanced-pymdp-features)
5. [Hierarchical and Multi-Agent Models](#hierarchical-and-multi-agent-models)
6. [Integration Patterns](#integration-patterns)
7. [Performance Optimization](#performance-optimization)
8. [Common Issues and Solutions](#common-issues-and-solutions)
9. [Best Practices](#best-practices)
10. [Troubleshooting](#troubleshooting)

## GNN to PyMDP Translation

### Conceptual Mapping

GNN provides a declarative specification of Active Inference models, while PyMDP implements these models computationally. The translation follows these key mappings:

| GNN Element | PyMDP Equivalent | Description |
|-------------|------------------|-------------|
| `s_f0`, `s_f1`, ... | State factors | Hidden state variables the agent tracks |
| `o_m0`, `o_m1`, ... | Observation modalities | What the agent can perceive |
| `u_c0`, `u_c1`, ... | Control factors | Actions the agent can take |
| `A_m0`, `A_m1`, ... | `A` matrices | Likelihood mappings (observations given states) |
| `B_f0`, `B_f1`, ... | `B` matrices | Transition dynamics (next states given current states and actions) |
| `C_m0`, `C_m1`, ... | `C` vectors | Log preferences over observations |
| `D_f0`, `D_f1`, ... | `D` vectors | Prior beliefs over initial states |

### Matrix Structure Translation

#### A Matrices (Likelihood/Observation Model)
```gnn
# GNN Specification
A_m0 = [[0.9, 0.1], [0.1, 0.9]]  # P(o_m0|s_f0)
```

```python
# Generated PyMDP Code
A = utils.obj_array(1)  # 1 observation modality
A[0] = np.array([[0.9, 0.1],
                 [0.1, 0.9]])
A[0] = utils.norm_dist(A[0])  # Ensure normalization
```

#### B Matrices (Transition/Dynamics Model)
```gnn
# GNN Specification  
B_f0 = [[[0.8, 0.2], [0.3, 0.7]], [[0.1, 0.9], [0.6, 0.4]]]  # P(s_f0'|s_f0,u_c0)
```

```python
# Generated PyMDP Code
B = utils.obj_array(1)  # 1 hidden state factor
B[0] = np.array([[[0.8, 0.2],   # Action 0 transition matrix
                  [0.3, 0.7]],
                 [[0.1, 0.9],   # Action 1 transition matrix
                  [0.6, 0.4]]])
B[0] = utils.norm_dist(B[0])  # Ensure column normalization
```

#### C Vectors (Preferences)
```gnn
# GNN Specification
C_m0 = [2.0, 0.0]  # Log preferences
```

```python
# Generated PyMDP Code
C = utils.obj_array(1)  # 1 observation modality
C[0] = np.array([2.0, 0.0])  # Log preferences (no normalization needed)
```

#### D Vectors (Priors)
```gnn
# GNN Specification
D_f0 = [0.6, 0.4]  # Prior beliefs
```

```python
# Generated PyMDP Code
D = utils.obj_array(1)  # 1 hidden state factor
D[0] = np.array([0.6, 0.4])
D[0] = utils.norm_dist(D[0])  # Ensure normalization
```

## Working with Generated PyMDP Code

### Basic Agent Structure

Generated PyMDP scripts follow this standard structure:

```python
#!/usr/bin/env python3
"""
PyMDP Agent Script - [Model Name]
Generated by GNN PyMDP Renderer
"""

import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Matrix definitions (A, B, C, D)
# ... (generated from GNN spec)

# Agent instantiation
agent = Agent(A=A, B=B, C=C, D=D)

# Example usage
if __name__ == "__main__":
    # Initial observation
    obs = [0]  # Observation for modality 0
    
    # Infer states and select action
    qs = agent.infer_states(obs)
    action = agent.sample_action()
    
    print(f"Inferred states: {qs}")
    print(f"Selected action: {action}")
```

### Advanced Features

#### Planning Horizon
```python
# For models with planning
agent = Agent(A=A, B=B, C=C, D=D, 
              policy_len=5,        # Planning horizon
              use_utility=True,    # Use preference-based planning
              use_states_info_gain=True,  # Information-seeking behavior
              use_param_info_gain=False)  # Parameter learning (if applicable)
```

#### Learning Parameters
```python
# For models with learning
agent = Agent(A=A, B=B, C=C, D=D,
              lr_pA=0.05,  # Learning rate for A matrices
              lr_pB=0.05,  # Learning rate for B matrices
              use_param_info_gain=True)
```

#### Multi-Factor Models
```python
# For complex models with multiple state factors and observation modalities
# A[m][states_0, states_1, ..., states_n] for modality m
# B[f][states_f, states_f, actions] for factor f

A = utils.obj_array(2)  # 2 observation modalities
A[0] = np.random.rand(3, 4, 2)  # 3 obs × 4 states_f0 × 2 states_f1
A[1] = np.random.rand(2, 4, 2)  # 2 obs × 4 states_f0 × 2 states_f1

B = utils.obj_array(2)  # 2 hidden state factors
B[0] = np.random.rand(4, 4, 3)  # 4 states × 4 states × 3 actions
B[1] = np.random.rand(2, 2, 1)  # 2 states × 2 states × 1 action (controllable/uncontrollable)

# Normalize all matrices
for m in range(len(A)):
    A[m] = utils.norm_dist(A[m])
for f in range(len(B)):
    B[f] = utils.norm_dist(B[f])
```

## Simulation Examples

### Basic Perception Task
```python
"""Simple perception without actions"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Define model (from GNN translation)
num_obs = [2]  # 2 possible observations
num_states = [2]  # 2 possible states

A = utils.random_A_matrix(num_obs, num_states)
D = utils.uniform_categorical(num_states)

# Static model (no actions/transitions)
agent = Agent(A=A, D=D)

# Simulate perception
observations = [0, 1, 0, 1, 0]  # Sequence of observations

beliefs_over_time = []
for obs in observations:
    qs = agent.infer_states([obs])
    beliefs_over_time.append(qs[0].copy())
    
print("Belief evolution:")
for t, belief in enumerate(beliefs_over_time):
    print(f"t={t}: {belief}")
```

### Active Inference with Actions
```python
"""Active inference with planning and action selection"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Model dimensions
num_obs = [4]    # 4 possible observations  
num_states = [4] # 4 possible states
num_controls = [4] # 4 possible actions

# Generate model matrices (or use GNN-specified values)
A = utils.random_A_matrix(num_obs, num_states)
B = utils.random_B_matrix(num_states, num_controls)

# Preferences (prefer observation 3)
C = utils.obj_array([np.array([0.0, 0.0, 0.0, 2.0])])
D = utils.uniform_categorical(num_states)

# Agent with planning
agent = Agent(A=A, B=B, C=C, D=D, 
              policy_len=3,  # 3-step planning
              use_utility=True)

# Simulation loop
T = 10  # Number of time steps
obs_seq = []
action_seq = []

for t in range(T):
    # Get observation (from environment)
    obs = [np.random.choice(num_obs[0])]  # Random environment
    obs_seq.append(obs[0])
    
    # Agent inference and action selection
    qs = agent.infer_states(obs)
    q_pi, neg_efe = agent.infer_policies()
    action = agent.sample_action()
    action_seq.append(action[0])
    
    print(f"t={t}: obs={obs[0]}, action={action[0]}, "
          f"state_belief={qs[0].argmax()}")

print(f"Observation sequence: {obs_seq}")
print(f"Action sequence: {action_seq}")
```

### Learning Example
```python
"""Parameter learning during interaction"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Initialize with uncertain A matrix
num_obs = [2]
num_states = [2]  
num_controls = [2]

# Start with flat/uncertain A matrix
A = utils.obj_array([np.ones((2, 2))])  # Uniform likelihood
A[0] = utils.norm_dist(A[0])

B = utils.random_B_matrix(num_states, num_controls)
C = utils.obj_array([np.array([0.0, 1.0])])  # Prefer obs=1
D = utils.uniform_categorical(num_states)

# Agent with learning enabled
agent = Agent(A=A, B=B, C=C, D=D,
              lr_pA=0.1,  # Learning rate for A
              use_param_info_gain=True)

# True environment A matrix (agent doesn't know this)
true_A = np.array([[0.9, 0.1],
                   [0.1, 0.9]])

print("Initial A matrix:")
print(agent.A[0])

# Learning through interaction
for t in range(50):
    # Environment generates observation based on true model
    state = np.random.choice(2)  # Random state
    obs_prob = true_A[:, state]
    obs = [np.random.choice(2, p=obs_prob)]
    
    # Agent learns from observation
    qs = agent.infer_states(obs)
    action = agent.sample_action()
    
    if t % 10 == 0:
        print(f"t={t}, A matrix after learning:")
        print(agent.A[0])

print("Final learned A matrix:")
print(agent.A[0])
print("True A matrix:")
print(true_A)
```

## Advanced PyMDP Features

### Object Arrays and Factorized Distributions

PyMDP uses object arrays (`utils.obj_array`) to handle factorized probability distributions efficiently:

```python
import numpy as np
from pymdp import utils

# Create object array for multiple factors
num_factors = 3
num_states = [4, 3, 2]  # Different dimensionalities per factor

# Initialize factorized prior
D = utils.obj_array(num_factors)
for f in range(num_factors):
    D[f] = utils.uniform_categorical(num_states[f])

# Factorized posterior beliefs
qs = utils.obj_array(num_factors)
for f in range(num_factors):
    qs[f] = np.random.dirichlet(np.ones(num_states[f]))
    qs[f] = utils.norm_dist(qs[f])

print("Factorized beliefs:")
for f, q in enumerate(qs):
    print(f"Factor {f}: {q}")
```

### Advanced Agent Configuration

#### Sophisticated Planning and Inference

```python
from pymdp.agent import Agent
from pymdp import utils

# Advanced agent configuration
agent = Agent(
    A=A, B=B, C=C, D=D,
    
    # Planning parameters
    policy_len=5,              # Planning horizon
    use_utility=True,          # Preference-based planning
    use_states_info_gain=True, # Information-seeking behavior
    use_param_info_gain=True,  # Parameter learning
    
    # Learning rates
    lr_pA=0.05,               # A matrix learning rate
    lr_pB=0.05,               # B matrix learning rate
    lr_pD=0.05,               # D vector learning rate
    
    # Inference algorithm selection
    inference_algo="MMP",      # Message passing algorithm
    policy_sep_prior=False,    # Use separate policy priors
    
    # Action sampling
    action_precision=1.0,      # Action selection precision
    sampling_type="multinomial"  # Action sampling method
)
```

#### Categorical Distributions and Sampling

```python
# Working with categorical distributions
from pymdp.utils import sample, kl_divergence, entropy

# Sample from categorical distribution
cat_dist = np.array([0.1, 0.3, 0.4, 0.2])
samples = [sample(cat_dist) for _ in range(100)]

# Calculate information-theoretic quantities
uniform_dist = utils.uniform_categorical([4])
kl_div = kl_divergence(cat_dist, uniform_dist)
entropy_val = entropy(cat_dist)

print(f"KL divergence: {kl_div}")
print(f"Entropy: {entropy_val}")
```

### Variational Message Passing

```python
# Advanced inference with variational message passing
from pymdp.algos import run_mmp

# Manual message passing for custom control
obs_seq = [[0], [1], [0]]  # Observation sequence
qs_seq, q_pi, neg_efe = run_mmp(
    A, B, C, D,
    obs_seq,
    policy_len=3,
    num_iter=16,  # Number of VMP iterations
    grad_descent=True,
    tau=0.25  # Precision parameter
)

print("Posterior beliefs over time:")
for t, qs in enumerate(qs_seq):
    print(f"t={t}: {qs[0]}")
```

## Hierarchical and Multi-Agent Models

### Hierarchical Active Inference

```python
"""
Implementing hierarchical models with multiple temporal scales
"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

class HierarchicalAgent:
    def __init__(self, high_level_agent, low_level_agent):
        self.high_agent = high_level_agent
        self.low_agent = low_level_agent
        self.current_goal = None
        
    def step(self, observation):
        # High-level agent sets goals/context
        high_obs = self.extract_high_level_obs(observation)
        high_qs = self.high_agent.infer_states(high_obs)
        self.current_goal = self.high_agent.sample_action()
        
        # Low-level agent acts given goal context
        low_obs = self.augment_with_goal(observation, self.current_goal)
        low_qs = self.low_agent.infer_states(low_obs)
        action = self.low_agent.sample_action()
        
        return action, (high_qs, low_qs)
    
    def extract_high_level_obs(self, obs):
        # Extract relevant features for high-level reasoning
        return [obs[0] // 2]  # Example: spatial abstraction
    
    def augment_with_goal(self, obs, goal):
        # Combine observation with goal context
        return [obs[0], goal[0]]

# Example usage
high_A = utils.random_A_matrix([2], [3])  # Abstract observations
high_B = utils.random_B_matrix([3], [2])  # Goal selection
high_C = utils.obj_array([np.array([1.0, 0.0])])  # Goal preferences

low_A = utils.random_A_matrix([4, 2], [8])  # Detailed obs + goal
low_B = utils.random_B_matrix([8], [4])     # Detailed actions
low_C = utils.obj_array([np.array([0.0, 0.0, 1.0, 0.0])])

high_agent = Agent(A=high_A, B=high_B, C=high_C)
low_agent = Agent(A=low_A, B=low_B, C=low_C)

hierarchical_agent = HierarchicalAgent(high_agent, low_agent)
```

### Multi-Agent Coordination

```python
"""
Multi-agent active inference with coordination
"""
class MultiAgentSystem:
    def __init__(self, agents, communication_matrix=None):
        self.agents = agents
        self.comm_matrix = communication_matrix
        self.shared_observations = []
        
    def step(self, observations):
        """Coordinate multiple agents with optional communication"""
        actions = []
        beliefs = []
        
        # Individual inference
        for i, (agent, obs) in enumerate(zip(self.agents, observations)):
            # Augment observation with shared information
            if self.shared_observations:
                augmented_obs = self.combine_observations(obs, i)
            else:
                augmented_obs = obs
                
            qs = agent.infer_states(augmented_obs)
            action = agent.sample_action()
            
            actions.append(action)
            beliefs.append(qs)
        
        # Update shared observations for next step
        if self.comm_matrix is not None:
            self.update_shared_observations(observations, beliefs)
            
        return actions, beliefs
    
    def combine_observations(self, private_obs, agent_idx):
        """Combine private and shared observations"""
        if not self.shared_observations:
            return private_obs
            
        # Simple concatenation (can be more sophisticated)
        shared_info = self.shared_observations[agent_idx]
        return private_obs + [shared_info]
    
    def update_shared_observations(self, observations, beliefs):
        """Update shared information based on communication matrix"""
        self.shared_observations = []
        
        for i in range(len(self.agents)):
            shared_info = 0
            for j in range(len(self.agents)):
                if self.comm_matrix[i, j] > 0:
                    # Share belief about most likely state
                    shared_info += beliefs[j][0].argmax() * self.comm_matrix[i, j]
            
            self.shared_observations.append(int(shared_info))

# Example multi-agent setup
num_agents = 3
agents = []

for i in range(num_agents):
    A = utils.random_A_matrix([4, 2], [6])  # Private + shared obs
    B = utils.random_B_matrix([6], [3])
    C = utils.obj_array([np.random.rand(4, 2)])
    D = utils.uniform_categorical([6])
    
    agents.append(Agent(A=A, B=B, C=C, D=D))

# Communication matrix (who communicates with whom)
comm_matrix = np.array([
    [0.0, 0.5, 0.3],  # Agent 0 receives from agents 1,2
    [0.4, 0.0, 0.6],  # Agent 1 receives from agents 0,2  
    [0.3, 0.7, 0.0]   # Agent 2 receives from agents 0,1
])

multi_agent_system = MultiAgentSystem(agents, comm_matrix)
```

## Integration Patterns

### OpenAI Gym Integration

```python
"""
Enhanced Gym wrapper with proper observation/action space handling
"""
import gym
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

class EnhancedPyMDPGymWrapper:
    def __init__(self, env_name, A, B, C, D, obs_mapping=None, action_mapping=None):
        self.env = gym.make(env_name)
        self.agent = Agent(A=A, B=B, C=C, D=D)
        
        # Observation and action space mappings
        self.obs_mapping = obs_mapping or self.default_obs_mapping
        self.action_mapping = action_mapping or self.default_action_mapping
        
        # Track performance
        self.episode_rewards = []
        self.episode_lengths = []
        
    def default_obs_mapping(self, gym_obs):
        """Default observation mapping for discrete spaces"""
        if isinstance(self.env.observation_space, gym.spaces.Discrete):
            return [gym_obs]
        elif isinstance(self.env.observation_space, gym.spaces.Box):
            # Discretize continuous observations
            return [int(np.digitize(gym_obs[0], bins=np.linspace(-1, 1, 4)))]
        else:
            raise NotImplementedError(f"Observation space {type(self.env.observation_space)} not supported")
    
    def default_action_mapping(self, pymdp_action):
        """Default action mapping for discrete spaces"""
        return pymdp_action[0]
    
    def run_episode(self, max_steps=1000, render=False, verbose=False):
        """Run a single episode with the PyMDP agent"""
        obs = self.env.reset()
        total_reward = 0
        step_count = 0
        
        episode_data = {
            'observations': [],
            'actions': [],
            'rewards': [],
            'beliefs': [],
            'policies': []
        }
        
        for step in range(max_steps):
            if render:
                self.env.render()
                
            # Convert observation and perform inference
            pymdp_obs = self.obs_mapping(obs)
            qs = self.agent.infer_states(pymdp_obs)
            
            # Optional: analyze policies
            if hasattr(self.agent, 'infer_policies'):
                q_pi, neg_efe = self.agent.infer_policies()
                episode_data['policies'].append((q_pi, neg_efe))
            
            # Select action
            action = self.agent.sample_action()
            gym_action = self.action_mapping(action)
            
            # Environment step
            obs, reward, done, info = self.env.step(gym_action)
            
            # Store episode data
            episode_data['observations'].append(pymdp_obs)
            episode_data['actions'].append(action)
            episode_data['rewards'].append(reward)
            episode_data['beliefs'].append([q.copy() for q in qs])
            
            total_reward += reward
            step_count += 1
            
            if verbose:
                print(f"Step {step}: obs={pymdp_obs}, action={action}, reward={reward:.3f}")
                print(f"  Beliefs: {[q.argmax() for q in qs]}")
            
            if done:
                break
        
        self.episode_rewards.append(total_reward)
        self.episode_lengths.append(step_count)
        
        if render:
            self.env.close()
            
        return total_reward, episode_data
    
    def evaluate(self, num_episodes=100, verbose=False):
        """Evaluate agent performance over multiple episodes"""
        results = []
        
        for episode in range(num_episodes):
            reward, data = self.run_episode(verbose=verbose and episode % 10 == 0)
            results.append((reward, data))
            
            if verbose and episode % 10 == 0:
                avg_reward = np.mean(self.episode_rewards[-10:])
                print(f"Episode {episode}: avg_reward={avg_reward:.3f}")
        
        return {
            'mean_reward': np.mean(self.episode_rewards),
            'std_reward': np.std(self.episode_rewards),
            'mean_length': np.mean(self.episode_lengths),
            'episode_data': results
        }

# Example usage with CartPole
def create_cartpole_agent():
    """Create PyMDP agent for CartPole environment"""
    # Discretized state space: [cart_pos, cart_vel, pole_angle, pole_vel]
    # Simplified to position and angle for this example
    num_obs = [4]  # Discretized observations
    num_states = [8]  # Hidden states representing system dynamics
    num_controls = [2]  # Left/right actions
    
    # Create model matrices
    A = utils.random_A_matrix(num_obs, num_states)
    B = utils.random_B_matrix(num_states, num_controls)
    
    # Preferences for balanced pole (middle observations preferred)
    C = utils.obj_array([np.array([0.0, 1.0, 2.0, 1.0])])
    D = utils.uniform_categorical(num_states)
    
    return A, B, C, D

# Run CartPole experiment
A, B, C, D = create_cartpole_agent()
wrapper = EnhancedPyMDPGymWrapper('CartPole-v1', A, B, C, D)
results = wrapper.evaluate(num_episodes=50, verbose=True)

print(f"CartPole Results: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}")
```

### PyTorch Integration

```python
"""
Integration with PyTorch for neural network components
"""
import torch
import torch.nn as nn
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

class NeuralPyMDPAgent:
    def __init__(self, obs_dim, state_dim, action_dim, hidden_dim=64):
        self.obs_dim = obs_dim
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Neural networks for model components
        self.obs_encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, state_dim),
            nn.Softmax(dim=-1)
        )
        
        self.dynamics_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, state_dim),
            nn.Softmax(dim=-1)
        )
        
        # Traditional PyMDP components
        A = utils.random_A_matrix([obs_dim], [state_dim])
        B = utils.random_B_matrix([state_dim], [action_dim])
        C = utils.obj_array([np.zeros(obs_dim)])
        D = utils.uniform_categorical([state_dim])
        
        self.pymdp_agent = Agent(A=A, B=B, C=C, D=D)
        
    def encode_observation(self, raw_obs):
        """Use neural network to encode raw observation to state belief"""
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(raw_obs).unsqueeze(0)
            state_belief = self.obs_encoder(obs_tensor).numpy().flatten()
        return state_belief
    
    def predict_dynamics(self, state, action):
        """Use neural network to predict state transitions"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state)
            action_tensor = torch.FloatTensor(action)
            input_tensor = torch.cat([state_tensor, action_tensor]).unsqueeze(0)
            next_state = self.dynamics_net(input_tensor).numpy().flatten()
        return next_state
    
    def step(self, raw_observation):
        """Hybrid neural-PyMDP inference step"""
        # Neural encoding of observation
        state_belief = self.encode_observation(raw_observation)
        
        # Use encoded belief for PyMDP inference
        # Convert continuous belief to discrete observation
        discrete_obs = [np.argmax(state_belief)]
        
        # Standard PyMDP inference
        qs = self.pymdp_agent.infer_states(discrete_obs)
        action = self.pymdp_agent.sample_action()
        
        return action, qs, state_belief
    
    def train_neural_components(self, experience_buffer):
        """Train neural networks from experience"""
        # Implementation would include:
        # - Loss functions for observation encoding
        # - Dynamics prediction loss
        # - Integration with PyMDP updates
        pass

# Example usage
neural_agent = NeuralPyMDPAgent(obs_dim=4, state_dim=8, action_dim=2)
```

## Enhanced Performance Optimization

### Vectorized Operations

```python
"""
Optimized PyMDP operations using vectorization and caching
"""
import numpy as np
from pymdp import utils
from functools import lru_cache
import numba

@numba.jit(nopython=True)
def fast_dot_product(A, B):
    """Numba-accelerated matrix operations"""
    return np.dot(A, B)

class OptimizedAgent:
    def __init__(self, A, B, C, D, cache_size=1000):
        self.A = A
        self.B = B  
        self.C = C
        self.D = D
        
        # Precompute commonly used quantities
        self.log_A = [np.log(A_m + 1e-16) for A_m in A]
        self.log_B = [np.log(B_f + 1e-16) for B_f in B]
        
        # Set up caching
        self.infer_states_cached = lru_cache(maxsize=cache_size)(self._infer_states)
        
    def _infer_states(self, obs_tuple):
        """Cached state inference (obs must be hashable tuple)"""
        obs = list(obs_tuple)
        # Standard inference implementation
        qs = utils.obj_array(len(self.D))
        
        for f, (A_f, D_f) in enumerate(zip(self.A, self.D)):
            log_likelihood = self.log_A[f][obs[f], :]
            log_prior = np.log(D_f + 1e-16)
            log_posterior = log_likelihood + log_prior
            qs[f] = utils.softmax(log_posterior)
            
        return qs
    
    def infer_states(self, obs):
        """Public interface with caching"""
        obs_tuple = tuple(obs)
        return self.infer_states_cached(obs_tuple)
    
    def batch_inference(self, obs_batch):
        """Vectorized inference for multiple observations"""
        batch_size = len(obs_batch)
        num_factors = len(self.D)
        
        # Prepare batch arrays
        qs_batch = []
        
        for obs in obs_batch:
            qs = self.infer_states(obs)
            qs_batch.append(qs)
            
        return qs_batch

# Memory-efficient large model handling
class SparseModelAgent:
    def __init__(self, A_sparse, B_sparse, C, D):
        """Agent optimized for sparse, large models"""
        from scipy.sparse import csr_matrix
        
        self.A = [csr_matrix(A_m) if hasattr(A_m, 'toarray') else csr_matrix(A_m) 
                  for A_m in A_sparse]
        self.B = [csr_matrix(B_f) if hasattr(B_f, 'toarray') else csr_matrix(B_f)
                  for B_f in B_sparse]
        self.C = C
        self.D = D
        
    def sparse_inference(self, obs):
        """Inference using sparse matrix operations"""
        qs = utils.obj_array(len(self.D))
        
        for f, (A_f, D_f) in enumerate(zip(self.A, self.D)):
            # Sparse matrix operations
            likelihood = A_f[obs[f], :].toarray().flatten()
            posterior = likelihood * D_f
            qs[f] = utils.norm_dist(posterior)
            
        return qs
```

### Parallel Processing

```python
"""
Parallel processing for multi-agent and ensemble methods
"""
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import numpy as np

def run_agent_episode(agent_params):
    """Worker function for parallel agent execution"""
    agent_id, A, B, C, D, obs_sequence = agent_params
    
    from pymdp.agent import Agent
    agent = Agent(A=A, B=B, C=C, D=D)
    
    trajectory = []
    for obs in obs_sequence:
        qs = agent.infer_states(obs)
        action = agent.sample_action()
        trajectory.append((obs, qs, action))
    
    return agent_id, trajectory

class ParallelMultiAgent:
    def __init__(self, agent_configs, num_workers=None):
        self.agent_configs = agent_configs
        self.num_workers = num_workers or mp.cpu_count()
        
    def run_parallel_episodes(self, obs_sequences):
        """Run multiple agents in parallel"""
        # Prepare worker arguments
        worker_args = []
        for i, (A, B, C, D) in enumerate(self.agent_configs):
            worker_args.append((i, A, B, C, D, obs_sequences[i]))
        
        # Execute in parallel
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            results = list(executor.map(run_agent_episode, worker_args))
        
        return dict(results)

# Ensemble learning with parallel execution
class EnsembleAgent:
    def __init__(self, base_agents, ensemble_method='majority_vote'):
        self.agents = base_agents
        self.ensemble_method = ensemble_method
        
    def parallel_inference(self, obs):
        """Run inference across ensemble in parallel"""
        def agent_inference(agent):
            return agent.infer_states(obs)
        
        with ThreadPoolExecutor(max_workers=len(self.agents)) as executor:
            belief_futures = [executor.submit(agent_inference, agent) 
                            for agent in self.agents]
            beliefs = [future.result() for future in belief_futures]
        
        # Combine ensemble predictions
        return self.combine_beliefs(beliefs)
    
    def combine_beliefs(self, beliefs_list):
        """Combine beliefs from ensemble members"""
        if self.ensemble_method == 'majority_vote':
            # Take mode of most likely states
            votes = []
            for beliefs in beliefs_list:
                votes.append([q.argmax() for q in beliefs])
            
            # Find most common vote for each factor
            ensemble_belief = utils.obj_array(len(beliefs_list[0]))
            for f in range(len(ensemble_belief)):
                factor_votes = [vote[f] for vote in votes]
                most_common = max(set(factor_votes), key=factor_votes.count)
                
                # Convert back to probability distribution
                ensemble_belief[f] = np.zeros_like(beliefs_list[0][f])
                ensemble_belief[f][most_common] = 1.0
                
            return ensemble_belief
            
        elif self.ensemble_method == 'average':
            # Average probability distributions
            ensemble_belief = utils.obj_array(len(beliefs_list[0]))
            for f in range(len(ensemble_belief)):
                factor_beliefs = [beliefs[f] for beliefs in beliefs_list]
                ensemble_belief[f] = np.mean(factor_beliefs, axis=0)
                ensemble_belief[f] = utils.norm_dist(ensemble_belief[f])
                
            return ensemble_belief
```

## Enhanced Common Issues and Solutions

### Advanced Debugging Techniques

```python
"""
Comprehensive debugging and validation tools
"""
import numpy as np
from pymdp import utils
import matplotlib.pyplot as plt

class PyMDPDebugger:
    def __init__(self, agent):
        self.agent = agent
        
    def validate_model_matrices(self):
        """Comprehensive model validation"""
        issues = []
        
        # Check A matrices
        for m, A_m in enumerate(self.agent.A):
            # Normalization check
            col_sums = A_m.sum(axis=0)
            if not np.allclose(col_sums, 1.0, atol=1e-6):
                issues.append(f"A[{m}] columns don't sum to 1: {col_sums}")
            
            # Non-negativity check
            if np.any(A_m < 0):
                issues.append(f"A[{m}] contains negative values")
            
            # Dimension consistency
            expected_shape = (self.agent.num_obs[m], np.prod([len(D) for D in self.agent.D]))
            if A_m.shape != expected_shape:
                issues.append(f"A[{m}] shape {A_m.shape} != expected {expected_shape}")
        
        # Check B matrices
        for f, B_f in enumerate(self.agent.B):
            # Normalization check (columns should sum to 1)
            if B_f.ndim == 3:  # Standard case
                for a in range(B_f.shape[2]):
                    col_sums = B_f[:, :, a].sum(axis=0)
                    if not np.allclose(col_sums, 1.0, atol=1e-6):
                        issues.append(f"B[{f}][:,:,{a}] columns don't sum to 1")
        
        # Check C vectors (preferences can be any real values)
        for m, C_m in enumerate(self.agent.C):
            if len(C_m) != self.agent.num_obs[m]:
                issues.append(f"C[{m}] length {len(C_m)} != num_obs[{m}] {self.agent.num_obs[m]}")
        
        # Check D vectors (priors must sum to 1)
        for f, D_f in enumerate(self.agent.D):
            if not np.allclose(D_f.sum(), 1.0, atol=1e-6):
                issues.append(f"D[{f}] doesn't sum to 1: {D_f.sum()}")
        
        return issues
    
    def trace_inference(self, obs, verbose=True):
        """Detailed inference tracing"""
        if verbose:
            print(f"Tracing inference for observation: {obs}")
        
        trace = {
            'observation': obs,
            'likelihood': [],
            'prior': [],
            'posterior': [],
            'steps': []
        }
        
        for f, (A_f, D_f) in enumerate(zip(self.agent.A, self.agent.D)):
            if verbose:
                print(f"\nFactor {f}:")
                print(f"  Prior D[{f}]: {D_f}")
            
            # Calculate likelihood
            if A_f.ndim == 2 and len(self.agent.D) == 1:
                likelihood = A_f[obs[f], :]
            else:
                # Multi-factor case - marginalize appropriately
                likelihood = A_f[obs[f], ...]  # Simplified
            
            if verbose:
                print(f"  Likelihood: {likelihood}")
            
            # Posterior computation
            posterior = likelihood * D_f
            posterior = utils.norm_dist(posterior)
            
            if verbose:
                print(f"  Posterior: {posterior}")
            
            trace['likelihood'].append(likelihood.copy())
            trace['prior'].append(D_f.copy())
            trace['posterior'].append(posterior.copy())
        
        return trace
    
    def visualize_beliefs(self, belief_history, save_path=None):
        """Visualize belief evolution over time"""
        num_factors = len(belief_history[0])
        fig, axes = plt.subplots(num_factors, 1, figsize=(10, 3*num_factors))
        
        if num_factors == 1:
            axes = [axes]
        
        for f in range(num_factors):
            belief_matrix = np.array([beliefs[f] for beliefs in belief_history]).T
            
            im = axes[f].imshow(belief_matrix, aspect='auto', cmap='viridis')
            axes[f].set_title(f'Factor {f} Beliefs Over Time')
            axes[f].set_xlabel('Time Step')
            axes[f].set_ylabel('State')
            plt.colorbar(im, ax=axes[f])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig
    
    def analyze_policy_preferences(self, num_simulations=100):
        """Analyze policy selection patterns"""
        if not hasattr(self.agent, 'infer_policies'):
            print("Agent doesn't support policy inference")
            return None
        
        policy_counts = {}
        
        for _ in range(num_simulations):
            # Random observation
            obs = [np.random.choice(self.agent.num_obs[m]) 
                   for m in range(len(self.agent.num_obs))]
            
            # Infer policies
            qs = self.agent.infer_states(obs)
            q_pi, neg_efe = self.agent.infer_policies()
            
            # Sample policy
            policy_idx = utils.sample(q_pi)
            
            if policy_idx not in policy_counts:
                policy_counts[policy_idx] = 0
            policy_counts[policy_idx] += 1
        
        # Analyze results
        total_sims = sum(policy_counts.values())
        policy_probs = {k: v/total_sims for k, v in policy_counts.items()}
        
        print("Policy Selection Analysis:")
        for policy_idx, prob in sorted(policy_probs.items()):
            print(f"  Policy {policy_idx}: {prob:.3f}")
        
        return policy_counts, policy_probs

# Usage example
debugger = PyMDPDebugger(agent)
issues = debugger.validate_model_matrices()
if issues:
    print("Model validation issues found:")
    for issue in issues:
        print(f"  - {issue}")
```

### Memory and Performance Profiling

```python
"""
Performance profiling and memory management for PyMDP
"""
import time
import psutil
import numpy as np
from memory_profiler import profile
import cProfile
import pstats

class PyMDPProfiler:
    def __init__(self):
        self.timing_data = {}
        self.memory_data = {}
        
    def time_function(self, func, *args, **kwargs):
        """Time a function execution"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        execution_time = end_time - start_time
        memory_used = end_memory - start_memory
        
        func_name = func.__name__
        self.timing_data[func_name] = execution_time
        self.memory_data[func_name] = memory_used
        
        print(f"{func_name}: {execution_time:.4f}s, {memory_used:.2f}MB")
        
        return result
    
    def profile_agent_episode(self, agent, obs_sequence):
        """Profile a complete agent episode"""
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run episode
        for obs in obs_sequence:
            agent.infer_states(obs)
            agent.sample_action()
        
        profiler.disable()
        
        # Analyze results
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # Top 20 functions
        
        return stats
    
    @profile
    def memory_profile_inference(self, agent, obs):
        """Memory profiling for inference"""
        return agent.infer_states(obs)

# Performance benchmarking
def benchmark_agent_sizes():
    """Benchmark performance across different model sizes"""
    from pymdp.agent import Agent
    
    sizes = [
        (2, 2, 2),    # Small
        (4, 4, 4),    # Medium
        (8, 8, 4),    # Large
        (16, 16, 8),  # Very large
    ]
    
    results = {}
    profiler = PyMDPProfiler()
    
    for num_obs, num_states, num_actions in sizes:
        print(f"\nBenchmarking size: obs={num_obs}, states={num_states}, actions={num_actions}")
        
        # Create agent
        A = utils.random_A_matrix([num_obs], [num_states])
        B = utils.random_B_matrix([num_states], [num_actions])
        C = utils.obj_array([np.random.rand(num_obs)])
        D = utils.uniform_categorical([num_states])
        
        agent = Agent(A=A, B=B, C=C, D=D)
        
        # Benchmark inference
        obs = [0]
        inference_time = profiler.time_function(agent.infer_states, obs)
        
        # Benchmark action selection
        action_time = profiler.time_function(agent.sample_action)
        
        results[(num_obs, num_states, num_actions)] = {
            'inference_time': profiler.timing_data['infer_states'],
            'action_time': profiler.timing_data['sample_action'],
            'inference_memory': profiler.memory_data.get('infer_states', 0),
            'action_memory': profiler.memory_data.get('sample_action', 0)
        }
    
    return results

# Memory optimization utilities
def optimize_memory_usage(agent):
    """Apply memory optimizations to PyMDP agent"""
    
    # Convert to float32 if appropriate
    for m, A_m in enumerate(agent.A):
        if A_m.dtype == np.float64:
            agent.A[m] = A_m.astype(np.float32)
    
    for f, B_f in enumerate(agent.B):
        if B_f.dtype == np.float64:
            agent.B[f] = B_f.astype(np.float32)
    
    # Use sparse matrices for large, sparse models
    from scipy.sparse import csr_matrix
    
    for m, A_m in enumerate(agent.A):
        density = np.count_nonzero(A_m) / A_m.size
        if density < 0.1 and A_m.size > 1000:  # Large and sparse
            print(f"Converting A[{m}] to sparse (density: {density:.3f})")
            agent.A[m] = csr_matrix(A_m)
    
    return agent
```

## Best Practices

1. **Start Simple**: Begin with basic perception models before adding actions
2. **Validate Matrices**: Always check that probabilities sum to 1
3. **Use Utilities**: Leverage PyMDP's utility functions for common operations
4. **Test Incrementally**: Validate each component before building complex models
5. **Monitor Performance**: Profile code for large models or long simulations
6. **Document Assumptions**: Clearly note any simplifications made during translation

## References

- [PyMDP Documentation](https://pymdp.readthedocs.io/)
- [Active Inference Tutorial](https://pymdp.readthedocs.io/en/latest/notebooks/active_inference_demo.html)
- [Smith et al. (2022) Active Inference Tutorial](https://doi.org/10.1016/j.jmp.2021.102632)
- [GNN Specification](../gnn/gnn_syntax.md)
- [Matrix Algebra in Active Inference](../gnn/gnn_implementation.md)

## Troubleshooting

For PyMDP-specific issues:
1. Check the [PyMDP GitHub Issues](https://github.com/infer-actively/pymdp/issues)
2. Review [common PyMDP patterns](https://pymdp.readthedocs.io/en/latest/examples.html)
3. Consult the [GNN troubleshooting guide](../troubleshooting/README.md)
4. Post questions in [GNN Discussions](https://github.com/ActiveInferenceInstitute/GeneralizedNotationNotation/discussions)

## Latest PyMDP Developments and Features

### Active Learning and Parameter Inference

Recent versions of PyMDP have enhanced support for active learning where agents actively seek information to improve their model parameters:

```python
"""
Advanced parameter learning with active information seeking
"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Create agent with parameter learning capabilities
agent = Agent(
    A=A, B=B, C=C, D=D,
    
    # Enhanced learning parameters
    lr_pA=0.1,                    # Learning rate for observation model
    lr_pB=0.1,                    # Learning rate for dynamics model
    lr_pD=0.05,                   # Learning rate for priors
    
    # Information-seeking behavior
    use_param_info_gain=True,     # Seek information about parameters
    use_states_info_gain=True,    # Seek information about states
    
    # Advanced inference options
    inference_algo="MMP",         # Message passing algorithm
    policy_sep_prior=True,        # Separate policy priors
    
    # Exploration parameters
    action_precision=4.0,         # Higher precision = less exploration
    temp_factor=1.0              # Temperature for action sampling
)

# Example: Active exploration for model learning
def active_learning_episode(agent, true_environment, num_steps=100):
    """
    Run an episode where the agent actively learns about the environment
    """
    learning_trajectory = {
        'observations': [],
        'actions': [],
        'beliefs': [],
        'model_updates': [],
        'information_gain': []
    }
    
    for t in range(num_steps):
        # Agent's current beliefs about the world
        current_A = [A_m.copy() for A_m in agent.A]
        current_B = [B_f.copy() for B_f in agent.B]
        
        # Generate observation from true environment
        obs = true_environment.get_observation()
        
        # Agent inference with learning
        qs = agent.infer_states(obs)
        
        # Policy inference (considers information gain)
        if hasattr(agent, 'infer_policies'):
            q_pi, neg_efe = agent.infer_policies()
            
            # Calculate expected information gain for each policy
            info_gains = []
            for policy_idx in range(len(q_pi)):
                # Simplified information gain calculation
                expected_ig = -neg_efe[policy_idx] if neg_efe is not None else 0
                info_gains.append(expected_ig)
            
            learning_trajectory['information_gain'].append(info_gains)
        
        # Action selection
        action = agent.sample_action()
        
        # Environment step
        next_state = true_environment.step(action)
        
        # Store trajectory data
        learning_trajectory['observations'].append(obs)
        learning_trajectory['actions'].append(action)
        learning_trajectory['beliefs'].append([q.copy() for q in qs])
        
        # Calculate model changes
        model_change = {
            'A_diff': [np.linalg.norm(agent.A[m] - current_A[m]) for m in range(len(agent.A))],
            'B_diff': [np.linalg.norm(agent.B[f] - current_B[f]) for f in range(len(agent.B))]
        }
        learning_trajectory['model_updates'].append(model_change)
        
        if t % 20 == 0:
            print(f"Step {t}: Model update magnitude: {sum(model_change['A_diff']):.4f}")
    
    return learning_trajectory

# Analyze learning trajectory
def analyze_learning_performance(trajectory):
    """Analyze how well the agent learned during the episode"""
    model_updates = trajectory['model_updates']
    
    # Calculate learning curves
    A_learning_curve = [sum(update['A_diff']) for update in model_updates]
    B_learning_curve = [sum(update['B_diff']) for update in model_updates]
    
    # Information gain over time
    info_gains = trajectory['information_gain']
    avg_info_gain = [np.mean(ig) if ig else 0 for ig in info_gains]
    
    return {
        'A_learning_curve': A_learning_curve,
        'B_learning_curve': B_learning_curve,
        'information_gain_curve': avg_info_gain,
        'final_model_accuracy': A_learning_curve[-1] if A_learning_curve else 0
    }
```

### Sophisticated Planning Algorithms

PyMDP now includes advanced planning capabilities with sophisticated policy optimization:

```python
"""
Advanced planning with sophisticated policy optimization
"""

# Sophisticated planning configuration
sophisticated_agent = Agent(
    A=A, B=B, C=C, D=D,
    
    # Advanced planning parameters
    policy_len=8,                 # Longer planning horizon
    num_iter=32,                  # More VMP iterations
    
    # Planning algorithm selection
    inference_algo="MMP",         # Message passing
    use_utility=True,             # Goal-directed planning
    use_states_info_gain=True,    # Information-seeking
    
    # Policy optimization
    policy_sep_prior=True,        # Separate policy priors
    alpha=1.0,                    # Precision parameter
    beta=1.0,                     # Precision over policies
    
    # Action selection parameters
    action_precision=16.0,        # High precision action selection
    sampling_type="deterministic" # Deterministic action selection
)

def analyze_planning_depth(agent, initial_obs, max_depth=10):
    """
    Analyze how planning depth affects decision quality
    """
    results = {}
    
    for depth in range(1, max_depth + 1):
        # Temporarily set planning depth
        original_policy_len = agent.policy_len
        agent.policy_len = depth
        
        # Run inference
        qs = agent.infer_states(initial_obs)
        q_pi, neg_efe = agent.infer_policies()
        
        # Calculate policy entropy (measure of decision confidence)
        policy_entropy = -np.sum(q_pi * np.log(q_pi + 1e-16))
        
        # Expected free energy of best policy
        best_policy_efe = np.min(neg_efe) if neg_efe is not None else float('inf')
        
        results[depth] = {
            'policy_entropy': policy_entropy,
            'best_policy_efe': best_policy_efe,
            'selected_policy': utils.sample(q_pi)
        }
        
        # Restore original planning depth
        agent.policy_len = original_policy_len
    
    return results

# Example usage
planning_analysis = analyze_planning_depth(sophisticated_agent, [0, 1])
for depth, metrics in planning_analysis.items():
    print(f"Depth {depth}: entropy={metrics['policy_entropy']:.3f}, "
          f"EFE={metrics['best_policy_efe']:.3f}")
```

## Comparative Analysis: PyMDP vs Other Frameworks

### PyMDP vs RxInfer Integration

GNN models can be translated to both PyMDP and RxInfer. Here's a comparative analysis:

```python
"""
Comparative analysis between PyMDP and RxInfer implementations
"""

class FrameworkComparison:
    def __init__(self, gnn_model_spec):
        self.gnn_spec = gnn_model_spec
        self.pymdp_results = None
        self.rxinfer_results = None
        
    def run_pymdp_simulation(self, num_steps=100):
        """Run simulation using PyMDP"""
        from pymdp.agent import Agent
        
        # Convert GNN to PyMDP
        A, B, C, D = self.convert_gnn_to_pymdp()
        
        agent = Agent(A=A, B=B, C=C, D=D)
        
        results = {
            'beliefs_trajectory': [],
            'actions_trajectory': [],
            'computational_time': [],
            'memory_usage': []
        }
        
        import time
        import psutil
        
        for t in range(num_steps):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss
            
            # Simulate observation
            obs = [np.random.choice(len(A[0]))]
            
            # Inference and action
            qs = agent.infer_states(obs)
            action = agent.sample_action()
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss
            
            results['beliefs_trajectory'].append([q.copy() for q in qs])
            results['actions_trajectory'].append(action)
            results['computational_time'].append(end_time - start_time)
            results['memory_usage'].append(end_memory - start_memory)
        
        self.pymdp_results = results
        return results
    
    def run_rxinfer_simulation(self, num_steps=100):
        """Run simulation using RxInfer (conceptual - would need actual RxInfer implementation)"""
        # This would interface with RxInfer.jl via PyCall or similar
        # For demonstration purposes, we'll simulate the interface
        
        results = {
            'beliefs_trajectory': [],
            'actions_trajectory': [],
            'computational_time': [],
            'memory_usage': [],
            'posterior_samples': []
        }
        
        # RxInfer typically provides different capabilities:
        # - Continuous-valued states/observations
        # - Particle filtering
        # - Variational message passing
        # - Automatic differentiation
        
        print("Note: RxInfer simulation would require Julia integration")
        print("This is a placeholder for the actual implementation")
        
        self.rxinfer_results = results
        return results
    
    def compare_frameworks(self):
        """Compare the performance and characteristics of both frameworks"""
        if not self.pymdp_results or not self.rxinfer_results:
            print("Please run both simulations first")
            return None
        
        comparison = {
            'pymdp': {
                'avg_compute_time': np.mean(self.pymdp_results['computational_time']),
                'avg_memory_usage': np.mean(self.pymdp_results['memory_usage']),
                'framework_strengths': [
                    'Discrete state spaces',
                    'POMDP formulation',
                    'Active inference focus',
                    'Python ecosystem integration',
                    'Extensive documentation'
                ],
                'use_cases': [
                    'Discrete decision making',
                    'Cognitive modeling', 
                    'Robotics with discrete actions',
                    'Game-theoretic scenarios'
                ]
            },
            'rxinfer': {
                'framework_strengths': [
                    'Continuous state spaces',
                    'Automatic differentiation',
                    'Scalable inference',
                    'Julia performance',
                    'Advanced message passing'
                ],
                'use_cases': [
                    'Continuous control',
                    'Signal processing',
                    'Large-scale models',
                    'Real-time inference'
                ]
            }
        }
        
        return comparison
    
    def convert_gnn_to_pymdp(self):
        """Convert GNN specification to PyMDP matrices"""
        # Simplified conversion - would parse actual GNN file
        num_obs = [4]
        num_states = [4] 
        num_controls = [2]
        
        A = utils.random_A_matrix(num_obs, num_states)
        B = utils.random_B_matrix(num_states, num_controls)
        C = utils.obj_array([np.array([1.0, 0.5, 0.0, -0.5])])
        D = utils.uniform_categorical(num_states)
        
        return A, B, C, D

# Usage example
# comparison = FrameworkComparison("path/to/gnn/model.md")
# pymdp_results = comparison.run_pymdp_simulation(100)
# rxinfer_results = comparison.run_rxinfer_simulation(100)
# analysis = comparison.compare_frameworks()
```

## Advanced Research Applications

### Cognitive Neuroscience Modeling

PyMDP is increasingly used for modeling cognitive neuroscience phenomena:

```python
"""
Cognitive neuroscience applications using PyMDP
"""

class CognitiveModel:
    def __init__(self, model_type="working_memory"):
        self.model_type = model_type
        self.agent = None
        
    def create_working_memory_model(self):
        """
        Model working memory using hierarchical active inference
        """
        # Working memory model with multiple timescales
        
        # Fast timescale: sensory processing
        fast_A = utils.random_A_matrix([8], [4])  # Sensory observations -> working memory states
        fast_B = utils.random_B_matrix([4], [3])  # Memory updating actions
        fast_C = utils.obj_array([np.array([1.0, 0.8, 0.6, 0.4, 0.2, 0.0, -0.2, -0.4])])
        fast_D = utils.uniform_categorical([4])
        
        # Slow timescale: cognitive control
        slow_A = utils.random_A_matrix([4], [3])  # Working memory states -> control signals
        slow_B = utils.random_B_matrix([3], [2])  # Control actions
        slow_C = utils.obj_array([np.array([1.0, 0.5, 0.0, -0.5])])
        slow_D = utils.uniform_categorical([3])
        
        # Hierarchical agent (simplified)
        self.fast_agent = Agent(A=fast_A, B=fast_B, C=fast_C, D=fast_D)
        self.slow_agent = Agent(A=slow_A, B=slow_B, C=slow_C, D=slow_D)
        
        return self.fast_agent, self.slow_agent
    
    def create_attention_model(self):
        """
        Model selective attention using precision-weighted inference
        """
        # Attention model with precision control
        num_locations = 4  # Spatial locations
        num_features = 3   # Feature types
        
        # Observation model: features at locations
        A = utils.obj_array(2)  # Two modalities: spatial, featural
        A[0] = utils.random_A_matrix([num_locations], [num_locations, num_features])
        A[1] = utils.random_A_matrix([num_features], [num_locations, num_features])
        
        # Transition model: attention shifts
        B = utils.obj_array(2)
        B[0] = utils.random_B_matrix([num_locations], [4])  # Spatial attention shifts
        B[1] = utils.random_B_matrix([num_features], [1])   # Feature attention (fixed)
        
        # Preferences: attend to relevant features
        C = utils.obj_array(2)
        C[0] = np.array([0.0, 0.0, 0.0, 0.0])  # No spatial preference
        C[1] = np.array([2.0, 1.0, 0.0])       # Prefer first feature type
        
        # Priors: start with diffuse attention
        D = utils.obj_array(2)
        D[0] = utils.uniform_categorical([num_locations])
        D[1] = utils.uniform_categorical([num_features])
        
        # Agent with precision control
        attention_agent = Agent(
            A=A, B=B, C=C, D=D,
            policy_len=3,
            use_utility=True,
            use_states_info_gain=True,
            action_precision=8.0  # High precision for focused attention
        )
        
        self.agent = attention_agent
        return attention_agent
    
    def simulate_cognitive_task(self, task_type="n_back", num_trials=50):
        """
        Simulate cognitive tasks using the active inference framework
        """
        if task_type == "n_back":
            return self.simulate_n_back_task(num_trials)
        elif task_type == "attention":
            return self.simulate_attention_task(num_trials)
        else:
            raise ValueError(f"Unknown task type: {task_type}")
    
    def simulate_n_back_task(self, num_trials):
        """
        Simulate n-back working memory task
        """
        if not hasattr(self, 'fast_agent'):
            self.create_working_memory_model()
        
        results = {
            'accuracy': [],
            'reaction_times': [],
            'working_memory_load': []
        }
        
        # Simplified n-back simulation
        stimulus_sequence = np.random.choice(4, size=num_trials)
        n_back = 2  # 2-back task
        
        for trial in range(num_trials):
            current_stimulus = stimulus_sequence[trial]
            
            # Target: stimulus matches n trials back
            target = (trial >= n_back and 
                     stimulus_sequence[trial] == stimulus_sequence[trial - n_back])
            
            # Agent observes stimulus
            obs = [current_stimulus]
            qs = self.fast_agent.infer_states(obs)
            action = self.fast_agent.sample_action()
            
            # Simplified response mapping
            response = action[0] > 0.5  # Binary response
            
            # Calculate accuracy and reaction time
            accuracy = response == target
            reaction_time = np.random.normal(0.6, 0.1)  # Simulated RT
            
            # Working memory load (entropy of beliefs)
            wm_load = -np.sum(qs[0] * np.log(qs[0] + 1e-16))
            
            results['accuracy'].append(accuracy)
            results['reaction_times'].append(reaction_time)
            results['working_memory_load'].append(wm_load)
        
        return results

# Example usage
cognitive_model = CognitiveModel()
attention_agent = cognitive_model.create_attention_model()
n_back_results = cognitive_model.simulate_cognitive_task("n_back", 100)

print(f"N-back accuracy: {np.mean(n_back_results['accuracy']):.3f}")
print(f"Average RT: {np.mean(n_back_results['reaction_times']):.3f}s")
print(f"Average WM load: {np.mean(n_back_results['working_memory_load']):.3f}")
```

### Social Cognition and Theory of Mind

```python
"""
Modeling social cognition and theory of mind using PyMDP
"""

class SocialCognitionModel:
    def __init__(self):
        self.self_agent = None
        self.other_agent = None
        self.meta_agent = None  # Models the other agent
        
    def create_theory_of_mind_model(self):
        """
        Create agents capable of modeling other agents (theory of mind)
        """
        # Self agent
        self_A = utils.random_A_matrix([6], [4])  # Observations about environment
        self_B = utils.random_B_matrix([4], [3])  # Actions in environment
        self_C = utils.obj_array([np.array([2.0, 1.0, 0.0, -1.0, -2.0, -3.0])])
        self_D = utils.uniform_categorical([4])
        
        self.self_agent = Agent(A=self_A, B=self_B, C=self_C, D=self_D)
        
        # Other agent (with different preferences)
        other_C = utils.obj_array([np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])])  # Opposite preferences
        self.other_agent = Agent(A=self_A, B=self_B, C=other_C, D=self_D)
        
        # Meta-agent: models the other agent
        # Observations include: environment state + other's actions
        meta_A = utils.random_A_matrix([6, 3], [4, 4])  # Environment + other's actions -> joint state
        meta_B = utils.random_B_matrix([4, 4], [3])     # Predicting joint dynamics
        meta_C = utils.obj_array([np.zeros((6, 3))])    # No direct preferences over joint observations
        meta_D = utils.obj_array([utils.uniform_categorical([4]), utils.uniform_categorical([4])])
        
        self.meta_agent = Agent(A=meta_A, B=meta_B, C=meta_C, D=meta_D)
        
        return self.self_agent, self.other_agent, self.meta_agent
    
    def simulate_social_interaction(self, num_steps=50):
        """
        Simulate interaction between agents with theory of mind
        """
        if not all([self.self_agent, self.other_agent, self.meta_agent]):
            self.create_theory_of_mind_model()
        
        interaction_log = {
            'self_actions': [],
            'other_actions': [],
            'self_beliefs': [],
            'meta_beliefs': [],  # Self's beliefs about other
            'cooperation_score': []
        }
        
        for step in range(num_steps):
            # Generate shared observation
            shared_obs = [np.random.choice(6)]
            
            # Other agent acts first (self observes)
            other_qs = self.other_agent.infer_states(shared_obs)
            other_action = self.other_agent.sample_action()
            
            # Self agent observes environment + other's action
            self_obs = shared_obs
            self_qs = self.self_agent.infer_states(self_obs)
            
            # Meta-agent (self's model of other) processes joint observation
            joint_obs = [shared_obs[0], other_action[0]]
            meta_qs = self.meta_agent.infer_states(joint_obs)
            
            # Self agent acts based on environment and model of other
            self_action = self.self_agent.sample_action()
            
            # Calculate cooperation (simplified)
            cooperation = 1.0 if self_action[0] == other_action[0] else 0.0
            
            # Store interaction data
            interaction_log['self_actions'].append(self_action)
            interaction_log['other_actions'].append(other_action)
            interaction_log['self_beliefs'].append([q.copy() for q in self_qs])
            interaction_log['meta_beliefs'].append([q.copy() for q in meta_qs])
            interaction_log['cooperation_score'].append(cooperation)
        
        return interaction_log
    
    def analyze_theory_of_mind_performance(self, interaction_log):
        """
        Analyze how well the agent models the other agent
        """
        # Predict other's actions based on meta-beliefs
        predicted_actions = []
        actual_actions = interaction_log['other_actions']
        
        for meta_belief in interaction_log['meta_beliefs']:
            # Simplified prediction based on most likely state
            predicted_state = np.argmax(meta_belief[1])  # Other's state factor
            predicted_action = predicted_state  # Simplified mapping
            predicted_actions.append(predicted_action)
        
        # Calculate prediction accuracy
        accuracy = np.mean([pred == actual[0] for pred, actual in 
                           zip(predicted_actions, actual_actions)])
        
        # Calculate cooperation rate
        cooperation_rate = np.mean(interaction_log['cooperation_score'])
        
        return {
            'prediction_accuracy': accuracy,
            'cooperation_rate': cooperation_rate,
            'avg_self_uncertainty': np.mean([entropy(belief[0]) for belief in interaction_log['self_beliefs']]),
            'avg_meta_uncertainty': np.mean([entropy(belief[0]) for belief in interaction_log['meta_beliefs']])
        }

# Example usage
social_model = SocialCognitionModel()
self_agent, other_agent, meta_agent = social_model.create_theory_of_mind_model()
interaction_log = social_model.simulate_social_interaction(100)
tom_analysis = social_model.analyze_theory_of_mind_performance(interaction_log)

print(f"Theory of Mind Analysis:")
print(f"  Prediction accuracy: {tom_analysis['prediction_accuracy']:.3f}")
print(f"  Cooperation rate: {tom_analysis['cooperation_rate']:.3f}")
print(f"  Self uncertainty: {tom_analysis['avg_self_uncertainty']:.3f}")
print(f"  Meta uncertainty: {tom_analysis['avg_meta_uncertainty']:.3f}")
```

## Integration with Modern ML Frameworks

### PyMDP + Transformers

```python
"""
Integration of PyMDP with transformer architectures for cognitive modeling
"""
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class TransformerPyMDPAgent:
    def __init__(self, model_name="bert-base-uncased", state_dim=64, action_dim=8):
        # Transformer for processing complex observations
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.transformer = AutoModel.from_pretrained(model_name)
        
        # Neural networks to interface with PyMDP
        self.obs_processor = nn.Sequential(
            nn.Linear(768, 256),  # BERT hidden size to intermediate
            nn.ReLU(),
            nn.Linear(256, state_dim),
            nn.Softmax(dim=-1)
        )
        
        # Traditional PyMDP components
        A = utils.random_A_matrix([state_dim], [state_dim])
        B = utils.random_B_matrix([state_dim], [action_dim])
        C = utils.obj_array([np.zeros(state_dim)])
        D = utils.uniform_categorical([state_dim])
        
        self.pymdp_agent = Agent(A=A, B=B, C=C, D=D,
                                use_utility=True,
                                use_states_info_gain=True)
        
    def process_text_observation(self, text):
        """Process text observation through transformer"""
        # Tokenize and encode
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        
        with torch.no_grad():
            # Get transformer embeddings
            outputs = self.transformer(**inputs)
            pooled_output = outputs.pooler_output  # [CLS] token representation
            
            # Convert to PyMDP observation space
            observation_probs = self.obs_processor(pooled_output)
            
        return observation_probs.numpy().flatten()
    
    def step(self, text_observation):
        """Process text observation and take action"""
        # Convert text to observation probabilities
        obs_probs = self.process_text_observation(text_observation)
        
        # Use most likely observation for PyMDP
        discrete_obs = [np.argmax(obs_probs)]
        
        # PyMDP inference and action selection
        qs = self.pymdp_agent.infer_states(discrete_obs)
        action = self.pymdp_agent.sample_action()
        
        return action, qs, obs_probs

# Example usage
transformer_agent = TransformerPyMDPAgent()

# Process different text observations
text_observations = [
    "The weather is sunny and warm today.",
    "There seems to be a problem with the system.",
    "Everything is working perfectly fine.",
    "I need help with this task."
]

for text in text_observations:
    action, beliefs, obs_probs = transformer_agent.step(text)
    print(f"Text: '{text}'")
    print(f"  Action: {action}")
    print(f"  Most likely state: {np.argmax(beliefs[0])}")
    print(f"  Observation entropy: {-np.sum(obs_probs * np.log(obs_probs + 1e-16)):.3f}")
    print() 