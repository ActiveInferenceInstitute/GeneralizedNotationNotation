# GNN-PyMDP Integration Guide

## Overview

PyMDP is a Python package for building and simulating Active Inference agents using Partially Observable Markov Decision Processes (POMDPs). This guide explains how GNN models are translated to PyMDP code and how to work with the generated implementations.

## GNN to PyMDP Translation

### Conceptual Mapping

GNN provides a declarative specification of Active Inference models, while PyMDP implements these models computationally. The translation follows these key mappings:

| GNN Element | PyMDP Equivalent | Description |
|-------------|------------------|-------------|
| `s_f0`, `s_f1`, ... | State factors | Hidden state variables the agent tracks |
| `o_m0`, `o_m1`, ... | Observation modalities | What the agent can perceive |
| `u_c0`, `u_c1`, ... | Control factors | Actions the agent can take |
| `A_m0`, `A_m1`, ... | `A` matrices | Likelihood mappings (observations given states) |
| `B_f0`, `B_f1`, ... | `B` matrices | Transition dynamics (next states given current states and actions) |
| `C_m0`, `C_m1`, ... | `C` vectors | Log preferences over observations |
| `D_f0`, `D_f1`, ... | `D` vectors | Prior beliefs over initial states |

### Matrix Structure Translation

#### A Matrices (Likelihood/Observation Model)
```gnn
# GNN Specification
A_m0 = [[0.9, 0.1], [0.1, 0.9]]  # P(o_m0|s_f0)
```

```python
# Generated PyMDP Code
A = utils.obj_array(1)  # 1 observation modality
A[0] = np.array([[0.9, 0.1],
                 [0.1, 0.9]])
A[0] = utils.norm_dist(A[0])  # Ensure normalization
```

#### B Matrices (Transition/Dynamics Model)
```gnn
# GNN Specification  
B_f0 = [[[0.8, 0.2], [0.3, 0.7]], [[0.1, 0.9], [0.6, 0.4]]]  # P(s_f0'|s_f0,u_c0)
```

```python
# Generated PyMDP Code
B = utils.obj_array(1)  # 1 hidden state factor
B[0] = np.array([[[0.8, 0.2],   # Action 0 transition matrix
                  [0.3, 0.7]],
                 [[0.1, 0.9],   # Action 1 transition matrix
                  [0.6, 0.4]]])
B[0] = utils.norm_dist(B[0])  # Ensure column normalization
```

#### C Vectors (Preferences)
```gnn
# GNN Specification
C_m0 = [2.0, 0.0]  # Log preferences
```

```python
# Generated PyMDP Code
C = utils.obj_array(1)  # 1 observation modality
C[0] = np.array([2.0, 0.0])  # Log preferences (no normalization needed)
```

#### D Vectors (Priors)
```gnn
# GNN Specification
D_f0 = [0.6, 0.4]  # Prior beliefs
```

```python
# Generated PyMDP Code
D = utils.obj_array(1)  # 1 hidden state factor
D[0] = np.array([0.6, 0.4])
D[0] = utils.norm_dist(D[0])  # Ensure normalization
```

## Working with Generated PyMDP Code

### Basic Agent Structure

Generated PyMDP scripts follow this standard structure:

```python
#!/usr/bin/env python3
"""
PyMDP Agent Script - [Model Name]
Generated by GNN PyMDP Renderer
"""

import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Matrix definitions (A, B, C, D)
# ... (generated from GNN spec)

# Agent instantiation
agent = Agent(A=A, B=B, C=C, D=D)

# Example usage
if __name__ == "__main__":
    # Initial observation
    obs = [0]  # Observation for modality 0
    
    # Infer states and select action
    qs = agent.infer_states(obs)
    action = agent.sample_action()
    
    print(f"Inferred states: {qs}")
    print(f"Selected action: {action}")
```

### Advanced Features

#### Planning Horizon
```python
# For models with planning
agent = Agent(A=A, B=B, C=C, D=D, 
              policy_len=5,        # Planning horizon
              use_utility=True,    # Use preference-based planning
              use_states_info_gain=True,  # Information-seeking behavior
              use_param_info_gain=False)  # Parameter learning (if applicable)
```

#### Learning Parameters
```python
# For models with learning
agent = Agent(A=A, B=B, C=C, D=D,
              lr_pA=0.05,  # Learning rate for A matrices
              lr_pB=0.05,  # Learning rate for B matrices
              use_param_info_gain=True)
```

#### Multi-Factor Models
```python
# For complex models with multiple state factors and observation modalities
# A[m][states_0, states_1, ..., states_n] for modality m
# B[f][states_f, states_f, actions] for factor f

A = utils.obj_array(2)  # 2 observation modalities
A[0] = np.random.rand(3, 4, 2)  # 3 obs × 4 states_f0 × 2 states_f1
A[1] = np.random.rand(2, 4, 2)  # 2 obs × 4 states_f0 × 2 states_f1

B = utils.obj_array(2)  # 2 hidden state factors
B[0] = np.random.rand(4, 4, 3)  # 4 states × 4 states × 3 actions
B[1] = np.random.rand(2, 2, 1)  # 2 states × 2 states × 1 action (controllable/uncontrollable)

# Normalize all matrices
for m in range(len(A)):
    A[m] = utils.norm_dist(A[m])
for f in range(len(B)):
    B[f] = utils.norm_dist(B[f])
```

## Simulation Examples

### Basic Perception Task
```python
"""Simple perception without actions"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Define model (from GNN translation)
num_obs = [2]  # 2 possible observations
num_states = [2]  # 2 possible states

A = utils.random_A_matrix(num_obs, num_states)
D = utils.uniform_categorical(num_states)

# Static model (no actions/transitions)
agent = Agent(A=A, D=D)

# Simulate perception
observations = [0, 1, 0, 1, 0]  # Sequence of observations

beliefs_over_time = []
for obs in observations:
    qs = agent.infer_states([obs])
    beliefs_over_time.append(qs[0].copy())
    
print("Belief evolution:")
for t, belief in enumerate(beliefs_over_time):
    print(f"t={t}: {belief}")
```

### Active Inference with Actions
```python
"""Active inference with planning and action selection"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Model dimensions
num_obs = [4]    # 4 possible observations  
num_states = [4] # 4 possible states
num_controls = [4] # 4 possible actions

# Generate model matrices (or use GNN-specified values)
A = utils.random_A_matrix(num_obs, num_states)
B = utils.random_B_matrix(num_states, num_controls)

# Preferences (prefer observation 3)
C = utils.obj_array([np.array([0.0, 0.0, 0.0, 2.0])])
D = utils.uniform_categorical(num_states)

# Agent with planning
agent = Agent(A=A, B=B, C=C, D=D, 
              policy_len=3,  # 3-step planning
              use_utility=True)

# Simulation loop
T = 10  # Number of time steps
obs_seq = []
action_seq = []

for t in range(T):
    # Get observation (from environment)
    obs = [np.random.choice(num_obs[0])]  # Random environment
    obs_seq.append(obs[0])
    
    # Agent inference and action selection
    qs = agent.infer_states(obs)
    q_pi, neg_efe = agent.infer_policies()
    action = agent.sample_action()
    action_seq.append(action[0])
    
    print(f"t={t}: obs={obs[0]}, action={action[0]}, "
          f"state_belief={qs[0].argmax()}")

print(f"Observation sequence: {obs_seq}")
print(f"Action sequence: {action_seq}")
```

### Learning Example
```python
"""Parameter learning during interaction"""
import numpy as np
from pymdp.agent import Agent
from pymdp import utils

# Initialize with uncertain A matrix
num_obs = [2]
num_states = [2]  
num_controls = [2]

# Start with flat/uncertain A matrix
A = utils.obj_array([np.ones((2, 2))])  # Uniform likelihood
A[0] = utils.norm_dist(A[0])

B = utils.random_B_matrix(num_states, num_controls)
C = utils.obj_array([np.array([0.0, 1.0])])  # Prefer obs=1
D = utils.uniform_categorical(num_states)

# Agent with learning enabled
agent = Agent(A=A, B=B, C=C, D=D,
              lr_pA=0.1,  # Learning rate for A
              use_param_info_gain=True)

# True environment A matrix (agent doesn't know this)
true_A = np.array([[0.9, 0.1],
                   [0.1, 0.9]])

print("Initial A matrix:")
print(agent.A[0])

# Learning through interaction
for t in range(50):
    # Environment generates observation based on true model
    state = np.random.choice(2)  # Random state
    obs_prob = true_A[:, state]
    obs = [np.random.choice(2, p=obs_prob)]
    
    # Agent learns from observation
    qs = agent.infer_states(obs)
    action = agent.sample_action()
    
    if t % 10 == 0:
        print(f"t={t}, A matrix after learning:")
        print(agent.A[0])

print("Final learned A matrix:")
print(agent.A[0])
print("True A matrix:")
print(true_A)
```

## Common Issues and Solutions

### Matrix Dimension Errors
```python
# Problem: Dimension mismatch between GNN spec and PyMDP requirements
# Solution: Check A matrix dimensions match observation and state spaces

# Correct A matrix shape for 1 modality, 1 factor:
# A[0].shape = (num_obs[0], num_states[0])

# For multiple factors:
# A[0].shape = (num_obs[0], num_states[0], num_states[1], ...)
```

### Normalization Issues
```python
# Problem: Matrices not properly normalized
# Solution: Always use utils.norm_dist()

# For A matrices (columns must sum to 1):
A[0] = utils.norm_dist(A[0])

# For B matrices (columns must sum to 1):  
B[0] = utils.norm_dist(B[0])

# For D vectors (must sum to 1):
D[0] = utils.norm_dist(D[0])
```

### Planning with No Actions
```python
# Problem: Agent tries to plan but model has no B matrices
# Solution: Set inference_algo appropriately

# For perception-only models:
agent = Agent(A=A, D=D, inference_algo="VANILLA")

# For active inference models:
agent = Agent(A=A, B=B, C=C, D=D, inference_algo="MMP")
```

## Performance Optimization

### Efficient Matrix Operations
```python
# Use sparse matrices for large, mostly zero matrices
from scipy.sparse import csr_matrix

# Convert dense to sparse if beneficial
if np.count_nonzero(A[0]) < 0.1 * A[0].size:
    A[0] = csr_matrix(A[0])
```

### Batch Processing
```python
# Process multiple observations efficiently
batch_obs = [[0], [1], [0], [1]]  # List of observations
batch_qs = []

for obs in batch_obs:
    qs = agent.infer_states(obs)
    batch_qs.append(qs)

# Or use vectorized operations where possible
```

### Memory Management
```python
# For long simulations, manage memory usage
import gc

# Clear unnecessary variables
del large_matrix
gc.collect()

# Use smaller data types if precision allows
A[0] = A[0].astype(np.float32)  # Instead of float64
```

## Integration with Other Frameworks

### Gym Environment Interface
```python
import gym
from pymdp.agent import Agent

class PyMDPGymWrapper:
    def __init__(self, agent, env):
        self.agent = agent
        self.env = env
        
    def run_episode(self, max_steps=100):
        obs = self.env.reset()
        total_reward = 0
        
        for step in range(max_steps):
            # Convert gym obs to PyMDP format
            pymdp_obs = self.gym_to_pymdp_obs(obs)
            
            # PyMDP inference and action selection
            qs = self.agent.infer_states(pymdp_obs)
            action = self.agent.sample_action()
            
            # Convert PyMDP action to gym format
            gym_action = self.pymdp_to_gym_action(action)
            
            obs, reward, done, info = self.env.step(gym_action)
            total_reward += reward
            
            if done:
                break
                
        return total_reward
    
    def gym_to_pymdp_obs(self, gym_obs):
        # Convert gym observation to PyMDP observation format
        # Implementation depends on specific observation spaces
        return [gym_obs]
    
    def pymdp_to_gym_action(self, pymdp_action):
        # Convert PyMDP action to gym action format
        return pymdp_action[0]
```

### RxInfer Integration
```python
# While PyMDP and RxInfer target different use cases,
# models can be compared by translating results

def compare_pymdp_rxinfer_results(gnn_model_path):
    """Compare PyMDP and RxInfer results for the same GNN model"""
    
    # Load GNN model
    # Generate PyMDP code
    # Generate RxInfer code
    # Run both and compare results
    
    pass  # Implementation would depend on specific comparison metrics
```

## Best Practices

1. **Start Simple**: Begin with basic perception models before adding actions
2. **Validate Matrices**: Always check that probabilities sum to 1
3. **Use Utilities**: Leverage PyMDP's utility functions for common operations
4. **Test Incrementally**: Validate each component before building complex models
5. **Monitor Performance**: Profile code for large models or long simulations
6. **Document Assumptions**: Clearly note any simplifications made during translation

## References

- [PyMDP Documentation](https://pymdp.readthedocs.io/)
- [Active Inference Tutorial](https://pymdp.readthedocs.io/en/latest/notebooks/active_inference_demo.html)
- [Smith et al. (2022) Active Inference Tutorial](https://doi.org/10.1016/j.jmp.2021.102632)
- [GNN Specification](../gnn/gnn_syntax.md)
- [Matrix Algebra in Active Inference](../gnn/gnn_implementation.md)

## Troubleshooting

For PyMDP-specific issues:
1. Check the [PyMDP GitHub Issues](https://github.com/infer-actively/pymdp/issues)
2. Review [common PyMDP patterns](https://pymdp.readthedocs.io/en/latest/examples.html)
3. Consult the [GNN troubleshooting guide](../troubleshooting/README.md)
4. Post questions in [GNN Discussions](https://github.com/ActiveInferenceInstitute/GeneralizedNotationNotation/discussions) 