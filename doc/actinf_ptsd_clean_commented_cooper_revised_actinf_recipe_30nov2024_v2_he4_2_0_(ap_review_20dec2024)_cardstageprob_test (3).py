# -*- coding: utf-8 -*-
"""ActInf PTSD clean-commented - Cooper - Revised ActInf Recipe 30nov2024 v2_HE4_2_0 (AP review 20DEC2024)_CardStageProb_test

Author: Andrew Blake Pashea - github.com/apashea
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ma-c9d67igZRbT9_dG0mhzEEdwx3YPZE
"""

# Install pymdp

! python --version
! pip install inferactively-pymdp

# # Alternative: directly pull from github (sophisticated inference and other updates)
# !git clone https://github.com/infer-actively/pymdp.git
# %cd pymdp
# !pip install .

# Library imports

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pymdp
from pymdp import utils
from pymdp.maths import softmax
from pymdp.agent import Agent
from pymdp import control
import copy
import math
import random

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# def plot_likelihood(matrix, title_str = "Likelihood distribution (A)"):
#     """
#     Plots a 2-D likelihood matrix as a heatmap
#     """

#     if not np.isclose(matrix.sum(axis=0), 1.0).all():
#       raise ValueError("Distribution not column-normalized! Please normalize (ensure matrix.sum(axis=0) == 1.0 for all columns)")

#     fig = plt.figure(figsize = (6,6))
#     ax = sns.heatmap(matrix, cmap = 'gray', cbar = False, vmin = 0.0, vmax = 1.0)
#     plt.title(title_str)
#     plt.show()


# def plot_beliefs(belief_dist, title_str=""):
#     """
#     Plot a categorical distribution or belief distribution, stored in the 1-D numpy vector `belief_dist`
#     """

#     if not np.isclose(belief_dist.sum(), 1.0):
#       raise ValueError("Distribution not normalized! Please normalize")

#     plt.grid(zorder=0)
#     plt.bar(range(belief_dist.shape[0]), belief_dist, color='r', zorder=3)
#     plt.xticks(range(belief_dist.shape[0]))
#     plt.title(title_str)
#     plt.ylim(0,1)
#     plt.show()

# Level 1 of the hierarchical model: defining hidden states, observations, and actions

# Define hidden state factors with respective hidden states
trustworthiness_states = ['trust','distrust'] # Hidden state factor denoting beliefs about advisor's trustworthiness, used in B[0] and D[0] and A matrix
correct_card_states = ['blue','green'] # Hidden state factor denoting beliefs about the correct color of the card, used in B[1] and D[1] and A matrix
affect_states = ['angry','calm']     # Hidden state factor denoting beliefs about affective state, used in B[2] and D[2] and A matrix
choice_states = ['blue','green','null']   # Hidden state factor denoting belief about the action the agent took (if they chose the blue card, green card, or took no action), used in B[3] and D[3] and A matrix
stage_states = ['null','advice','decision']   # Hidden state factor denoting beliefs about the current stage of the trial, used in B[4] and D[4] and A matrix
num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]
num_factors = len(num_states)

# Define observation modalities with respective discrete observations
advice_obs = ['blue','green','null']                # Advice modality, used in A[0] and C[0]
feedback_obs = ['correct','incorrect','null']       # Feedback modality, used in A[1] and C[1]
arousal_obs = ['high','low']                        # Arousal modality, used in A[2] and C[2]
choice_obs = ['blue','green','null']                # Choice modality, used in A[3] and C[3]
num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)]
num_modalities = len(num_obs)

# Define control factors with respective actions
choice_trust_actions = ['trust','distrust']    # Control factor with actions to trust or distrust, used in B[0] and B[2]
choice_card_actions = ['blue','green','null']  # Control factor with actions for choosing the card color (or null in timesteps where choosing is not applicable), used in B[3]
null_actions = ['NULL']                         # Control factor with null action for the uncontrollable hidden state factors correct card and stage, used B[1] and B[4]

# Level 1 of the hierarchical model. Here we define the components of the lower level agent.

# A matrix (likelihood / perception) containing conditional probability distributions of each observation modality with all hidden state factors.
# Each submatrix A[i] is the conditional probability distribution P(o_i|s) where s denotes all hidden state factors.
# During the process of inference, with model inversion via Bayes' theorem, the A matrix contributes to updating posterior beliefs Q(s|o).
A = utils.obj_array(num_modalities)   # Initialize A matrix with number of observation modalities

# A[0]: Advice modality which determinstically maps agent's trust belief, correct card belief, and other hidden state beliefs to advice received.
# The `p_advice` parameter determines probabilities linking observed advice from advisor, agent's belief in advisor's trutworthiness, and agent's belief in the correct card.
# If p_advice=0.9: If agent believes advisor is trustworthy then they believe advisor's advice will match agent's own belief about the true card color.
p_advice=0.9
A[0] = np.zeros(( len(advice_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
# regardless of affect and choice beliefs, if advice matches correct card belief, then 'trust' (and if non-match, then 'distrust')
for affect_state in range(len(affect_states)):  # cycles thru both affect states
  for choice_state in range(len(choice_states)):
    #print(f"{affect_state}, {choice_state}")
    A[0][0,0,0,affect_state,choice_state,1] = p_advice    # P(blue|trust,blue,<any affect>,<any choice>,) = p_advice
    A[0][1,0,1,affect_state,choice_state,1] = p_advice    # P(green|trust,green,<any affect>,<any choice>) = p_advice
    A[0][0,1,1,affect_state,choice_state,1] = p_advice    # P(blue|distrust,green,<any affect>,<any choice>) = p_advice (if agent believes 'green', and advice is 'blue', distrust)
    A[0][1,1,0,affect_state,choice_state,1] = p_advice    # P(green|distrust,blue,<any affect>,<any choice>) = p_advice (if agent believes 'blue', and advice is 'green', distrust)
    A[0][1,0,0,affect_state,choice_state,1] = 1-p_advice    # P(green|trust,blue,<any affect>,<any choice>,) = 1-p_advice
    A[0][0,0,1,affect_state,choice_state,1] = 1-p_advice    # P(blue|distrust,green,<any affect>,<any choice>) = 1-p_advice
    A[0][1,1,1,affect_state,choice_state,1] = 1-p_advice    # P(green|distrust,green,<any affect>,<any choice>) = 1-p_advice (if agent believes 'green', and advice is 'green', less distrust)
    A[0][0,1,0,affect_state,choice_state,1] = 1-p_advice    # P(blue|distrust,blue,<any affect>,<any choice>) = 1-p_advice (if agent believes 'blue', and advice is 'blue', less distrust)
    A[0][2,:,:,affect_state,choice_state,2] = 1           # P(null|<any trust>,<any correct>,<any affect>,<any choice>,decision> = 1 (expects no advice during final decision stage)
    A[0][2,:,:,affect_state,choice_state,0] = 1           # P(null|<any trust>,<any correct>,<any affect>,<any choice>,null) = 1 (expects no advice during initial null stage)
A[0] = utils.norm_dist(A[0])
print(f"is A[0] normalized? : {utils.is_normalized(A[0])}")

# A[1]: Feedback modality maps from the hidden states of correct card (blue or green), choice made (blue or green), and other hidden state beliefs to feedback outcomes
#     (correct, incorrect, or null if before feedback is received)) with 90% probability.
# The `alpha` parameter determines how precisely these beliefs map to feedback observations. An alpha of 1.0 will map these beliefs with full one-to-one precision.
alpha = 0.9
A[1] = np.zeros( (len(feedback_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
for trustworthiness_state in range(len(trustworthiness_states)):
  for affect_state in range(len(affect_states)):
    #for stage_state in range(len(stage_states)):
    #for non_null_choice in [0,1]:
      A[1][0,trustworthiness_state,0,affect_state,0,2] = alpha     # P(correct|<any trust>,blue,<any affect>,blue,<any stage>) = alpha (if believes 'blue' is correct and chose 'blue', then expects feedback is 'correct')
      A[1][0,trustworthiness_state,1,affect_state,1,2] = alpha     # P(correct|<any trust>,green,<any affect>,green,<any stage>) = alpha (if believes 'green' is correct and chose 'green', then expects feedback outcome is 'correct')
      A[1][0,trustworthiness_state,0,affect_state,1,2] = 1-alpha   # P(correct|<any trust>,blue,<any affect>,green,<any stage>) = 1-alpha (if believes 'blue' is correct and chose 'green', then expects feedback outcome is not 'correct')
      A[1][0,trustworthiness_state,1,affect_state,0,2] = 1-alpha   # P(correct|<any trust>,green,<any affect>,blue,<any stage>) = 1-alpha (if believes 'green' is correct and chose 'blue', then expects feedback outcome is not 'correct')
      A[1][1,trustworthiness_state,0,affect_state,1,2] = alpha     # P(incorrect|<any trust>,blue,<any affect>,green,<any stage>) = alpha (if believes 'blue' is correct and chose 'green', then expects feedback is 'incorrect')
      A[1][1,trustworthiness_state,1,affect_state,0,2] = alpha     # P(incorrect|<any trust>,green,<any affect>,blue,<any stage>) = alpha (if believes 'green' is correct and chose 'blue', then expects feedback is 'incorrect')
      A[1][1,trustworthiness_state,0,affect_state,0,2] = 1-alpha   # P(incorrect|<any trust>,blue,<any affect>,blue,<any stage>) = 1-alpha (if believes 'blue' is correct and chose 'blue', then expects feedback is not 'incorrect')
      A[1][1,trustworthiness_state,1,affect_state,1,2] = 1-alpha   # P(incorrect|<any trust>,green,<any affect>,green,<any stage>) = 1-alpha (if believes 'green' is correct and chose 'green', then expects feedback is not 'incorrect')
      A[1][2,trustworthiness_state,:,affect_state,2,2] = 1         # P(null|<any trust>,<any correct>,<any affect>,null,decision>) = 1 (in decision stage, if no decision made, expects feedback outcome 'null')
      A[1][2,trustworthiness_state,:,affect_state,:,1] = 1         # P(null|<any trust>,<any correct>,<any affect>,<any choice>,null>) = 1 (in null stage, expects feedback outcome 'null')
      A[1][2,trustworthiness_state,:,affect_state,:,0] = 1         # P(null|<any trust>,<any correct>,<any affect>,<any choice>,null>) = 1 (in null stage, expects feedback outcome 'null')
##        A[1][2,trustworthiness_state,correct_card_state,affect_state,2,stage_states] = 1   # P(null|<any trust>,<any correct>,<any affect>,null,<any stage>)

A[1] = utils.norm_dist(A[1])
print(A[1])
print(f"is A[1] normalized? : {utils.is_normalized(A[1])}")

# A[2]: Arousal modality maps deterministically from hidden affective state (angry or calm) to an interoceptive outcome of arousal (high or low), regardless of other hidden state beliefs.
# In this simulation, the agent deterministically infers 'anger' from 'high' arousal and infers 'calm' from 'low' arousal.
A[2] = np.zeros( (len(arousal_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
for trustworthiness_state in range(len(trustworthiness_states)):
  for correct_card_state in range(len(correct_card_states)):
    for choice_state in range(len(choice_states)):
      for stage_state in range(len(stage_states)):
        A[2][0,trustworthiness_state,correct_card_state,0,choice_state,stage_state] = 1.0   # P(high|<any trust>,<any correct>,angry,<any choice>) = 1.0
        A[2][1,trustworthiness_state,correct_card_state,1,choice_state,stage_state] = 1.0   # P(low|<any trust>,<any correct>,calm,<any choice>) = 1.0
A[2] = utils.norm_dist(A[2])
print(f"is A[2] normalized? : {utils.is_normalized(A[2])}")

# A[3]: Choice modality maps deterministically from the hidden state of decision (null, blue or green) to a proprioceptive outcome of choice made (null, blue or green).
# In this simulation, the agent determinstically infers
A[3] = np.zeros( (len(choice_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
for trustworthiness_state in range(len(trustworthiness_states)):
  for correct_card_state in range(len(correct_card_states)):
    for affect_state in range(len(affect_states)):
      for choice in range(len(choice_states)):     # choice_states and choice_obs indeces are aligned therefore we can re-use the same indeces
        for stage_state in range(len(stage_states)):
          A[3][choice,trustworthiness_state,correct_card_state,affect_state,choice,stage_state] = 1.0   # P(blue|<any trust>,<any correct>,<any affect>,blue,decision) = 1.0

print(A[3])
A[3] = utils.norm_dist(A[3])
print(f"is A[3] normalized? : {utils.is_normalized(A[3])}")
print(f"A[3] = {A[3]}")

# def plot_A_submatrix(A_matrix, modality_name, x_dim=None, y_dim=None, fixed_dims=None):
#     """
#     Plots specified hidden state dimensions of A matrices with explicit axis control

#     Parameters:
#     A_matrix (np.ndarray): The A matrix to visualize
#     modality_name (str): Name of the observation modality
#     x_dim (int): Hidden state factor index for x-axis
#     y_dim (int): Hidden state factor index for y-axis
#     fixed_dims (dict): Fixed dimensions {dim: value}
#     """
#     # Get model-specific labels
#     hidden_state_labels = [
#         ['trust', 'distrust'],          # Factor 0
#         ['blue', 'green'],              # Factor 1
#         ['angry', 'calm'],              # Factor 2
#         ['blue', 'green', 'null'],      # Factor 3
#         ['null', 'advice', 'decision']  # Factor 4
#     ]
#     factor_names = [
#         "Trustworthiness",
#         "Correct Card",
#         "Affect",
#         "Choice State",
#         "Stage"
#     ]

#     # Validate dimensions
#     num_factors = len(factor_names)
#     if x_dim is not None and (x_dim < 0 or x_dim >= num_factors):
#         raise ValueError(f"x_dim must be between 0 and {num_factors-1}")
#     if y_dim is not None and (y_dim < 0 or y_dim >= num_factors):
#         raise ValueError(f"y_dim must be between 0 and {num_factors-1}")

#     # Get observation labels
#     obs_labels = {
#         len(advice_obs): advice_obs,
#         len(feedback_obs): feedback_obs,
#         len(arousal_obs): arousal_obs,
#         len(choice_obs): choice_obs
#     }[A_matrix.shape[0]]

#     num_obs = len(obs_labels)
#     fig, axs = plt.subplots(1, num_obs, figsize=(4*num_obs, 5))
#     if num_obs == 1:
#         axs = [axs]

#     for obs_idx, obs_label in enumerate(obs_labels):
#         ax = axs[obs_idx]
#         slice_obj = [obs_idx] + [slice(None)]*(A_matrix.ndim-1)

#         # Apply fixed dimensions
#         if fixed_dims:
#             for dim, val in fixed_dims.items():
#                 slice_obj[dim+1] = val  # +1 for observation dimension

#         # Handle explicit axis selection
#         if x_dim is not None or y_dim is not None:
#             # Set unspecified dimensions to 0 if not fixed
#             for dim in range(num_factors):
#                 if dim not in [x_dim, y_dim] and dim+1 not in slice_obj:
#                     slice_obj[dim+1] = 0

#         submatrix = A_matrix[tuple(slice_obj)].squeeze()

#         # Create axis labels and data based on specified dimensions
#         if x_dim is not None and y_dim is not None:
#             x_labels = [hidden_state_labels[x_dim][i] for i in range(submatrix.shape[x_dim])]
#             y_labels = [hidden_state_labels[y_dim][i] for i in range(submatrix.shape[y_dim])]
#             data = submatrix
#         elif x_dim is not None:
#             x_labels = [hidden_state_labels[x_dim][i] for i in range(submatrix.shape[x_dim])]
#             y_labels = []
#             data = submatrix.reshape(1, -1)
#         else:
#             # Auto-detect if dimensions not specified
#             varying_dims = [i for i in range(submatrix.ndim) if submatrix.shape[i] > 1][:2]
#             if len(varying_dims) >= 1:
#                 x_dim = varying_dims[0]
#                 x_labels = [hidden_state_labels[x_dim][i] for i in range(submatrix.shape[x_dim])]
#             if len(varying_dims) >= 2:
#                 y_dim = varying_dims[1]
#                 y_labels = [hidden_state_labels[y_dim][i] for i in range(submatrix.shape[y_dim])]
#             data = submatrix

#         # Plotting logic
#         if data.ndim == 2:
#             sns.heatmap(data.T, annot=True, fmt=".2f", cmap="viridis",
#                         xticklabels=x_labels, yticklabels=y_labels,
#                         ax=ax, cbar=False)
#             ax.set_xlabel(factor_names[x_dim] if x_dim is not None else "")
#             ax.set_ylabel(factor_names[y_dim] if y_dim is not None else "")
#         else:
#             # 1D case
#             sns.heatmap(data[np.newaxis,:], annot=True, fmt=".2f", cmap="viridis",
#                         xticklabels=x_labels, yticklabels=False,
#                         ax=ax, cbar=False)
#             ax.set_xlabel(factor_names[x_dim] if x_dim is not None else "")

#         ax.set_title(f"{modality_name}: {obs_label}")

#     plt.tight_layout()
#     plt.show()

# plot_A_submatrix(A[0], "Advice", fixed_dims={
#     3: 0,  # Affect state = angry
#     4: 1,  # Stage = advice
#     2: 0   # Choice = blue
# })

# plot_A_submatrix(A[2], "Arousal Mapping", fixed_dims={
#     0: 0,  # Trust = trust (fixed)
#     1: 0,  # Card = blue (fixed)
#     3: 0,  # Choice = blue (fixed)
#     4: 2   # Stage = decision (fixed)
# })

# B matrix (transitions) for 5-action 2-step policies

# B matrix (hidden state transitions) containing conditional probability distributions of each hidden state at the next discrete timestep, conditioned on the
# hidden state at the current discrete time step and action at the current discrete timestep, P(s_{t+1}|s_{t},pi).
# Each submatrix B[i] is the conditional probability distribution P(s_{i,t+1}|s_{i,t},pi) s_{i} denotes a particular hidden state factor and pi denotes all policies for controlling
# that hidden state factor.
B = utils.obj_array(num_factors)

# B[0] : The state transition matrix for trustworthiness hidden state factor transitions, with discrete levels 'trust' and 'distrust'.
# Trustworthiness hidden state factor transitions (trustworthiness_t+1|trustworthiness,action)
# For the trust and distrust actions, regardless of trustworthiness belief in current timestep,
# the trustworthiness for next timestep (t+1) has a high probability (parameterized by p_Btrust) if agent chooses the corresponding
# trustworthiness action (ex. even if agent distrusts in their belief now, if the agent chooses 'trust' action,
# agent believes advisor is more trustworthy for next time step)
B[0] = np.zeros((len(trustworthiness_states), len(trustworthiness_states), len(choice_trust_actions)))
p_Btrust=0.9

B[0][0,0,0] = p_Btrust # P(trust_t+1|trust,trust) = 0.9
B[0][0,1,0] = p_Btrust # P(trust_t+1|distrust,trust) = 0.9
B[0][1,0,0] = 1-p_Btrust # P(distrust_t+1|trust,trust) = 0.1
B[0][1,1,0] = 1-p_Btrust # P(distrust_t+1|distrust,trust) = 0.1

B[0][1,1,1] = p_Btrust # P(distrust_t+1|distrust,distrust) = 0.9
B[0][1,0,1] = p_Btrust # P(distrust_t+1|trust,distrust) = 0.9
B[0][0,0,1] = 1-p_Btrust # P(trust_t+1|trust,distrust) = 0.1
B[0][0,1,1] = 1-p_Btrust # P(trust_t+1|distrust,distrust) = 0.1

print(f"is B[0] normalized? : {utils.is_normalized(B[0])}")


# B[1] : The state transition matrix for the correct card state. This hidden state factor is considered to be 'uncontrollable' (the agent can choose a card,
# but there is no action that can 'change' which card is correct), thus this matrix is simpler with a single 'NULL' action.
# correct card hidden state factor transitions -- highly deterministic and parameterized by p_Bcorrectcard, though a probability of 0.9 (rather than 1.0)
# introduces slight uncertainty into the agent's belief about the correct card over time.
# Note that this differs slightly from the Adams et. al 2022 "Everything is connected: Inference and attractors in delusions" simulation, whose agent
# believes the correct card remains the same with full certainty.
p_Bcorrectcard=0.9
B[1] = np.ones(( len(correct_card_states), len(correct_card_states), len(null_actions) ))*(1-p_Bcorrectcard)
for correct_card_state in range(len(correct_card_states)):
  for null_action in range(len(null_actions)):
    B[1][correct_card_state,correct_card_state,null_action] = p_Bcorrectcard
    print(f"B[1] (correct_card_state prob): P({correct_card_state}_t+1|{correct_card_state}_t,{null_action}) = {B[1][correct_card_state,correct_card_state,null_action]}")
print(f"B[1] = {B[1]}")
print(f"is B[1] normalized? : {utils.is_normalized(B[1])}")

# B[2] : The state transition matrix for the agent's affective state, which is deemed 'controllable' based on choosing to trust or distrust.
# Regardless of the agent's current emotional state, choosing trust has a 2/3 probability of transitioning to a calm affective state
# and vice-versa for distrust transitioning to an angry affective state.
# affect hidden state factor transitions --
B[2] = np.zeros((len(affect_states), len(affect_states), len(choice_trust_actions)))
B[2][0,0,0] = 0.3333  # P(angry_t+1|angry_t,trust)
B[2][0,1,0] = 0.3333  # P(angry_t+1|calm_t,trust)
B[2][1,0,0] = 0.6667  # P(calm_t+1|angry_t,trust)
B[2][1,1,0] = 0.6667  # P(calm_t+1|calm_t,trust)

B[2][0,0,1] = 0.6667 # P(angry_t+1|angry_t,distrust)
B[2][0,1,1] = 0.6667  # P(angry_t+1|calm_t,distrust)
B[2][1,0,1] = 0.3333  # P(calm_t+1|angry_t,distrust)
B[2][1,1,1] = 0.3333  # P(calm_t+1|angry_t,distrust)
B[2] = utils.norm_dist(B[2])
print(f"is B[2] normalized? : {utils.is_normalized(B[2])}")


# B[3] : The state transition matrix for the agent's beliefs about the card choice (blue, green null) hidden state factor,
# i.e. the card they chose rather than what the correct card actually is.
# The transitions are highly deterministic (parameterized by p_Bchoice with value of probability 0.95), making the
# choice the agent takes and the belief it has to most likely be what is transitioned to.
# For example, the probability that the agent chose blue is 0.95 when in the last timestep the agent believed they chose blue (hidden state)
# as well as in fact chose blue (action taken).
p_Bchoice=0.95
B[3] = np.ones((len(choice_states), len(choice_states), len(choice_card_actions)))*(1-p_Bchoice)/2
for choice in range(len(choice_states)):
  B[3][choice,:,choice]=p_Bchoice
print(f"B[3] = {B[3]}")
print(f"is B[3] normalized? : {utils.is_normalized(B[3])}")

# B[4] : The state transition matrix for the stage hidden state factor, which is considered 'uncontrollable' (the stages of each trial are
# assumed to follow in the same order without uncertainty, thus there is only a single 'NULL' action).
# This maintains a deterministic temporal succession, where if the agent believes the advice stage follows the
# initial null stage, and the choice stage follows the advice stage.
p_Bstage=1
B[4] = np.ones((len(stage_states), len(stage_states), len(null_actions)))*(1-p_Bstage)/2
B[4][1,0,0] = p_Bstage  # P(advice_t+1|null_t,null_action) = 1.0
B[4][2,1,0] = p_Bstage   # P(decision_t+1|advice_t,null_action) = 1.0
B[4][0,2,0] = p_Bstage   # P(null_t+1|decision_t,null_action) = 1.0
print(f"is B[4] normalized? : {utils.is_normalized(B[4])}")
print(f"B[4] = {B[4]}")
print(f"""
B[4][1,0,0] = {B[4][1,0,0]}
B[4][2,1,0] = {B[4][2,1,0]}
B[4][0,2,0] = {B[4][0,2,0]}
""")

# C matrix (priors over observations, i.e. "preferences")
# The `cc` parameter determines the agent's preference for receiving 'correct', and dyspreference for receiving 'incorrect', as the final feedback for their choice.
cc=0.5
# The `arousal_low_preference` parameter determines the agent's preference for the 'low' arousal observation
arousal_low_preference = 0.35
C = utils.obj_array_zeros(num_obs)
C[0] = utils.norm_dist(softmax(np.array([1,1,1])))  # C[0]: Advice modality: indifferent to if blue or green advice
C[1] = utils.norm_dist(softmax(np.array([cc, -cc-3,0]))) # C[1]: Feedback modality: strongly prefers 'correct' card feedback, dysprefers 'incorrect' card, indifferent to null
C[2] = utils.norm_dist(np.array([1-arousal_low_preference, arousal_low_preference]))   # C[2]: Arousal modality: prefers low arousal
C[3] = utils.norm_dist(softmax(np.array([1,1,1])))  # C[3]: Preference for choice modality: indifferent to choice (!!*if null=withdrawal, consider changing)

for i in range(len(C)):
  print(f"Is C[{i}] normalized? : {utils.is_normalized(C[i])}")
  print(f"C[{i}] = {C[i]}")

# D matrix (prior beliefs about hidden states), i.e. an agent's starting beliefs.
D = utils.obj_array(num_factors)
D[0] = utils.norm_dist(np.array([0.5, 0.5]))   # D[0]: priors over trustworthiness_states = ['trust','distrust']: agent is unsure of trust
D[1] = utils.norm_dist(np.array([0.5, 0.5]))   # D[1]: priors over correct correct_card_states = ['blue','green']: agent is unsure of correct card
D[2] = utils.norm_dist(np.array([0.5, 0.5]))   # D[2]: priors over affect_states = ['angry','calm']: agent is unsure of affect
D[3] = utils.norm_dist(np.array([0,0,1]))      # D[3]: priors over their own choice from self-observation, choice_states = ['blue','green','null']: agent is certain they have not made a choice
D[4] = utils.norm_dist(np.array([1,0,0]))   # D[4]: priors over current stage of the trial: agent believes they are at the first 'null' stage (!!consider modifying)

for i in range(len(D)):
    print(f"Is D[{i}] normalized? : {utils.is_normalized(D[i])}")

# E matrix (priors over policies, i.e. habits), a vector (1D matrix) with prior probabilities indexed to each of the six available policies (defined below).
# In this construction, habits are simplified into static (not learned) uniform probabilities, which will thus have no impact on final action selection but
# nonetheless are included as necessary for the matrix operations and further allow for future experimentation.
# As E figures in the EFE (expected free energy) computation during policy inference within a softmax function, it does not need to be normalized
# upon initialization.

E=np.zeros(6)

#habit = 0
#E=np.array([1, 1, 1, 1, 1,1])/4/100*habit
print(f"E = {E}")

# Policies (pi)
# Construction:
# Our 'deep inference' construction allows specifying specific policies the agent can choose for simplification and alignment with the reality of the
# simulation trial flow, rather than including all possible policy trajectories.
# As our agent only takes non-null actions at the end of timestep 1 and timestep 2 of the three-stage (three timestep) trial, policies are constructed
# to encompass two timesteps (an array of two chronological subarrays). the only relevant actions at the agent can take in

# Policies map to hidden state factors: [trustworthiness, correct_card, affect, choice, stage]
# Policies map to particular actions for controlling hidden state factors: [trust/distrust, null, trust/distrust, blue/green/null, null]

policies=[np.array([[0, 0, 0, 2, 0],[0, 0, 0, 1, 0]]), # trust,null,trust,null,null -> trust,null,trust,green,null              # The 1st policy involves trusting the advisor, and then choosing green
          np.array([[0, 0, 0, 2, 0],[0, 0, 0, 0, 0]]), # trust,null,trust,null,null -> trust,null,trust,blue,null               # The 2nd policy involves trusting the advisor, and then choosing blue
          np.array([[1, 0, 1, 2, 0],[1, 0, 1, 0, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,blue,null   # The 3rd policy involves distrusting the advisor, and then choosing blue
          np.array([[1, 0, 1, 2, 0],[1, 0, 1, 1, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,green,null  # The 4th policy involves distrusting the advisor, and then choosing green
          np.array([[0, 0, 0, 2, 0],[0, 0, 0, 2, 0]]), # trust,null,trust,null,null -> trust,null,trust,null,null               # The 5th policy involves trusting the advisor, but withdrawing from the task
          np.array([[1, 0, 1, 2, 0],[1, 0, 1, 2, 0]])] # distrust,null,distrust,null,null -> distrust,null,distrust,null,null   # The 6th policy involves distrusting the advisor, and then withdrawing from the task

print(len(policies))
for pol_idx, policy in enumerate(policies):
  print(f"policies[{pol_idx}].shape = {policy.shape}")
  for t in range(policy.shape[0]):
    print(f"policies[{pol_idx}][{t}] = {policy[t]}")

# Defining a custom `Agent_he` class for allowing deep inference via modifying the original pymdp `Agent` class.
import warnings
import numpy as np
from pymdp import inference, control, learning
from pymdp import utils, maths
import copy

class Agent_he(object):
    """
    The Agent class, the highest-level API that wraps together processes for action, perception, and learning under active inference.

    The basic usage is as follows:

    >>> my_agent = Agent(A = A, B = C, <more_params>)
    >>> observation = env.step(initial_action)
    >>> qs = my_agent.infer_states(observation)
    >>> q_pi, G = my_agent.infer_policies()
    >>> next_action = my_agent.sample_action()
    >>> next_observation = env.step(next_action)

    This represents one timestep of an active inference process. Wrapping this step in a loop with an ``Env()`` class that returns
    observations and takes actions as inputs, would entail a dynamic agent-environment interaction.
    """

    def __init__(
        self,
        A,
        B,
        C=None,
        D=None,
        E = None,
        pA=None,
        pB = None,
        pD = None,
        num_controls=None,
        policy_len=1,
        inference_horizon=1,
        control_fac_idx=None,
        policies=None,
        gamma=16.0,
        alpha=16.0,
        use_utility=True,
        use_states_info_gain=True,
        use_param_info_gain=False,
        action_selection="stochastic",
        sampling_mode = "marginal", # whether to sample from full posterior over policies ("full") or from marginal posterior over actions ("marginal")
        inference_algo="VANILLA",
        inference_params=None,
        modalities_to_learn="all",
        lr_pA=1.0,
        factors_to_learn="all",
        lr_pB=1.0,
        lr_pD=1.0,
        use_BMA = True,
        policy_sep_prior = False,
        save_belief_hist = False
    ):

        ### Constant parameters ###

        # policy parameters
        self.policy_len = policy_len
        self.gamma = gamma
        self.alpha = alpha
        self.action_selection = action_selection
        self.sampling_mode = sampling_mode
        self.use_utility = use_utility
        self.use_states_info_gain = use_states_info_gain
        self.use_param_info_gain = use_param_info_gain

        # learning parameters
        self.modalities_to_learn = modalities_to_learn
        self.lr_pA = lr_pA
        self.factors_to_learn = factors_to_learn
        self.lr_pB = lr_pB
        self.lr_pD = lr_pD

        # Initialise observation model (A matrices)
        if not isinstance(A, np.ndarray):
            raise TypeError(
                'A matrix must be a numpy array'
            )

        self.A = utils.to_obj_array(A)

        assert utils.is_normalized(self.A), "A matrix is not normalized (i.e. A.sum(axis = 0) must all equal 1.0)"

        # Determine number of observation modalities and their respective dimensions
        self.num_obs = [self.A[m].shape[0] for m in range(len(self.A))]
        self.num_modalities = len(self.num_obs)

        # Assigning prior parameters on observation model (pA matrices)
        self.pA = pA

        # Initialise transition model (B matrices)
        if not isinstance(B, np.ndarray):
            raise TypeError(
                'B matrix must be a numpy array'
            )

        self.B = utils.to_obj_array(B)

        assert utils.is_normalized(self.B), "B matrix is not normalized (i.e. B.sum(axis = 0) must all equal 1.0)"

        # Determine number of hidden state factors and their dimensionalities
        self.num_states = [self.B[f].shape[0] for f in range(len(self.B))]
        self.num_factors = len(self.num_states)

        # Assigning prior parameters on transition model (pB matrices)
        self.pB = pB

        # If no `num_controls` are given, then this is inferred from the shapes of the input B matrices
        if num_controls == None:
            self.num_controls = [self.B[f].shape[2] for f in range(self.num_factors)]
        else:
            self.num_controls = num_controls

        # Users have the option to make only certain factors controllable.
        # default behaviour is to make all hidden state factors controllable
        # (i.e. self.num_states == self.num_controls)
        if control_fac_idx == None:
            self.control_fac_idx = [f for f in range(self.num_factors) if self.num_controls[f] > 1]
        else:

            assert max(control_fac_idx) <= (self.num_factors - 1), "Check control_fac_idx - must be consistent with `num_states` and `num_factors`..."
            self.control_fac_idx = control_fac_idx

            for factor_idx in self.control_fac_idx:
                assert self.num_controls[factor_idx] > 1, "Control factor (and B matrix) dimensions are not consistent with user-given control_fac_idx"

        # Again, the use can specify a set of possible policies, or
        # all possible combinations of actions and timesteps will be considered
        if policies == None:
            policies = self._construct_policies()
        self.policies = policies

        assert all([len(self.num_controls) == policy.shape[1] for policy in self.policies]), "Number of control states is not consistent with policy dimensionalities"

        all_policies = np.vstack(self.policies)

        assert all([n_c == max_action for (n_c, max_action) in zip(self.num_controls, list(np.max(all_policies, axis =0)+1))]), "Maximum number of actions is not consistent with `num_controls`"

        # Construct prior preferences (uniform if not specified)

        if C is not None:
            if not isinstance(C, np.ndarray):
                raise TypeError(
                    'C vector must be a numpy array'
                )
            self.C = utils.to_obj_array(C)

            assert len(self.C) == self.num_modalities, f"Check C vector: number of sub-arrays must be equal to number of observation modalities: {self.num_modalities}"

            for modality, c_m in enumerate(self.C):
                assert c_m.shape[0] == self.num_obs[modality], f"Check C vector: number of rows of C vector for modality {modality} should be equal to {self.num_obs[modality]}"
        else:
            self.C = self._construct_C_prior()

        # Construct prior over hidden states (uniform if not specified)

        if D is not None:
            if not isinstance(D, np.ndarray):
                raise TypeError(
                    'D vector must be a numpy array'
                )
            self.D = utils.to_obj_array(D)

            assert len(self.D) == self.num_factors, f"Check D vector: number of sub-arrays must be equal to number of hidden state factors: {self.num_factors}"

            for f, d_f in enumerate(self.D):
                assert d_f.shape[0] == self.num_states[f], f"Check D vector: number of entries of D vector for factor {f} should be equal to {self.num_states[f]}"
        else:
            if pD is not None:
                self.D = utils.norm_dist_obj_arr(pD)
            else:
                self.D = self._construct_D_prior()

        assert utils.is_normalized(self.D), "A matrix is not normalized (i.e. A.sum(axis = 0) must all equal 1.0"

        # Assigning prior parameters on initial hidden states (pD vectors)
        self.pD = pD

        # Construct prior over policies (uniform if not specified)
        if E is not None:
            if not isinstance(E, np.ndarray):
                raise TypeError(
                    'E vector must be a numpy array'
                )
            self.E = E

            assert len(self.E) == len(self.policies), f"Check E vector: length of E must be equal to number of policies: {len(self.policies)}"

        else:
            self.E = self._construct_E_prior()

        self.edge_handling_params = {}
        self.edge_handling_params['use_BMA'] = use_BMA # creates a 'D-like' moving prior
        self.edge_handling_params['policy_sep_prior'] = policy_sep_prior # carries forward last timesteps posterior, in a policy-conditioned way

        # use_BMA and policy_sep_prior can both be False, but both cannot be simultaneously be True. If one of them is True, the other must be False
        if policy_sep_prior:
            if use_BMA:
                warnings.warn(
                    "Inconsistent choice of `policy_sep_prior` and `use_BMA`.\
                    You have set `policy_sep_prior` to True, so we are setting `use_BMA` to False"
                )
                self.edge_handling_params['use_BMA'] = False

        if inference_algo == None:
            self.inference_algo = "VANILLA"
            self.inference_params = self._get_default_params()
            if inference_horizon > 1:
                warnings.warn(
                    "If `inference_algo` is VANILLA, then inference_horizon must be 1\n. \
                    Setting inference_horizon to default value of 1...\n"
                    )
                self.inference_horizon = 1
            else:
                self.inference_horizon = 1
        else:
            self.inference_algo = inference_algo
            self.inference_params = self._get_default_params()
            self.inference_horizon = inference_horizon

        if save_belief_hist:
            self.qs_hist = []
            self.q_pi_hist = []

        self.prev_obs = []
        self.reset()

        self.action = None
        self.prev_actions = None

    def _construct_C_prior(self):

        C = utils.obj_array_zeros(self.num_obs)

        return C

    def _construct_D_prior(self):

        D = utils.obj_array_uniform(self.num_states)

        return D

    def _construct_policies(self):

        policies =  control.construct_policies(
            self.num_states, self.num_controls, self.policy_len, self.control_fac_idx
        )

        return policies

    def _construct_num_controls(self):
        num_controls = control.get_num_controls_from_policies(
            self.policies
        )

        return num_controls

    def _construct_E_prior(self):
        E = np.ones(len(self.policies)) / len(self.policies)
        return E

    def reset(self, init_qs=None):
        """
        Resets the posterior beliefs about hidden states of the agent to a uniform distribution, and resets time to first timestep of the simulation's temporal horizon.
        Returns the posterior beliefs about hidden states.

        Returns
        ---------
        qs: ``numpy.ndarray`` of dtype object
           Initialized posterior over hidden states. Depending on the inference algorithm chosen and other parameters (such as the parameters stored within ``edge_handling_paramss),
           the resulting ``qs`` variable will have additional sub-structure to reflect whether beliefs are additionally conditioned on timepoint and policy.
            For example, in case the ``self.inference_algo == 'MMP' `, the indexing structure of ``qs`` is policy->timepoint-->factor, so that
            ``qs[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx``
            at timepoint ``t_idx``. In this case, the returned ``qs`` will only have entries filled out for the first timestep, i.e. for ``q[p_idx][0]``, for all
            policy-indices ``p_idx``. Subsequent entries ``q[:][1, 2, ...]`` will be initialized to empty ``numpy.ndarray`` objects.
        """

        self.curr_timestep = 0

        if init_qs is None:
            if self.inference_algo == 'VANILLA':
                self.qs = utils.obj_array_uniform(self.num_states)
            else: # in the case you're doing MMP (i.e. you have an inference_horizon > 1), we have to account for policy- and timestep-conditioned posterior beliefs
                self.qs = utils.obj_array(len(self.policies))
                for p_i, _ in enumerate(self.policies):
                    self.qs[p_i] = utils.obj_array(self.inference_horizon + self.policy_len + 1) # + 1 to include belief about current timestep
                    self.qs[p_i][0] = utils.obj_array_uniform(self.num_states)

                first_belief = utils.obj_array(len(self.policies))
                for p_i, _ in enumerate(self.policies):
                    first_belief[p_i] = copy.deepcopy(self.D)

                if self.edge_handling_params['policy_sep_prior']:
                    self.set_latest_beliefs(last_belief = first_belief)
                else:
                    self.set_latest_beliefs(last_belief = self.D)

        else:
            self.qs = init_qs

        return self.qs

    def step_time(self):
        """
        Advances time by one step. This involves updating the ``self.prev_actions``, and in the case of a moving
        inference horizon, this also shifts the history of post-dictive beliefs forward in time (using ``self.set_latest_beliefs()``),
        so that the penultimate belief before the beginning of the horizon is correctly indexed.

        Returns
        ---------
        curr_timestep: ``int``
            The index in absolute simulation time of the current timestep.
        """

        if self.prev_actions is None:
            self.prev_actions = [self.action]
        else:
            self.prev_actions.append(self.action)

        self.curr_timestep += 1

        if self.inference_algo == "MMP" and (self.curr_timestep - self.inference_horizon) >= 0:
            self.set_latest_beliefs()

        return self.curr_timestep

    def set_latest_beliefs(self,last_belief=None):
        """
        Both sets and returns the penultimate belief before the first timestep of the backwards inference horizon.
        In the case that the inference horizon includes the first timestep of the simulation, then the ``latest_belief`` is
        simply the first belief of the whole simulation, or the prior (``self.D``). The particular structure of the ``latest_belief``
        depends on the value of ``self.edge_handling_params['use_BMA']``.

        Returns
        ---------
        latest_belief: ``numpy.ndarray`` of dtype object
            Penultimate posterior beliefs over hidden states at the timestep just before the first timestep of the inference horizon.
            Depending on the value of ``self.edge_handling_params['use_BMA']``, the shape of this output array will differ.
            If ``self.edge_handling_params['use_BMA'] == True``, then ``latest_belief`` will be a Bayesian model average
            of beliefs about hidden states, where the average is taken with respect to posterior beliefs about policies.
            Otherwise, `latest_belief`` will be the full, policy-conditioned belief about hidden states, and will have indexing structure
            policies->factors, such that ``latest_belief[p_idx][f_idx]`` refers to the penultimate belief about marginal factor ``f_idx``
            under policy ``p_idx``.
        """

        if last_belief is None:
            last_belief = utils.obj_array(len(self.policies))
            for p_i, _ in enumerate(self.policies):
                last_belief[p_i] = copy.deepcopy(self.qs[p_i][0])

        begin_horizon_step = self.curr_timestep - self.inference_horizon
        if self.edge_handling_params['use_BMA'] and (begin_horizon_step >= 0):
            if hasattr(self, "q_pi_hist"):
                self.latest_belief = inference.average_states_over_policies(last_belief, self.q_pi_hist[begin_horizon_step]) # average the earliest marginals together using contemporaneous posterior over policies (`self.q_pi_hist[0]`)
            else:
                self.latest_belief = inference.average_states_over_policies(last_belief, self.q_pi) # average the earliest marginals together using posterior over policies (`self.q_pi`)
        else:
            self.latest_belief = last_belief

        return self.latest_belief

    def get_future_qs(self):
        """
        Returns the last ``self.policy_len`` timesteps of each policy-conditioned belief
        over hidden states. This is a step of pre-processing that needs to be done before computing
        the expected free energy of policies. We do this to avoid computing the expected free energy of
        policies using beliefs about hidden states in the past (so-called "post-dictive" beliefs).

        Returns
        ---------
        future_qs_seq: ``numpy.ndarray`` of dtype object
            Posterior beliefs over hidden states under a policy, in the future. This is a nested ``numpy.ndarray`` object array, with one
            sub-array ``future_qs_seq[p_idx]`` for each policy. The indexing structure is policy->timepoint-->factor, so that
            ``future_qs_seq[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx``
            at future timepoint ``t_idx``, relative to the current timestep.
        """

        future_qs_seq = utils.obj_array(len(self.qs))
        for p_idx in range(len(self.qs)):
            future_qs_seq[p_idx] = self.qs[p_idx][-(self.policy_len+1):] # this grabs only the last `policy_len`+1 beliefs about hidden states, under each policy

        return future_qs_seq


    def infer_states(self, observation, distr_obs = False):
        """
        Update approximate posterior over hidden states by solving variational inference problem, given an observation.

        Parameters
        ----------
        observation: ``list`` or ``tuple`` of ints
            The observation input. Each entry ``observation[m]`` stores the index of the discrete
            observation for modality ``m``.

        Returns
        ---------
        qs: ``numpy.ndarray`` of dtype object
            Posterior beliefs over hidden states. Depending on the inference algorithm chosen, the resulting ``qs`` variable will have additional sub-structure to reflect whether
            beliefs are additionally conditioned on timepoint and policy.
            For example, in case the ``self.inference_algo == 'MMP' `` indexing structure is policy->timepoint-->factor, so that
            ``qs[p_idx][t_idx][f_idx]`` refers to beliefs about marginal factor ``f_idx`` expected under policy ``p_idx``
            at timepoint ``t_idx``.
        """

        observation = tuple(observation) if not distr_obs else observation

        if not hasattr(self, "qs"):
            self.reset()

        if self.inference_algo == "VANILLA":
            if self.action is not None:
                empirical_prior = control.get_expected_states(
                    self.qs, self.B, self.action.reshape(1, -1) #type: ignore
                )[0]
            else:
                empirical_prior = self.D
            qs = inference.update_posterior_states(
            self.A,
            observation,
            empirical_prior,
            **self.inference_params
            )
        elif self.inference_algo == "MMP":

            self.prev_obs.append(observation)
            if len(self.prev_obs) > self.inference_horizon:
                latest_obs = self.prev_obs[-self.inference_horizon:]
                latest_actions = self.prev_actions[-(self.inference_horizon-1):]
            else:
                latest_obs = self.prev_obs
                latest_actions = self.prev_actions

            qs, F = inference.update_posterior_states_full(
                self.A,
                self.B,
                latest_obs,
                self.policies,
                latest_actions,
                prior = self.latest_belief,
                policy_sep_prior = self.edge_handling_params['policy_sep_prior'],
                **self.inference_params
            )

            self.F = F # variational free energy of each policy

        if hasattr(self, "qs_hist"):
            self.qs_hist.append(qs)
        self.qs = qs

        return qs

    def _infer_states_test(self, observation):
        """
        Test version of ``infer_states()`` that additionally returns intermediate variables of MMP, such as
        the prediction errors and intermediate beliefs from the optimization. Used for benchmarking against SPM outputs.
        """
        observation = tuple(observation)

        if not hasattr(self, "qs"):
            self.reset()

        if self.inference_algo == "VANILLA":
            if self.action is not None:
                empirical_prior = control.get_expected_states(
                    self.qs, self.B, self.action.reshape(1, -1) #type: ignore
                )
            else:
                empirical_prior = self.D
            qs = inference.update_posterior_states(
            self.A,
            observation,
            empirical_prior,
            **self.inference_params
            )
        elif self.inference_algo == "MMP":

            self.prev_obs.append(observation)
            if len(self.prev_obs) > self.inference_horizon:
                latest_obs = self.prev_obs[-self.inference_horizon:]
                latest_actions = self.prev_actions[-(self.inference_horizon-1):]
            else:
                latest_obs = self.prev_obs
                latest_actions = self.prev_actions

            qs, F, xn, vn = inference._update_posterior_states_full_test(
                self.A,
                self.B,
                latest_obs,
                self.policies,
                latest_actions,
                prior = self.latest_belief,
                policy_sep_prior = self.edge_handling_params['policy_sep_prior'],
                **self.inference_params
            )

            self.F = F # variational free energy of each policy

        if hasattr(self, "qs_hist"):
            self.qs_hist.append(qs)

        self.qs = qs

        return qs, xn, vn

    def infer_policies(self):
        """
        Perform policy inference by optimizing a posterior (categorical) distribution over policies.
        This distribution is computed as the softmax of ``G * gamma + lnE`` where ``G`` is the negative expected
        free energy of policies, ``gamma`` is a policy precision and ``lnE`` is the (log) prior probability of policies.
        This function returns the posterior over policies as well as the negative expected free energy of each policy.

        Returns
        ----------
        q_pi: 1D ``numpy.ndarray``
            Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
        G: 1D ``numpy.ndarray``
            Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
        """

        if self.inference_algo == "VANILLA":
            q_pi, G = control.update_posterior_policies(
                self.qs,
                self.A,
                self.B,
                self.C,
                self.policies,
                self.use_utility,
                self.use_states_info_gain,
                self.use_param_info_gain,
                self.pA,
                self.pB,
                E = self.E,
                gamma = self.gamma
            )
        elif self.inference_algo == "MMP":

            future_qs_seq = self.get_future_qs()

            q_pi, G = control.update_posterior_policies_full(
                future_qs_seq,
                self.A,
                self.B,
                self.C,
                self.policies,
                self.use_utility,
                self.use_states_info_gain,
                self.use_param_info_gain,
                self.latest_belief,
                self.pA,
                self.pB,
                F = self.F,
                E = self.E,
                gamma = self.gamma
            )

        if hasattr(self, "q_pi_hist"):
            self.q_pi_hist.append(q_pi)
            if len(self.q_pi_hist) > self.inference_horizon:
                self.q_pi_hist = self.q_pi_hist[-(self.inference_horizon-1):]

        self.q_pi = q_pi
        self.G = G
        return q_pi, G

    # Custom action sampling function for deep policy inference
    def sample_action_he(self,t):
        """
        Sample or select a discrete action from the posterior over control states.
        This function both sets or cachs the action as an internal variable with the agent and returns it.
        This function also updates time variable (and thus manages consequences of updating the moving reference frame of beliefs)
        using ``self.step_time()``.

        Returns
        ----------
        action: 1D ``numpy.ndarray``
            Vector containing the indices of the actions for each control factor
        """

        if self.sampling_mode == "marginal":
            action = control_he.sample_action_he(
                self.q_pi, self.policies, self.num_controls, action_selection = self.action_selection, alpha = self.alpha, t = t
            )
        elif self.sampling_mode == "full":
            action = control.sample_policy(
                self.q_pi, self.policies, self.num_controls, action_selection = self.action_selection, alpha = self.alpha,t = t
            )

        self.action = action

        self.step_time()

        return action

    def _sample_action_test(self):
        """
        Sample or select a discrete action from the posterior over control states.
        This function both sets or cachs the action as an internal variable with the agent and returns it.
        This function also updates time variable (and thus manages consequences of updating the moving reference frame of beliefs)
        using ``self.step_time()``.

        Returns
        ----------
        action: 1D ``numpy.ndarray``
            Vector containing the indices of the actions for each control factor
        """

        if self.sampling_mode == "marginal":
            action, p_dist = control._sample_action_test(
                self.q_pi, self.policies, self.num_controls, action_selection = self.action_selection, alpha = self.alpha
            )
        elif self.sampling_mode == "full":
            action, p_dist = control._sample_policy_test(
                self.q_pi, self.policies, self.num_controls, action_selection = self.action_selection, alpha = self.alpha
            )

        self.action = action

        self.step_time()

        return action, p_dist

    def update_A(self, obs):
        """
        Update approximate posterior beliefs about Dirichlet parameters that parameterise the observation likelihood or ``A`` array.

        Parameters
        ----------
        observation: ``list`` or ``tuple`` of ints
            The observation input. Each entry ``observation[m]`` stores the index of the discrete
            observation for modality ``m``.

        Returns
        -----------
        qA: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over observation model (same shape as ``A``), after having updated it with observations.
        """

        qA = learning.update_obs_likelihood_dirichlet(
            self.pA,
            self.A,
            obs,
            self.qs,
            self.lr_pA,
            self.modalities_to_learn
        )

        self.pA = qA # set new prior to posterior
        self.A = utils.norm_dist_obj_arr(qA) # take expected value of posterior Dirichlet parameters to calculate posterior over A array

        return qA

    def update_B(self, qs_prev):
        """
        Update posterior beliefs about Dirichlet parameters that parameterise the transition likelihood

        Parameters
        -----------
        qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
            Marginal posterior beliefs over hidden states at previous timepoint.

        Returns
        -----------
        qB: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
        """

        qB = learning.update_state_likelihood_dirichlet(
            self.pB,
            self.B,
            self.action,
            self.qs,
            qs_prev,
            self.lr_pB,
            self.factors_to_learn
        )

        self.pB = qB # set new prior to posterior
        self.B = utils.norm_dist_obj_arr(qB)  # take expected value of posterior Dirichlet parameters to calculate posterior over B array

        return qB

    def update_B_ap(self, qs_prev, no_actions=False):
        """
        Update posterior beliefs about Dirichlet parameters that parameterise the transition likelihood

        Parameters
        -----------
        qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
            Marginal posterior beliefs over hidden states at previous timepoint.
        no_actions: Boolean (True/False) denoting if agent has an action history.

        Returns
        -----------
        qB: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
        """
        if no_actions == True:
          actions = np.zeros(len(self.B))
        else:
          actions = self.action

        qB = learning.update_state_likelihood_dirichlet(
            self.pB,
            self.B,
            #self.action,
            actions,
            self.qs,
            qs_prev,
            self.lr_pB,
            self.factors_to_learn
        )

        self.pB = qB # set new prior to posterior
        self.B = utils.norm_dist_obj_arr(qB)  # take expected value of posterior Dirichlet parameters to calculate posterior over B array

        return qB

    def update_D(self, qs_t0 = None):
        """
        Update Dirichlet parameters of the initial hidden state distribution
        (prior beliefs about hidden states at the beginning of the inference window).

        Parameters
        -----------
        qs_t0: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, or ``None``
            Marginal posterior beliefs over hidden states at current timepoint. If ``None``, the
            value of ``qs_t0`` is set to ``self.qs_hist[0]`` (i.e. the initial hidden state beliefs at the first timepoint).
            If ``self.inference_algo == "MMP"``, then ``qs_t0`` is set to be the Bayesian model average of beliefs about hidden states
            at the first timestep of the backwards inference horizon, where the average is taken with respect to posterior beliefs about policies.

        Returns
        -----------
        qD: ``numpy.ndarray`` of dtype object
            Posterior Dirichlet parameters over initial hidden state prior (same shape as ``qs_t0``), after having updated it with state beliefs.
        """

        if self.inference_algo == "VANILLA":

            if qs_t0 is None:

                try:
                    #qs_t0 = self.qs_hist[0]
                    qs_t0 = self.qs_hist[0]
                except ValueError:
                    print("qs_t0 must either be passed as argument to `update_D` or `save_belief_hist` must be set to True!")

        elif self.inference_algo == "MMP":

            if self.edge_handling_params['use_BMA']:
                qs_t0 = self.latest_belief
            elif self.edge_handling_params['policy_sep_prior']:

                qs_pi_t0 = self.latest_belief

                # get beliefs about policies at the time at the beginning of the inference horizon
                if hasattr(self, "q_pi_hist"):
                    begin_horizon_step = max(0, self.curr_timestep - self.inference_horizon)
                    q_pi_t0 = np.copy(self.q_pi_hist[begin_horizon_step])
                else:
                    q_pi_t0 = np.copy(self.q_pi)

                qs_t0 = inference.average_states_over_policies(qs_pi_t0,q_pi_t0) # beliefs about hidden states at the first timestep of the inference horizon

        qD = learning.update_state_prior_dirichlet(self.pD, qs_t0, self.lr_pD, factors = self.factors_to_learn)

        self.pD = qD # set new prior to posterior
        self.D = utils.norm_dist_obj_arr(qD) # take expected value of posterior Dirichlet parameters to calculate posterior over D array

        return qD

    def _get_default_params(self):
        method = self.inference_algo
        default_params = None
        if method == "VANILLA":
            default_params = {"num_iter": 10, "dF": 1.0, "dF_tol": 0.001}
        elif method == "MMP":
            default_params = {"num_iter": 10, "grad_descent": True, "tau": 0.25}
        elif method == "VMP":
            raise NotImplementedError("VMP is not implemented")
        elif method == "BP":
            raise NotImplementedError("BP is not implemented")
        elif method == "EP":
            raise NotImplementedError("EP is not implemented")
        elif method == "CV":
            raise NotImplementedError("CV is not implemented")

        return default_params

# import pymdp
# from pymdp.agent import Agent
# import inspect

# # Print the full source code of the Agent class
# print(inspect.getsource(Agent))

# import pymdp
# import pymdp.control
# import inspect

# # Print the full source code of the Agent class
# print(inspect.getsource(pymdp.control))

# Defining a custom `control_he` class and custom `sample_action_he` helper method via modifying the original `control` module and `sample_action` helper.

import itertools
import numpy as np
from pymdp.maths import softmax, softmax_obj_arr, spm_dot, spm_wnorm, spm_MDP_G, spm_log_single, spm_log_obj_array
from pymdp import utils
import copy

class control_he(object):
    def sample_action_he(q_pi, policies, num_controls, action_selection="deterministic", alpha = 16.0, t = None):
      """
      Computes the marginal posterior over actions and then samples an action from it, one action per control factor.

      Parameters
      ----------
      q_pi: 1D ``numpy.ndarray``
          Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
      policies: ``list`` of 2D ``numpy.ndarray``
          ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]``
          is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
          depth of the policy and ``num_factors`` is the number of control factors.
      num_controls: ``list`` of ``int``
          ``list`` of the dimensionalities of each control state factor.
      action_selection: string, default "deterministic"
          String indicating whether whether the selected action is chosen as the maximum of the posterior over actions,
          or whether it's sampled from the posterior marginal over actions
      alpha: float, default 16.0
          Action selection precision -- the inverse temperature of the softmax that is used to scale the
          action marginals before sampling. This is only used if ``action_selection`` argument is "stochastic"

      Returns
      ----------
      selected_policy: 1D ``numpy.ndarray``
          Vector containing the indices of the actions for each control factor
      """

      num_factors = len(num_controls)

      action_marginals = utils.obj_array_zeros(num_controls)

      # weight each action according to its integrated posterior probability over policies and timesteps
      # for pol_idx, policy in enumerate(policies):
      #     for t in range(policy.shape[0]):
      #         for factor_i, action_i in enumerate(policy[t, :]):
      #             action_marginals[factor_i][action_i] += q_pi[pol_idx]

      # weight each action according to its integrated posterior probability under all policies at the current timestep
      for pol_idx, policy in enumerate(policies):
          for factor_i, action_i in enumerate(policy[t, :]):
              action_marginals[factor_i][action_i] += q_pi[pol_idx]

      action_marginals = utils.norm_dist_obj_arr(action_marginals)

      selected_policy = np.zeros(num_factors)
      for factor_i in range(num_factors):

          # Either you do this:
          if action_selection == 'deterministic':
              selected_policy[factor_i] = np.argmax(action_marginals[factor_i])
          elif action_selection == 'stochastic':
              log_marginal_f = spm_log_single(action_marginals[factor_i])
              p_actions = softmax(log_marginal_f * alpha)
              selected_policy[factor_i] = utils.sample(p_actions)

      return selected_policy

# Define matrix Dirichlet distributions as hyperparameters for lower level agent to allow learning A, B, and D matrices
pA = utils.dirichlet_like(A, scale = 1.0)   # Prior matrix Dirichlet distribution for A matrix (likelihood model)
pB = utils.dirichlet_like(B, scale = 1.0)   # Prior matrix Dirichlet distribution for B matrix (state transitions model)
pD = utils.dirichlet_like(D, scale = 1.0)   # Prior matrix Dirichlet distribution for D matrix (priors about initial states)

my_agent = Agent_he(A = A, B = B, C = C, D = D, E = E, pA=pA, pB=pB, pD=pD,
                 policies=policies, inference_algo="VANILLA", save_belief_hist = True,)

# Hierarchical layer level

# Hidden state factors at level 2
safety_self_states_2 = ['safe','danger']
safety_world_states_2 = ['safe','danger']
safety_other_states_2 = ['safe','danger']
num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
num_factors_2 = len(num_states_2)
# The level 2 model does not take actions, thus we simply supply a singular NULL action to allow the computations to play out coherently.
null_action_2 = ['NULL']

# A2 matrix (likelihood / perception model) for the higher level agent, containing conditional probability distributions of each observation modality with all hidden state factors.
# Each submatrix A2[i] is the conditional probability distribution P(o_i|s) where s denotes all hidden state factors at the higher level.
A2 = utils.obj_array(num_factors)

# A2[0]: Inferred trustworthiness modality which determinstically maps the lower level agent's posterior trustworthiness (as an observation)
# to the higher level agent's hidden state beliefs.
# The `trust_safety_association` parameter determines the probability linking observed 'trust' to the 'safe' beliefs in the higher level's safety hidden state factors.
# This is the case vice-versa for linking 'distrust' to the 'danger' beliefs.
trust_safety_association = 0.667    # The agent moderately associates 'trust' with 'safe' as well as 'distrust' with 'danger'
A2[0] = np.zeros(( len(trustworthiness_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
A2[0][0,0,0,0] = trust_safety_association       # P(trust|safe,safe,safe) = 0.667
A2[0][0,0,0,1] = 1-trust_safety_association     # P(trust|safe,safe,danger) = 0.333
A2[0][0,0,1,0] = 1-trust_safety_association     # P(trust|safe,danger,safe) = 0.333
A2[0][0,1,0,0] = 1-trust_safety_association     # P(trust|danger,safe,safe) = 0.333
A2[0][0,0,1,1] = 1-trust_safety_association     # P(trust|safe,danger,danger) = 0.333
A2[0][0,1,0,1] = 1-trust_safety_association     # P(trust|danger,safe,danger) = 0.333
A2[0][0,1,1,0] = 1-trust_safety_association     # P(trust|danger,danger,safe) = 0.333
A2[0][0,1,1,1] = 1-trust_safety_association     # P(trust|danger,danger,danger) = 0.333
A2[0][1,0,0,0] = 1-trust_safety_association     # P(distrust|safe,safe,safe) = 0.333
A2[0][1,0,0,1] = trust_safety_association       # P(distrust|safe,safe,danger) = 0.667
A2[0][1,0,1,0] = trust_safety_association       # P(distrust|safe,danger,safe) = 0.667
A2[0][1,1,0,0] = trust_safety_association       # P(distrust|danger,safe,safe) = 0.667
A2[0][1,0,1,1] = trust_safety_association       # P(distrust|safe,danger,danger) = 0.667
A2[0][1,1,0,1] = trust_safety_association       # P(distrust|danger,safe,danger) = 0.667
A2[0][1,1,1,0] = trust_safety_association       # P(distrust|danger,danger,safe) = 0.667
A2[0][1,1,1,1] = trust_safety_association       # P(distrust|danger,danger,danger) = 0.667
A2[0] = utils.norm_dist(A2[0])
# print(f"""P(trust|s_1^2,s_2^2,s_3^2)""")
# for trustworthiness_state in range(len(trustworthiness_states)):
#   for safety_self_state in range(len(safety_self_states_2)):
#     for safety_world_state in range(len(safety_world_states_2)):
#       for safety_other_state in range(len(safety_other_states_2)):
#         print(f"P({trustworthiness_states[trustworthiness_state]}|{safety_self_states_2[safety_self_state]},{safety_world_states_2[safety_world_state]},{safety_other_states_2[safety_other_state]}) = {A2[0][trustworthiness_state,safety_self_state,safety_world_state,safety_other_state]}")


# A[2] : Inferred correct card modality. There is no relationship between the lower level's belief about 'blue' or 'green' being the correct card with safety beliefs,
# thus this is fully uniform.
A2[1] = np.zeros(( len(correct_card_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
for correct_card_state in range(len(correct_card_states)):
  for safety_self_state in range(len(safety_self_states_2)):
    for safety_world_state in range(len(safety_world_states_2)):
      for safety_other_state in range(len(safety_other_states_2)):
        A2[1][correct_card_state,safety_self_state,safety_world_state,safety_other_state] = 1
A2[1] = utils.norm_dist(A2[1])
print(f"A2[1] = {A2[1]}")

# A[2] : Inferred affect modality linking affect and safety beliefs, where 'calm' is linked more strongly to 'safe' beliefs with parameter
# `affect_safety_association` (probability 0.667) and vice-versa for 'angry' and 'danger'.
#A2(angry|self=danger OR world=danger OR other=danger)=affect_safety_association
#A2(calm|self=safe AND world=safe AND other=safe)=affect_safety_association
affect_safety_association = 0.667
A2[2] = np.zeros(( len(affect_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
A2[2][0,0,0,0] = 1-affect_safety_association    # P(angry|safe,safe,safe) = 0.333
A2[2][0,0,0,1] = affect_safety_association      # P(angry|safe,safe,danger) = 0.667
A2[2][0,0,1,0] = affect_safety_association      # P(angry|safe,danger,safe) = 0.667
A2[2][0,1,0,0] = affect_safety_association      # P(angry|danger,safe,safe) = 0.667
A2[2][0,0,1,1] = affect_safety_association      # P(angry|safe,danger,danger) = 0.667
A2[2][0,1,0,1] = affect_safety_association      # P(angry|danger,safe,danger) = 0.667
A2[2][0,1,1,0] = affect_safety_association      # P(angry|danger,danger,safe) = 0.667
A2[2][0,1,1,1] = affect_safety_association      # P(angry|danger,danger,danger) = 0.667
A2[2][1,0,0,0] = affect_safety_association      # P(calm|safe,safe,safe) = 0.667
A2[2][1,0,0,1] = 1-affect_safety_association    # P(calm|safe,safe,danger) = 0.333
A2[2][1,0,1,0] = 1-affect_safety_association    # P(calm|safe,danger,safe) = 0.333
A2[2][1,1,0,0] = 1-affect_safety_association    # P(calm|dnger,safe,safe) = 0.333
A2[2][1,0,1,1] = 1-affect_safety_association    # P(calm|safe,danger,danger) = 0.333
A2[2][1,1,0,1] = 1-affect_safety_association    # P(calm|danger,safe,danger) = 0.333
A2[2][1,1,1,0] = 1-affect_safety_association    # P(calm|danger,danger,safe) = 0.333
A2[2][1,1,1,1] = 1-affect_safety_association    # P(calm|danger,danger,danger) = 0.333
A2[2] = utils.norm_dist(A2[2])
# print("P(affect|s_1^2,s_2^2,s_3^2)")
# for affect_state in range(len(affect_states)):
#   for safety_self_state in range(len(safety_self_states_2)):
#     for safety_world_state in range(len(safety_world_states_2)):
#       for safety_other_state in range(len(safety_other_states_2)):
#         print(f"P({affect_states[affect_state]}|{safety_self_states_2[safety_self_state]},{safety_world_states_2[safety_world_state]},{safety_other_states_2[safety_other_state]}) = {A2[2][affect_state,safety_self_state,safety_world_state,safety_other_state]}")

# A2[3] : Inferred choice modality linking the agent's belief about the choice they made ('blue','green','null') with safety beliefs.
# There is no relationship, thus the distribution is uniform.
# choice_states (blue,green,null): no relationship with safety
A2[3] = np.zeros(( len(choice_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
for choice_state in range(len(choice_states)):
  for safety_self_state in range(len(safety_self_states_2)):
    for safety_world_state in range(len(safety_world_states_2)):
      for safety_other_state in range(len(safety_other_states_2)):
        A2[3][choice_state,safety_self_state,safety_world_state,safety_other_state] = 1
A2[3] = utils.norm_dist(A2[3])

# A2[4] : Inferred state modality linking the agent's belief about the current stage of the trial with safety beliefs.
# There is no relationship, thus the distribution is uniform.
# stage_states (null,advice,decision): no relationship with safety (???)
A2[4] = np.zeros(( len(stage_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
for stage_state in range(len(stage_states)):
  for safety_self_state in range(len(safety_self_states_2)):
    for safety_world_state in range(len(safety_world_states_2)):
      for safety_other_state in range(len(safety_other_states_2)):
        A2[4][stage_state,safety_self_state,safety_world_state,safety_other_state] = 1
A2[4] = utils.norm_dist(A2[4])

for i in range(len(A2)):
    try:
      print(f"Is A2[{i}] normalized? : {utils.is_normalized(A2[i])}")
    except:
      print(f"normalization error: A2[{i}] -- ")

# B2 : B matrix (hidden state transitions) containing conditional probability distributions of each hidden state at the next discrete timestep, conditioned on the
# hidden state at the current discrete time step and action at the current discrete timestep, P(s_{t+1}|s_{t},pi) at the higher level.
# Each submatrix 2B[i] is the conditional probability distribution P(s_{i,t+1}|s_{i,t},pi) s_{i} denotes a particular hidden state factor and pi denotes all policies for controlling
# that hidden state factor.
# The higher level does not have actions available and simply maps the safety belief from the current timestep to the next timestep.
B2 = utils.obj_array(num_factors_2)

# B2[0] : State transitions for safety beliefs about the self. The `B2_safety_self_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
# akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
#B2(self_t=safe|self_t-1=safe)=B2_safety_self_transition
#B2(self_t=danger|self_t-1=danger)=B2_safety_self_transition
B2_safety_self_transition = 1.0
B2[0] = np.zeros((len(safety_self_states_2), len(safety_self_states_2), len(null_action_2)))
B2[0][0,0,0] = B2_safety_self_transition
B2[0][1,1,0] = B2_safety_self_transition
B2[0][0,1,0] = 1-B2_safety_self_transition
B2[0][1,0,0] = 1-B2_safety_self_transition
B2[0] = utils.norm_dist(B2[0])
print(f"Is B2[0] normalized? : {utils.is_normalized(B2[0])}")

# B2[1] : State transitions for safety beliefs about the world. The `B2_safety_world_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
# akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
#B2(world_t=safe|world_t-1=safe)=B2_safety_world_transition
#B2(world_t=danger|world_t-1=danger)=B2_safety_world_transition
B2_safety_world_transition = 1.0
B2[1] = np.zeros((len(safety_world_states_2), len(safety_world_states_2), len(null_action_2)))
B2[1][0,0,0] = B2_safety_world_transition
B2[1][1,1,0] = B2_safety_world_transition
B2[1][0,1,0] = 1-B2_safety_world_transition
B2[1][1,0,0] = 1-B2_safety_world_transition
B2[1] = utils.norm_dist(B2[1])
print(f"Is B2[1] normalized? : {utils.is_normalized(B2[1])}")

# B2[2] : State transitions for safety beliefs about other people. The `B2_safety_other_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
# akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
#B2(other_t=safe|other_t-1=safe)=B2_safety_other_transition
#B2(other_t=danger|other_t-1=danger)=B2_safety_other_transition
B2_safety_other_transition = 1.0
B2[2] = np.zeros((len(safety_world_states_2), len(safety_world_states_2), len(null_action_2)))
B2[2][0,0,0] = B2_safety_other_transition
B2[2][1,1,0] = B2_safety_other_transition
B2[2][0,1,0] = 1-B2_safety_other_transition
B2[2][1,0,0] = 1-B2_safety_other_transition
B2[2] = utils.norm_dist(B2[2])
print(f"Is B2[2] normalized? : {utils.is_normalized(B2[2])}")

# C2: C matrix (preferences) for the higher level agent, which align with the hidden state factors at the lower level
# as the lower level's posterior inferences are passed upwards to the higher level as the latter's observations.
# This effectively makes C2 the higher level's preferences for hidden state posteriors at the lower level.
C2 = utils.obj_array(num_factors)    # hidden state factors at lower level are observation modalities at higher level
C2[0] = utils.norm_dist(np.array([0.5, 0.5]))   # no preference over trust
C2[1] = utils.norm_dist(np.array([0.5, 0.5]))   # no preference over correct card
C2[2] = utils.norm_dist(np.array([1, 0]))   # no preference over affect (angry, calm)
C2[3] = utils.norm_dist(np.array([1,1,1]))      # no preference over choice
C2[4] = utils.norm_dist(np.array([1,1,1]))   # no preference over stage
for i in range(len(C2)):
    print(f"Is C2[{i}] normalized? : {utils.is_normalized(C2[i])}")

# D2: D matrix (priors over initial hidden states) for the higher level agent. All safety beliefs are biased towards danger via the `prior_on_danger` parameter
# with probability 0.75.
D2 = utils.obj_array(num_factors_2)
prior_on_danger=0.75
D2[0] = utils.norm_dist(np.array([1-prior_on_danger, prior_on_danger]))   # priors over safety (self)
D2[1] = utils.norm_dist(np.array([1-prior_on_danger, prior_on_danger]))   # priors over safety (world)
D2[2] = utils.norm_dist(np.array([1-prior_on_danger, prior_on_danger]))   # priors over safety (others)
for i in range(len(D2)):
    print(f"Is D2[{i}] normalized? : {utils.is_normalized(D2[i])}")

# Matrix dirichlet distributions for learning A2, B2, and D2 for the higher level agent
pA2 = utils.dirichlet_like(A2, scale = 1.0)
pB2 = utils.dirichlet_like(B2, scale = 1.0)
pD2 = utils.dirichlet_like(D2, scale = 1.0)

# # Hierarchical inference test
# my_agent_high_test = Agent_he(A = A2, B = B2, C = C2, D = D2, pA=pA2, pB=pB2, pD=pD2,
#                  inference_algo="VANILLA", save_belief_hist = True,) # AP editted
# my_agent_high_D_prev_test = my_agent_high_test.D

# print(len(my_agent_high_test.qs_hist))
# my_agent_test = Agent_he(A = A, B = B, C = C, D = D, pA=pA, pB=pB, pD=pD,
#                  policies=policies, inference_algo="VANILLA", save_belief_hist = True,) # AP editted
# for t, obs_test in enumerate([[0,2,0,2],[0,0,1,0]]):     # [blue,null,high,null] then [blue,correct,low,blue]
#   qs_test = my_agent_test.infer_states(obs_test)
#   if t == 0:
#     q_pi_test, neg_efe_test = my_agent_test.infer_policies()
#   action = my_agent_test.sample_action_he(t)
#   print(f"t={t} : action={action}")
# my_agent_test.update_A(obs_test)
# print(f"my_agent_test.D (original) = {my_agent_test.D}")
# my_agent_test.update_D(qs_test)
# print(f"qs_test = {qs_test}")
# print(f"my_agent_test.D (update_D) = {my_agent_test.D}")
# qs_high_test = my_agent_high_test.infer_states(my_agent_test.qs, distr_obs = True)   # setting `distr_use = True` allows using a distribution (here, lower agent's qs) as an observation
# print(f"qs_high_test = {qs_high_test}")
# q_pi_2_test, neg_efe_2_test = my_agent_high_test.infer_policies()
# print(f"q_pi_2_test = {q_pi_2_test}")
# action_2_test = my_agent_high_test.sample_action_he(0)
# print(f"action_2_test = {action_2_test}")


# qs_pi_high_test = pymdp.control.get_expected_states(qs_high_test, B2, action_2_test.reshape(1,-1))   # note: might be unnecessary (depending on policies available(?))
# qo_pi_high_test = pymdp.control.get_expected_obs(qs_pi_high_test, A2)                                # if ^^^ is unnecessary, just use qs_pi=qs_high_test
# print(f"qs_pi_high_test = {qs_pi_high_test}")
# print(f"qo_pi_high_test = {qo_pi_high_test}")
# my_agent_test.D = qo_pi_high_test
# print(f"my_agent_test.D (from higher) = {my_agent_test.D}")
# my_agent_high_test.update_B(my_agent_high_D_prev_test)
# print(f"len(my_agent_high_test.qs_hist) = {len(my_agent_high_test.qs_hist)}")

# Define the logic for the CardAdvisor task

class CardAdvisor(object):

  def __init__(self, trustworthy_process = None, correct_card = None):
    """
    Simulates an environment for a card-advice trust task.

    The CardAdvisor class models an environment where an agent must select the color of an initially unseen card, where
    the card can only be blue or green.
    The advisor provides advice to the agent to select between two card colors ("blue" or "green") prior to revealing the true color.
    The advisor can be either trustworthy (gives correct advice) or untrustworthy (gives incorrect advice) relative to the true color of the card.
    The agent receives feedback as to whether the card was correct or not and simulated arousal responses based on outcomes.

    Args:
        trustworthy_process (str, optional): Advisor's trustworthiness for the episode.
            - "trustworthy": Advisor gives correct advice.
            - "untrustworthy": Advisor gives incorrect advice.
            - If None, randomly chosen at initialization.
        correct_card (str, optional): The correct card for the episode.
            - "blue" or "green".
            - If None, randomly chosen at initialization.

    Attributes:
        correct_card_names (list): Possible correct card colors ("blue", "green").
        trustworthy_names (list): Possible advisor types ("trustworthy", "untrustworthy").
        correct_card (str): The correct card for the current trial.
        trustworthy_process (str): Advisor's trustworthiness for the current trial.
        choice_obs (list): Possible observed card choices.
        feedback_obs (list): Possible feedback outcomes.
        stage_obs (list): Possible task stages.
        advice_obs (list): Possible advice options.
        arousal_obs (list): Possible arousal states ("high", "low").

    Public Methods:
        step(trust_action, card_action):
            Processes the agent's trust and card actions, returning a list of observations:
            [advice, feedback, arousal, choice].
    """
    self.correct_card_names = ["blue", "green"]
    self.trustworthy_names = ["trustworthy", "untrustworthy"]

    if correct_card == None:
      self.correct_card = self.correct_card_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample correct card color (blue or green)
    else:
      self.correct_card = correct_card

    if trustworthy_process == None:
      self.trustworthy_process = self.trustworthy_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample advisor trustworthiness (trustworthy, untrustworthy)
    else:
      self.trustworthy_process = trustworthy_process

    self.choice_obs = ['blue', 'green','null']
    self.feedback_obs = ['correct', 'incorrect', 'null']
    self.stage_obs = ['null', 'advice', 'decision']
    self.advice_obs = [ 'blue', 'green', 'null']
    self.arousal_obs = ['high', 'low']

  def step(self, trust_action,  card_action):
    """Process agent actions and return environment observations.
    The arguments pass the lower level agent's actions to the environment, advancing the stages of the trial.

    Args:
        trust_action (str): Trust-related action from agent. One of:
            - "trust": Trust advisor (outcome of 'low' arousal with probability 0.667, else returns 'high' arousal)
            - "distrust": Distrust advisor (outcome of 'high' arousal with probability 0.667, else returns 'low' arousal)
            - "null": No trust action (outcome of 'low' arousal with probability 0.5, else returns 'high' arousal)
        card_action (str): Card selection action. One of:
            - "blue"/"green": Card choice (during final stage, outcome of 'correct' if card choice is equivalent to true card color, else 'incorrect')
            - "null": No selection (null feedback/choice)

    Returns:
        list: Observation components [advice, feedback, arousal, choice]:
            - advice (str): Advisor's recommendation ("blue"/"green")
            - feedback (str): "correct"/"incorrect" if card played, else "null"
            - arousal (str): "high"/"low" emotional response
            - choice (str): Matches card_action or "null"

    Observation Logic:
        - Advice reflects advisor's trustworthiness (truthful vs. reversed)
        - Feedback shows if card matched environment's correct card
        - Arousal probabilities depend on trust_action type
        - Choice directly mirrors card_action input
    """

    if card_action == "null":    # if card action null, return null choice and feedback
      observed_choice = "null"
      observed_feedback = "null"

      #observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]
    elif card_action == "blue":
      observed_choice = "blue"   # map observed choice to card choice (observe self)
      if self.correct_card == "blue":
          observed_feedback = "correct"
      elif self.correct_card == "green":
          observed_feedback = "incorrect"
     # observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]
    elif card_action == "green":
      observed_choice = "green"
      if self.correct_card == "blue":
          observed_feedback = "incorrect"
      elif self.correct_card == "green":
          observed_feedback = "correct"
     # observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]

    if trust_action == "trust":
      #observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]
      observed_arousal = arousal_obs[utils.sample(np.array([0.3333, 0.6667]))]    # if choose to trust, 2/3 probability of low arousal
      if self.trustworthy_process == "trustworthy":
          if self.correct_card == "blue":
              observed_advice = "blue"            # if trustworthy, then match correct card to observed advice
          elif self.correct_card == "green":
              observed_advice = "green"
      elif self.trustworthy_process == "untrustworthy":
          if self.correct_card == "blue":
              observed_advice = "green"
          elif self.correct_card == "green":
              observed_advice = "blue"
    elif trust_action == "distrust":
      observed_arousal = arousal_obs[utils.sample(np.array([0.6667, 0.3333]))]   # if choose to trust, 2/3 probability of high arousal
      if self.trustworthy_process == "trustworthy":
          if self.correct_card == "blue":
              observed_advice = "blue"
          elif self.correct_card == "green":
              observed_advice = "green"
      elif self.trustworthy_process == "untrustworthy":
          if self.correct_card == "blue":
              observed_advice = "green"
          elif self.correct_card == "green":
              observed_advice = "blue"
    elif trust_action == "null":
      observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]   # if 'null' trust action, 1/2 probability of high arousal
      if self.trustworthy_process == "trustworthy":
          if self.correct_card == "blue":
              observed_advice = "blue"
          elif self.correct_card == "green":
              observed_advice = "green"
      elif self.trustworthy_process == "untrustworthy":
          if self.correct_card == "blue":
              observed_advice = "green"
          elif self.correct_card == "green":
              observed_advice = "blue"
    obs = [observed_advice, observed_feedback, observed_arousal, observed_choice]

    return obs

# # Define the full perception-action loop, incorporating the hierarchical agent and Card Advisor task, with arguments
# # for determining whether the lower lever and higher level agents should learn their respective matrices.

# def run_active_inference_loop(my_agent,                 # Lower level agent
#                               my_env,                   # Environment
#                               T = 3,                    # Number of timesteps per trial
#                               trial_results=False,      # Indicator to record results for trials
#                               learn_A=False,            # learn A at lower level
#                               learn_B=False,            # learn B at lower level
#                               learn_D=False,            # learn D at lower level
#                               my_agent_high = None,     # Supply higer level agent
#                               learn_A2=False,           # learn A2 at higher level
#                               learn_B2=False,           # learn B2 at higher level
#                               learn_D2=False            # learn D2 at higher level
#                               ):

#   """ Initialize the first observation """
#   arousal_obs_names = ['high', 'low']
#   obs_label = my_env.step("null","null")  # Transition (step) from first 'null' step of trial to second 'advice' step of trial, passing 'null' as both the agent's action and agent's card choice
#   obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return initial observations `obs` elicited from environment
#   qs_hist = my_agent.infer_states(obs)    # Lower level agent infers posterior beliefs (updates beliefs) based on initial observations `obs`
#   if trial_results == True:
#     results = {}

#   for t in range(T):
#     if t==0:
#       q_pi, neg_efe = my_agent.infer_policies()       # Lower level infers policies (computes posteriors over policies Q(pi) as well as negative EFE per policy)
#       chosen_action = my_agent.sample_action_he(t)    # Agent samples action from two-step policy chosen in t==0 (samples from the step in policy based on if t==0 or if t==1)
#       trust_action = choice_trust_actions[int(chosen_action[0])]    # Extract trust action (trust/distrust) from sampled policy for current timestep. The trust_action controls both trustworthiness_states and affect_states and is equivalent in each policy (always trust and trust or distrust and distrust)
#       card_action = choice_card_actions[int(chosen_action[3])]      # Extract card choice action (blue/green/null) from sampled policy for current timestep

#     else:
#       obs_label = my_env.step(trust_action,card_action)             # send trust_action and card_action to environment step to generate observations

#       obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return new observations from the environment

#       qs_hist = my_agent.infer_states(obs)  # Lower level agent infers posterior beliefs about hidden states (updates beliefs) based on new observation
#       print(qs_hist)
#       if t==1:
#         #q_pi, neg_efe = my_agent.infer_policies()    to ' neg_efe' # agent only infers policies at first timestep (infer_policies returns Q(policies) and negative EFE per policy)
#         chosen_action = my_agent.sample_action_he(t)  # agent samples action from two-step policy chosen in t==0 (samples from the step in policy based on if t==0 or if t==1)
#         trust_action = choice_trust_actions[int(chosen_action[0])]    # Extract trust action (trust/distrust)  from sampled policy for current timestep
#         card_action = choice_card_actions[int(chosen_action[3])]      # Extract card choice action (blue/green/null) from sampled policy for current timestep

#     if trial_results == True:
#       # Record trial results
#         results[t] = {'timestep' : t, 'qs_hist' : qs_hist, 'q_pi' : q_pi, 'efe' : neg_efe*-1, 'trust_action' : trust_action, 'card_action' : card_action, 'obs_label' : obs_label, 'obs' : obs,
#                       'D' : my_agent.D, 'true_trustworthiness' : my_env.trustworthy_process, 'true_color': my_env.correct_card, 'A':my_agent.A, 'B':my_agent.B}  # AP

#     if learn_B == True:
#       if t == 0:
#         qs_prev = my_agent.qs   # store `qs` (posterior beliefs) from 1st timestep to be used for learning B after 2nd timestep
#     if t == 2:
#       my_agent.step_time()

#   # After trial, learn A/B/D matrices / update higher level / learn A2/B2/D2 (where applicable)
#   if learn_A == True:
#     my_agent.update_A(obs)

#   if learn_B == True:
#     my_agent.update_B(qs_prev)

#   if learn_D == True:
#     my_agent.update_D(my_agent.qs)
#     # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
#     my_agent.D[1]=D[1]  # Reset to original priors over correct card states
#     my_agent.D[3]=D[3]  # Reset to original priors over card choice states
#     my_agent.D[4]=D[4]  # Reset to original priors over stage states

#   if trial_results == True:
#       results[T-1]['D'] = my_agent.D  # record results for learned D
#       results[T-1]['B'] = my_agent.B  # record results for learned B
#       results[T-1]['A'] = my_agent.A  # record results for learned A


#   if my_agent_high != None:
#     # Store higher level agent's previous matrices
#     my_agent_high_A_prev = my_agent_high.A
#     my_agent_high_B_prev = my_agent_high.B
#     my_agent_high_C_prev = my_agent_high.C
#     my_agent_high_D_prev = my_agent_high.D
#     # if hierarchical agent already has a qs history, store currently expected obs for result storing `qo_pi_high` in results for trial's first timestep (otherwise, initialize with higher level agent's preferences my_agent_high.C i.e. C2)
#     if len(my_agent_high.qs_hist) > 0:
#       qo_pi_high_prev = pymdp.control.get_expected_obs(my_agent_high.qs, my_agent_high.A)   # Compute expected obs at higher level, given new posteriors
#       qo_pi_high_prev = [subarray / subarray.sum() for subarray in qo_pi_high_prev[0]]   # Extract expected obs at current timestep, normalize each distribution
#     else:
#       qo_pi_high_prev = my_agent_high_C_prev
#     qs_high = my_agent_high.infer_states(my_agent.qs, distr_obs = True)   # higher level agent updates posterior beliefs using the lower level agent's posterior belief distribution as observations for the higher level
#     q_pi_high, neg_efe_high = my_agent_high.infer_policies()              # higher level agent infers its posterior beliefs about policies (a necessary step despite the higher level agent only having a single 'NULL' action)
#     action_high = my_agent_high.sample_action_he(0)                       # higher level agent samples action from its posterior beliefs about policies ()
#     qo_pi_high = pymdp.control.get_expected_obs(qs_high, my_agent_high.A)   # compute expected obs at higher level, given new posteriors
#     qo_pi_high = [subarray / subarray.sum() for subarray in qo_pi_high[0]]   # extract expected observations from higher level agent at current timestep, normalize each distribution
#     # print(f"qo_pi_high (empirical priors for my_agent.D) = {qo_pi_high}")
#     # print(f"qo_pi_high type = {type(qo_pi_high)}")
#     # print(f"my_agent.D before inheriting empirical priors = {my_agent.D}")
#     my_agent.D = qo_pi_high    # lower level inherits empirical priors from higher level
#     # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
#     my_agent.D[1]=D[1]  # Reset to original priors over correct card states
#     my_agent.D[3]=D[3]  # Reset to original priors over card choice states
#     my_agent.D[4]=D[4]  # Reset to original priors over stage states
#     if trial_results == True:
#       results[T-1]['D'] = my_agent.D

#   if my_agent_high != None and learn_A2 == True:
#     my_agent_high.update_A(my_agent.qs)    # higher level agent learns its A matrix at end of trial

#   if my_agent_high != None and learn_B2 == True:
#     # print("Learning B2:")
#     # print(f"my_agent_high_D_prev = {my_agent_high_D_prev}")
#     # print(f"qs_high = {qs_high}")
#     my_agent_high.update_B(my_agent_high_D_prev)    # higher level agent learns its B matrix at end of trial
#   if my_agent_high != None and learn_D2 == True:
#     my_agent_high.update_D(my_agent_high.qs)        # higher level agent learns its D matrix at end of trial
#   if my_agent_high != None and trial_results == True:
#     for t in range(T-1):
#       # Record results for higher level agent. Since the results are stored for each timestep per trial but the higher level agent's learning and updating
#       # only occur at the end of the trial, the higher level agent's results will be equivalent for the first two timesteps of the trial and only change
#       # for the final timestep.
#       tt=t
#       results[tt]['A2'] = my_agent_high_A_prev
#       results[tt]['B2'] = my_agent_high_B_prev
#       results[tt]['D2'] = my_agent_high_D_prev
#       results[tt]['qs_high_hist'] = my_agent_high_D_prev
#       results[tt]['qo_pi_high'] = qo_pi_high_prev

#     results[T-1]['A2'] = my_agent_high.A
#     results[T-1]['B2'] = my_agent_high.B
#     results[T-1]['D2'] = my_agent_high.D
#     results[T-1]['qs_high_hist'] = my_agent_high.qs
#     results[T-1]['qo_pi_high'] = qo_pi_high

#   if trial_results == True:
#     return results

# import inspect
# from pymdp.agent import Agent

# print(inspect.getsource(Agent))

# Define the full perception-action loop, incorporating the hierarchical agent and Card Advisor task, with arguments
# for determining whether the lower lever and higher level agents should learn their respective matrices.

def run_active_inference_loop(my_agent,                 # Lower level agent
                              my_env,                   # Environment
                              T = 3,                    # Number of timesteps per trial
                              trial_results=False,      # Indicator to record results for trials
                              learn_A=False,            # learn A at lower level
                              learn_B=False,            # learn B at lower level
                              learn_D=False,            # learn D at lower level
                              my_agent_high = None,     # Supply higer level agent
                              learn_A2=False,           # learn A2 at higher level
                              learn_B2=False,           # learn B2 at higher level
                              learn_D2=False            # learn D2 at higher level
                              ):

  """ Initialize the first observation """
  arousal_obs_names = ['high', 'low']
  obs_label = my_env.step("null","null")  # Transition (step) from first 'null' step of trial to second 'advice' step of trial, passing 'null' as both the agent's action and agent's card choice
  obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return initial observations `obs` elicited from environment
  qs_hist = my_agent.infer_states(obs)    # Lower level agent infers posterior beliefs (updates beliefs) based on initial observations `obs`
  if trial_results == True:
    results = {}

  for t in range(T):
    if t==0:
      q_pi, neg_efe = my_agent.infer_policies()       # Lower level infers policies (computes posteriors over policies Q(pi) as well as negative EFE per policy)
      chosen_action = my_agent.sample_action_he(t)    # Agent samples action from two-step policy chosen in t==0 (samples from the step in policy based on if t==0 or if t==1)
      trust_action = choice_trust_actions[int(chosen_action[0])]    # Extract trust action (trust/distrust) from sampled policy for current timestep. The trust_action controls both trustworthiness_states and affect_states and is equivalent in each policy (always trust and trust or distrust and distrust)
      card_action = choice_card_actions[int(chosen_action[3])]      # Extract card choice action (blue/green/null) from sampled policy for current timestep

    else:
      obs_label = my_env.step(trust_action,card_action)             # send trust_action and card_action to environment step to generate observations

      obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return new observations from the environment

      qs_hist = my_agent.infer_states(obs)  # Lower level agent infers posterior beliefs about hidden states (updates beliefs) based on new observation
      print(qs_hist)
      if t==1:
        # AP EDIT 30MAY2025 : uncommented second 'infer_policies()'
        q_pi, neg_efe = my_agent.infer_policies()    # to ' neg_efe' # agent only infers policies at first timestep (infer_policies returns Q(policies) and negative EFE per policy)
        chosen_action = my_agent.sample_action_he(t)  # agent samples action from two-step policy chosen in t==0 (samples from the step in policy based on if t==0 or if t==1)
        trust_action = choice_trust_actions[int(chosen_action[0])]    # Extract trust action (trust/distrust)  from sampled policy for current timestep
        card_action = choice_card_actions[int(chosen_action[3])]      # Extract card choice action (blue/green/null) from sampled policy for current timestep

    if trial_results == True:
      # Record trial results
        results[t] = {'timestep' : t, 'qs_hist' : qs_hist, 'q_pi' : q_pi, 'efe' : neg_efe*-1, 'trust_action' : trust_action, 'card_action' : card_action, 'obs_label' : obs_label, 'obs' : obs,
                      'D' : my_agent.D, 'true_trustworthiness' : my_env.trustworthy_process, 'true_color': my_env.correct_card, 'A':my_agent.A, 'B':my_agent.B}  # AP

    if learn_B == True:
      if t == 0:
        qs_prev = my_agent.qs   # store `qs` (posterior beliefs) from 1st timestep to be used for learning B after 2nd timestep
    if t == 2:
      my_agent.step_time()

  # After trial, learn A/B/D matrices / update higher level / learn A2/B2/D2 (where applicable)
  if learn_A == True:
    my_agent.update_A(obs)

  if learn_B == True:
    my_agent.update_B(qs_prev)

  if learn_D == True:
    my_agent.update_D(my_agent.qs)
    # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
    my_agent.D[1]=D[1]  # Reset to original priors over correct card states
    my_agent.D[3]=D[3]  # Reset to original priors over card choice states
    my_agent.D[4]=D[4]  # Reset to original priors over stage states

  if trial_results == True:
      results[T-1]['D'] = my_agent.D  # record results for learned D
      results[T-1]['B'] = my_agent.B  # record results for learned B
      results[T-1]['A'] = my_agent.A  # record results for learned A


  if my_agent_high != None:
    # Store higher level agent's previous matrices
    my_agent_high_A_prev = my_agent_high.A
    my_agent_high_B_prev = my_agent_high.B
    my_agent_high_C_prev = my_agent_high.C
    my_agent_high_D_prev = my_agent_high.D
    # if hierarchical agent already has a qs history, store currently expected obs for result storing `qo_pi_high` in results for trial's first timestep (otherwise, initialize with higher level agent's preferences my_agent_high.C i.e. C2)
    if len(my_agent_high.qs_hist) > 0:
      qo_pi_high_prev = pymdp.control.get_expected_obs(my_agent_high.qs, my_agent_high.A)   # Compute expected obs at higher level, given new posteriors
      qo_pi_high_prev = [subarray / subarray.sum() for subarray in qo_pi_high_prev[0]]   # Extract expected obs at current timestep, normalize each distribution
    else:
      qo_pi_high_prev = my_agent_high_C_prev
    qs_high = my_agent_high.infer_states(my_agent.qs, distr_obs = True)   # higher level agent updates posterior beliefs using the lower level agent's posterior belief distribution as observations for the higher level
    q_pi_high, neg_efe_high = my_agent_high.infer_policies()              # higher level agent infers its posterior beliefs about policies (a necessary step despite the higher level agent only having a single 'NULL' action)
    action_high = my_agent_high.sample_action_he(0)                       # higher level agent samples action from its posterior beliefs about policies ()
    qo_pi_high = pymdp.control.get_expected_obs(qs_high, my_agent_high.A)   # compute expected obs at higher level, given new posteriors
    qo_pi_high = [subarray / subarray.sum() for subarray in qo_pi_high[0]]   # extract expected observations from higher level agent at current timestep, normalize each distribution
    # print(f"qo_pi_high (empirical priors for my_agent.D) = {qo_pi_high}")
    # print(f"qo_pi_high type = {type(qo_pi_high)}")
    # print(f"my_agent.D before inheriting empirical priors = {my_agent.D}")
    my_agent.D = qo_pi_high    # lower level inherits empirical priors from higher level
    # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
    my_agent.D[1]=D[1]  # Reset to original priors over correct card states
    my_agent.D[3]=D[3]  # Reset to original priors over card choice states
    my_agent.D[4]=D[4]  # Reset to original priors over stage states
    if trial_results == True:
      results[T-1]['D'] = my_agent.D

  if my_agent_high != None and learn_A2 == True:
    my_agent_high.update_A(my_agent.qs)    # higher level agent learns its A matrix at end of trial

  if my_agent_high != None and learn_B2 == True:
    # print("Learning B2:")
    # print(f"my_agent_high_D_prev = {my_agent_high_D_prev}")
    # print(f"qs_high = {qs_high}")
    my_agent_high.update_B(my_agent_high_D_prev)    # higher level agent learns its B matrix at end of trial
  if my_agent_high != None and learn_D2 == True:
    my_agent_high.update_D(my_agent_high.qs)        # higher level agent learns its D matrix at end of trial
  if my_agent_high != None and trial_results == True:
    for t in range(T-1):
      # Record results for higher level agent. Since the results are stored for each timestep per trial but the higher level agent's learning and updating
      # only occur at the end of the trial, the higher level agent's results will be equivalent for the first two timesteps of the trial and only change
      # for the final timestep.
      tt=t
      results[tt]['A2'] = my_agent_high_A_prev
      results[tt]['B2'] = my_agent_high_B_prev
      results[tt]['D2'] = my_agent_high_D_prev
      results[tt]['qs_high_hist'] = my_agent_high_D_prev
      results[tt]['qo_pi_high'] = qo_pi_high_prev

    results[T-1]['A2'] = my_agent_high.A
    results[T-1]['B2'] = my_agent_high.B
    results[T-1]['D2'] = my_agent_high.D
    results[T-1]['qs_high_hist'] = my_agent_high.qs
    results[T-1]['qo_pi_high'] = qo_pi_high

  if trial_results == True:
    return results

results_1 = {}     # initialize Dict for storing results (see in run_active_inference_loop)

env2 = CardAdvisor(trustworthy_process ="untrustworthy", correct_card= "green")    # Initialize environment (untrustworthy: advisor will give blue advice but correct card is green)

n_trials = 30    # Set number of trials for experiment
n_timesteps=3    # Set number of timesteps per trial (must be 3 for this paradigm)

# Initialize lower level agent
my_agent = Agent_he(A = A, B = B, C = C, D = D, E = E, pA=pA, pB=pB, pD=pD,
                 policies=policies, inference_algo="VANILLA", save_belief_hist = True,)

arousal_obs_names = ['high', 'low']
obs_label = ["null", "null","low", "null"]   # named obs for agent to begin trials (note: forces low arousal obs)
trust_action=choice_trust_actions[utils.sample(np.array([0.5, 0.5]))]
obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]  #encode observations in obs_label to numeric indices for agent to read
for trial in range(n_trials):
    # Run active inference loop
    results_trial = run_active_inference_loop(my_agent, CardAdvisor(trustworthy_process ="untrustworthy", correct_card=correct_card_states[utils.sample(np.array([0.5, 0.5]))]), T = n_timesteps, trial_results=True,
                                              learn_A=True, learn_B=True, learn_D=True)
    results_1[trial] = results_trial  # Store all trial results

env_untrustworthy = CardAdvisor(trustworthy_process ="untrustworthy")
env_trustworthy = CardAdvisor(trustworthy_process ="trustworthy")    # Initialize trustworthy environment, random correct card color
print(f'Context: {env2.trustworthy_process}, {env2.correct_card}')


results_2 = {}     # Initialize Dict for storing results (see in run_active_inference_loop)
n_trials1 = 20     # Set number of trials for first period (untrustworthy environment)
n_trials2 = 60     # Set number of trials for second period (trustworthy environment)
n_timesteps=3      # Set number of timesteps per trial (must be 3 for this paradigm)

my_agent = Agent_he(A = A, B = B, C = C, D = D, E = E, policies=policies, inference_algo="VANILLA", save_belief_hist = True, pA = pA, pB = pB, pD = pD)  # AP added

# arousal_obs_names = ['high', 'low']
# obs_label = ["null", "null","low", "null"]
# trust_action=choice_trust_actions[utils.sample(np.array([0.5, 0.5]))]
# obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]
for trial in range(n_trials1):
    # Run active inference loop
    results_trial = run_active_inference_loop(my_agent, CardAdvisor(trustworthy_process ="untrustworthy", correct_card=correct_card_states[utils.sample(np.array([0.5, 0.5]))]), T = n_timesteps, trial_results=True,
                                              learn_A=True,learn_B=True,learn_D=True) # AP modified
    results_2[trial] = results_trial  # Store all trial results

for trial in range(n_trials2):
    # Run active inference loop
    results_trial = run_active_inference_loop(my_agent, CardAdvisor(trustworthy_process ="trustworthy", correct_card=correct_card_states[utils.sample(np.array([0.5, 0.5]))]), T = n_timesteps, trial_results=True,
                                              learn_A=True,learn_B=True,learn_D=True) # AP modified
    results_2[trial+n_trials1] = results_trial  # Store all trial results

# env_untrustworthy = CardAdvisor(trustworthy_process ="untrustworthy")   # AP commented out
# env_trustworthy = CardAdvisor(trustworthy_process ="trustworthy")    # AP # Initialize trustworthy environment, random correct card color
print(f'Context: {env2.trustworthy_process}, {env2.correct_card}')

results_3 = {}     # Initialize Dict for storing results (see in run_active_inference_loop)
n_trials1 = 20     # Set number of trials for first period (untrustworthy environment)
n_trials2 = 60     # Set number of trials for second period (trustworthy environment)
n_timesteps=3      # Set number of timesteps per trial (must be 3 for this paradigm)


pD = utils.dirichlet_like(D, scale = 1.0)    # AP added
my_agent = Agent_he(A = A, B = B, C = C, D = D, E = E, action_selection="deterministic", alpha = 2.0, policies=policies, inference_algo="VANILLA", save_belief_hist = True, pA = pA, pB = pB, pD = pD)  # AP added
my_agent_high = Agent_he(A=A2, B=B2, C=C2, D=D2, action_selection="deterministic", alpha = 2.0, inference_algo="VANILLA", save_belief_hist=True, pA=pA2, pB=pB2, pD=pD2)

#run_active_inference_loop(my_agent, env, T = 10)
# arousal_obs_names = ['high', 'low']
# obs_label = ["null", "null","low", "null"]
# trust_action=choice_trust_actions[utils.sample(np.array([0.5, 0.5]))]
# obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]
for trial in range(n_trials1):
    results_trial = run_active_inference_loop(my_agent, CardAdvisor(trustworthy_process ="untrustworthy", correct_card=correct_card_states[utils.sample(np.array([0.5, 0.5]))]), T = n_timesteps, trial_results=True,
                                              learn_A=True,learn_B=True,learn_D=True,my_agent_high=my_agent_high,learn_A2=True,learn_B2=False,learn_D2=True) # AP modified
    results_3[trial] = results_trial
for trial in range(n_trials2):

   # run a single trial
    results_trial = run_active_inference_loop(my_agent, CardAdvisor(trustworthy_process ="trustworthy", correct_card=correct_card_states[utils.sample(np.array([0.5, 0.5]))]), T = n_timesteps, trial_results=True,
                                              learn_A=True,learn_B=True,learn_D=True,my_agent_high=my_agent_high,learn_A2=True,learn_B2=True,learn_D2=True) # AP modified
    results_3[trial+n_trials] = results_trial

# Define functions for processing and plotting simulation results
import pandas as pd
import numpy as np

# plt.rc('font', size=20)          # default text size
# plt.rc('axes', titlesize=20)     # title font size
# plt.rc('axes', labelsize=20)     # x and y label font size
# plt.rc('xtick', labelsize=20)    # x-axis tick labels
# plt.rc('ytick', labelsize=20)    # y-axis tick labels

def process_simulation_results(results):
  """ Input results dictionary to process as a pandas DataFrame"""
  def process_array(arr):
      if isinstance(arr, np.ndarray):
          if arr.dtype == object:
              # Handle arrays of arrays
              return [process_array(x) if isinstance(x, np.ndarray)
                    else x.tolist() if isinstance(x, np.ndarray)
                    else x for x in arr]
          else:
              # Handle numeric arrays
              return np.round(arr, 2).tolist()
      elif isinstance(arr, list):
          return [process_array(x) if isinstance(x, np.ndarray)
                  else x for x in arr]
      return arr

  # Initialize an empty list to store dataframes
  dataframes = []

  # Loop through trials and timesteps
  for trial, timesteps in results.items():
      for timestep, data in timesteps.items():
          # Create a copy of the data to avoid modifying the original
          processed_data = {
              'trial': trial,
              'timestep': timestep
          }

          # Process each field in the timestep data
          for key, value in data.items():
              if isinstance(value, (np.ndarray, list)):
                  processed_data[key] = process_array(value)
              else:
                  processed_data[key] = value

          # Append to list of dataframes
          dataframes.append(pd.DataFrame([processed_data]))

  # Concatenate all dataframes
  final_df = pd.concat(dataframes, ignore_index=True)
  return final_df

def plot_agent_data(final_df, hierarchical=False):
    # Set up the figure and axes - 8 subplots in each column
    if hierarchical == False:
      fig, axes = plt.subplots(nrows=11, ncols=2, figsize=(15, 20), sharex=True)
    else:
    #   fig, axes = plt.subplots(nrows=13, ncols=2, figsize=(30, 40), sharex=True)
    # fig.subplots_adjust(hspace=0.2, wspace=0.2)
      fig, axes = plt.subplots(nrows=13, ncols=2, figsize=(15, 25), sharex=True)
    fig.subplots_adjust(hspace=0.5, wspace=0.3)

    # Get timesteps for x-axis
    timesteps = range(len(final_df))

    # Function to add grid lines and set y-axis for probability plots
    def setup_probability_plot(ax, title):
        ax.set_ylim(0, 1)
        ax.set_xticks(np.arange(0, len(timesteps), 2))  # Vertical grid lines at even timesteps
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_ylabel('Probability')

    # Plot prior beliefs (D)
    setup_probability_plot(axes[0, 0], 'Prior Probability of Trust')
    prior_trust = [d[0][0] for d in final_df['D']]
    axes[0, 0].plot(timesteps, prior_trust, label='Prior Trust', color='blue')
    axes[0, 0].legend()

    setup_probability_plot(axes[1, 0], 'Prior Probability of Blue Card')
    prior_blue = [d[1][0] for d in final_df['D']]
    axes[1, 0].plot(timesteps, prior_blue, label='Prior Blue Card', color='blue')
    axes[1, 0].legend()

    setup_probability_plot(axes[2, 0], 'Prior Probability of Angry State')
    prior_angry = [d[2][0] for d in final_df['D']]
    axes[2, 0].plot(timesteps, prior_angry, label='Prior Angry', color='red')
    axes[2, 0].legend()

    setup_probability_plot(axes[3, 0], 'Prior Choice Probabilities')
    prior_blue_choice = [d[3][0] for d in final_df['D']]
    prior_green_choice = [d[3][1] for d in final_df['D']]
    axes[3, 0].plot(timesteps, prior_blue_choice, label='Prior Blue Choice', color='blue')
    axes[3, 0].plot(timesteps, prior_green_choice, label='Prior Green Choice', color='green')
    axes[3, 0].legend()

    setup_probability_plot(axes[4, 0], 'Prior Stage Probabilities')
    prior_null = [d[4][0] for d in final_df['D']]
    prior_advice = [d[4][1] for d in final_df['D']]
    prior_decision = [d[4][2] for d in final_df['D']]
    axes[4, 0].plot(timesteps, prior_null, label='Prior Null', color='grey')
    axes[4, 0].plot(timesteps, prior_advice, label='Prior Advice', color='orange')
    axes[4, 0].plot(timesteps, prior_decision, label='Prior Decision', color='purple')
    axes[4, 0].legend()

    # Plot posterior beliefs (qs_hist)
    setup_probability_plot(axes[0, 1], 'Posterior Probability of Trust')
    trust_probs = [qs[0][0] for qs in final_df['qs_hist']]
    axes[0, 1].plot(timesteps, trust_probs, label='Trust', color='blue')
    axes[0, 1].legend()

    setup_probability_plot(axes[1, 1], 'Posterior Probability of Blue Card')
    blue_probs = [qs[1][0] for qs in final_df['qs_hist']]
    axes[1, 1].plot(timesteps, blue_probs, label='Blue Card', color='blue')
    axes[1, 1].legend()

    setup_probability_plot(axes[2, 1], 'Posterior Probability of Angry State')
    angry_probs = [qs[2][0] for qs in final_df['qs_hist']]
    axes[2, 1].plot(timesteps, angry_probs, label='Angry', color='red')
    axes[2, 1].legend()

    setup_probability_plot(axes[3, 1], 'Posterior Choice Probabilities')
    blue_choice_probs = [qs[3][0] for qs in final_df['qs_hist']]
    green_choice_probs = [qs[3][1] for qs in final_df['qs_hist']]
    null_choice_probs = [qs[3][2] for qs in final_df['qs_hist']]
    axes[3, 1].plot(timesteps, blue_choice_probs, label='Blue Choice', color='blue')
    axes[3, 1].plot(timesteps, green_choice_probs, label='Green Choice', color='green')
    axes[3, 1].plot(timesteps, null_choice_probs, label='Null Choice', color='gray')
    axes[3, 1].legend()

    setup_probability_plot(axes[4, 1], 'Posterior Stage Probabilities')
    null_probs = [qs[4][0] for qs in final_df['qs_hist']]
    advice_probs = [qs[4][1] for qs in final_df['qs_hist']]
    decision_probs = [qs[4][2] for qs in final_df['qs_hist']]
    axes[4, 1].plot(timesteps, null_probs, label='Null', color='grey')
    axes[4, 1].plot(timesteps, advice_probs, label='Advice', color='orange')
    axes[4, 1].plot(timesteps, decision_probs, label='Decision', color='purple')
    axes[4, 1].legend()

    # Function to setup action plots
    def setup_action_plot(ax, title, actions, color_map):
        ax.set_xticks(np.arange(0, len(timesteps), 2))  # Vertical grid lines at even timesteps
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_yticks([])
        colors = [color_map[action] for action in actions]
        scatter = ax.scatter(timesteps, [1]*len(timesteps), c=colors)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',
                           label=action, markerfacecolor=color, markersize=10)
                           for action, color in color_map.items()]
        ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))

    # Plot trust actions
    setup_action_plot(axes[5, 0], 'Trust Actions', final_df['trust_action'],
                      {'trust': 'blue', 'distrust': 'red'})

    # Plot card actions
    setup_action_plot(axes[6, 0], 'Card Actions', final_df['card_action'],
                      {'blue': 'blue', 'green': 'green', 'null': 'grey'})

    # Plot true trustworthiness
    setup_action_plot(axes[5, 1], 'True Trustworthiness', final_df['true_trustworthiness'],
                      {'trustworthy': 'blue', 'untrustworthy': 'red'})

    # Plot true color
    setup_action_plot(axes[6, 1], 'True Color', final_df['true_color'],
                      {'blue': 'blue', 'green': 'green'})

    # Plot expected free energies (efe)
    axes[7, 0].set_xticks(np.arange(0, len(timesteps), 2))  # Vertical grid lines at even timesteps
    axes[7, 0].grid(True, alpha=0.3)
    axes[7, 0].set_title('Expected Free Energy')
    axes[7, 0].set_ylabel('EFE')
    colors = ['green', 'blue', 'red', 'orange','purple','grey']
    labels = ['Trust+Green', 'Trust+Blue', 'Distrust+Blue', 'Distrust+Green','trust+null','Distrust+null']
    for i in range(6):
        efe_values = [e[i] for e in final_df['efe']]
        axes[7, 0].plot(timesteps, efe_values, label=labels[i], color=colors[i])
    axes[7, 0].legend()

    # Plot policy probabilities (q_pi)
    setup_probability_plot(axes[7, 1], 'Policy Probabilities')
    for i in range(6):
        policy_probs = [q[i] for q in final_df['q_pi']]
        axes[7, 1].plot(timesteps, policy_probs, label=labels[i], color=colors[i])
    axes[7, 1].legend()



    # Plot advice likelihood probabilities (A[0])
    setup_probability_plot(axes[8, 0], 'Advice Likelihood')
    A_TrueAdvice_Trust=0
    A_FalseAdvice_Distrust=0
    A_FalseAdvice_Trust=0
    A_TrueAdvice_Distrust=0

    for affect in range(len(affect_states)):
      for choice in range(len(choice_states)):
        A_TrueAdvice_Trust=A_TrueAdvice_Trust+np.array([a[0][0][0][0][affect][choice][1] for a in final_df['A']])+np.array([a[0][1][0][1][affect][choice][1] for a in final_df['A']])
        A_FalseAdvice_Distrust=A_FalseAdvice_Distrust+np.array([a[0][0][1][1][affect][choice][1] for a in final_df['A']])+np.array([a[0][1][1][0][affect][choice][1] for a in final_df['A']])
        A_FalseAdvice_Trust=A_TrueAdvice_Trust+np.array([a[0][0][0][1][affect][choice][1] for a in final_df['A']])+np.array([a[0][0][0][1][affect][choice][1] for a in final_df['A']])
        A_TrueAdvice_Distrust=A_FalseAdvice_Distrust+np.array([a[0][1][1][1][affect][choice][1] for a in final_df['A']])+np.array([a[0][0][1][0][affect][choice][1] for a in final_df['A']])

    A_TrueAdvice_Trust=A_TrueAdvice_Trust/(len(affect_states)*len(choice_states)*2)
    A_FalseAdvice_Distrust=A_FalseAdvice_Distrust/(len(affect_states)*len(choice_states)*2)
    A_FalseAdvice_Trust=A_FalseAdvice_Trust/(len(affect_states)*len(choice_states)*2)
    A_TrueAdvice_Distrust=A_TrueAdvice_Distrust/(len(affect_states)*len(choice_states)*2)

    axes[8, 0].plot(timesteps, A_TrueAdvice_Trust, label='A_TrueAdvice_Trust', color=colors[0])
    axes[8, 0].plot(timesteps, A_FalseAdvice_Distrust, label='A_FalseAdvice_Distrust', color=colors[1])
    axes[8, 0].plot(timesteps, A_FalseAdvice_Trust, label='A_TrueAdvice_Trust', color=colors[2])
    axes[8, 0].plot(timesteps, A_TrueAdvice_Distrust, label='A_TrueAdvice_Distrust', color=colors[3])
    axes[8, 0].legend()

    # Plot feedback likelihood probabilities (A[1])
    setup_probability_plot(axes[8, 1], 'Feedback Likelihood')
    A_Correct_Correct=0 # feedback: correct/ choice=correct_card
    A_Incorrect_Incorrect=0 # feedback: incorrect/ choice~=correct_card
    A_Incorrect_Correct=0 # feedback: incorrect/ choice=correct_card
    A_Correct_Incorrect=0 # feedback: correct/ choice~=correct_card
    for trust in range(len(trustworthiness_states)):
        for affect in range(len(affect_states)):
          A_Correct_Correct=A_Correct_Correct+np.array([a[1][0][trust][0][affect][0][2] for a in final_df['A']])+np.array([a[1][0][trust][1][affect][1][2] for a in final_df['A']])
          A_Incorrect_Incorrect=A_Incorrect_Incorrect+np.array([a[1][1][trust][0][affect][1][2] for a in final_df['A']])+np.array([a[1][1][trust][1][affect][0][2] for a in final_df['A']])
          A_Incorrect_Correct=A_Incorrect_Correct+np.array([a[1][1][trust][0][affect][0][2] for a in final_df['A']])+np.array([a[1][1][trust][1][affect][1][2] for a in final_df['A']])
          A_Correct_Incorrect=A_Correct_Incorrect+np.array([a[1][0][trust][0][affect][1][2] for a in final_df['A']])+np.array([a[1][0][trust][1][affect][0][2] for a in final_df['A']])

    A_Correct_Correct=A_Correct_Correct/(len(affect_states)*len(trustworthiness_states)*2)
    A_Incorrect_Incorrect=A_Incorrect_Incorrect/(len(affect_states)*len(trustworthiness_states)*2)
    A_Incorrect_Correct=A_Incorrect_Correct/(len(affect_states)*len(trustworthiness_states)*2)
    A_Correct_Incorrect=A_Correct_Incorrect/(len(affect_states)*len(trustworthiness_states)*2)

    axes[8, 1].plot(timesteps, A_Correct_Correct, label='A_Correct_Correct', color=colors[0])
    axes[8, 1].plot(timesteps, A_Incorrect_Incorrect, label='A_Incorrect_Incorrect', color=colors[1])
    axes[8, 1].plot(timesteps, A_Incorrect_Correct, label='A_InCorrect_Correct', color=colors[2])
    axes[8, 1].plot(timesteps, A_Correct_Incorrect, label='A_Correct_Incorrect', color=colors[3])
    axes[8, 1].legend()

    # Observed advice
    def setup_obs_plot(ax, title, obss, color_map):
        ax.set_xticks(np.arange(0, len(timesteps),3))  # Vertical grid lines at even timesteps
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_yticks([])
        colors = [color_map[obs] for obs in obss]
        scatter = ax.scatter(timesteps, [1]*len(timesteps), c=colors)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',
                           label=obs, markerfacecolor=color, markersize=5)
                           for obs, color in color_map.items()]
        ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))

    obs_advice = [row[0] for row in final_df['obs_label']]
    setup_obs_plot(axes[9, 0], 'Observed advice', obs_advice,
                      {'blue': 'blue', 'green': 'green', 'null': 'grey'})

    # Observed feedback


    obs_feedback = [row[1] for row in final_df['obs_label']]
    setup_obs_plot(axes[9, 1], 'Observed feedback', obs_feedback,
                      {'correct': 'black', 'incorrect': 'orange', 'null': 'grey'})

     # Inferred Correct Card


    state_cc = [row[1] for row in final_df['qs_hist']]
    state_cc_idx=np.zeros(len(state_cc))
    state_cc_label=["" for x in range(len(state_cc))]
    for i in range(len(state_cc)):
        state_cc_idx = state_cc[i].index(max(state_cc[i]))
        state_cc_label[i]= correct_card_states[state_cc_idx]
    setup_obs_plot(axes[10, 0], 'Inferred Correct Card', state_cc_label,
                      {'blue': 'blue', 'green': 'green'})

    # Plot stage transition probabilities (B[0])
    setup_probability_plot(axes[10, 1], 'stage transition')
    labels=['null<-null','null<-advice','null<-decision','advice<-null','advice<-advice','advice<-decision','decision<-null','decision<-advice','decision<-decision']
    colors=['grey','orange','purple','blue','green','black','red','yellow','pink']
    i=0
    for next_stage in range(len(stage_states)):
      for current_stage in range(len(stage_states)):
        stage_trans = [b[4][next_stage][current_stage][0] for b in final_df['B']]
        axes[10, 1].plot(timesteps, stage_trans, label=labels[i], color=colors[i])
        i=i+1
    axes[10,1].legend()


    if hierarchical == True:
      setup_probability_plot(axes[11, 0], 'Hierarchical Prior Safety Beliefs P(D)')
      safety_self_high = [d[0][0] for d in final_df['D2']]
      safety_world_high = [d[1][0] for d in final_df['D2']]
      safety_other_high = [d[2][0] for d in final_df['D2']]
      axes[11, 0].plot(timesteps, safety_self_high, label='Safe (Self)', color='grey')
      axes[11, 0].plot(timesteps, safety_world_high, label='Safe (World)', color='orange')
      axes[11, 0].plot(timesteps, safety_other_high, label='Safe (Others)', color='purple')
      axes[11, 0].legend()

      setup_probability_plot(axes[11, 1], 'Hierarchical Posterior Observations Q(o)')
      trustworthiness_qo = [qo[0][0] for qo in final_df['qo_pi_high']]
      affect_qo = [qo[2][0] for qo in final_df['qo_pi_high']]
      correct_qo = [qo[1][0] for qo in final_df['qo_pi_high']]
      axes[11, 1].plot(timesteps, trustworthiness_qo, label='Trustworthy Expectation', color='grey')
      axes[11, 1].plot(timesteps, affect_qo, label='Angry Expectation', color='orange')
      axes[11, 1].plot(timesteps, correct_qo, label='Blue Correct Expectation', color='purple')
      axes[11, 1].legend()

      setup_probability_plot(axes[12, 0], 'Hierarchical Posterior Safety Beliefs Q(s)')
      safety_self_high = [qs[0][0] for qs in final_df['qs_high_hist']]
      safety_world_high = [qs[1][0] for qs in final_df['qs_high_hist']]
      safety_other_high = [qs[2][0] for qs in final_df['qs_high_hist']]
      axes[12, 0].plot(timesteps, safety_self_high, label='Safe (Self)', color='grey')
      axes[12, 0].plot(timesteps, safety_world_high, label='Safe (World)', color='orange')
      axes[12, 0].plot(timesteps, safety_other_high, label='Safe (Others)', color='purple')
      axes[12, 0].legend()

      setup_probability_plot(axes[12, 1], 'Hierarchical Likelihood Q(A)')
      trust_allSafe_A2 = [A2[0][0][0][0][0] for A2 in final_df['A2']]
      angry_allSafe_A2 = [A2[1][0][0][0][0] for A2 in final_df['A2']]
      #safety_other_high = [A2[2][0] for A2 in final_df['A2']]
      axes[12, 1].plot(timesteps, safety_self_high, label='P(trustworthy|self=safe,world=safe,others=safe)', color='grey')
      axes[12, 1].plot(timesteps, angry_allSafe_A2, label='P(calm|self=safe,world=safe,others=safe)', color='orange')
      #axes[11, 1].plot(timesteps, safety_other_high, label='Safe (Others)', color='purple')
      axes[12, 1].legend()



    if hierarchical == False:
      # Set x-label for bottom subplots
      axes[10, 0].set_xlabel('Timestep')
      axes[10, 1].set_xlabel('Timestep')
    else:
      # Set x-label for bottom subplots
      axes[12, 0].set_xlabel('Timestep')
      axes[12, 1].set_xlabel('Timestep')

    return fig, axes

# Process simulation 1 results as data (DataFrame objects) and plot

print(f"Simulation 1 (results_1):")
final_df_1 = process_simulation_results(results_1)
display(final_df_1.head(10))   # print first 10 rows

# # To access elements from a specific row
# q_pi_values = final_df_1.loc[0, 'q_pi']  # This will be a list
# qs_hist_values = final_df_1.loc[0, 'qs_hist']  # This will be a list of lists
# print(q_pi_values)
# print(qs_hist_values)

print("Simulation 1 Plot")
fig, axes = plot_agent_data(final_df_1)
plt.tight_layout()
plt.show()

# Process simulation 2 results as data (DataFrame objects) and plot
print(f"Simulation 2 (results_2):")
final_df_2 = process_simulation_results(results_2)
display(final_df_2.head(10))   # print first 10 rows

print("Simulation 2 Plot")
fig, axes = plot_agent_data(final_df_2)
plt.tight_layout()
plt.show()

# Process simulation 2 results as data (DataFrame objects) and plot
print(f"Simulation 3 (results_3):")
final_df_3 = process_simulation_results(results_3)
#print(final_df_3.head(10).to_dict())
#print(final_df_3.D)
with pd.option_context('display.max_rows',None):
  display(final_df_3.tail(5).T)   # print first 10 rows

print("Simulation 3 Plot")
fig, axes = plot_agent_data(final_df_3, hierarchical=True)
plt.tight_layout()
plt.show()

# # View all simulation 1 results

# with pd.option_context('display.max_rows',None,'display.max_columns',None):
#   display(final_df_1)

# # View a subset of columns
# with pd.option_context('display.max_rows',None,'display.max_columns',None):
#   display(final_df_1[['trial','timestep','D']])

with pd.option_context('display.max_rows',None,'display.max_columns',None):
  display(final_df_3.head(10).T)

print(final_df_3.head(10).to_dict())
print('')

def analyze_card_action_patterns(df):
    """Analyzes card_action patterns across trials with 3 timesteps each."""
    # Group by trial and collect actions in timestep order
    pattern_df = df.groupby('trial')['card_action'].apply(list).reset_index()

    # Calculate pattern frequencies with explicit column naming
    pattern_counts = (
        pattern_df['card_action']
        .value_counts()
        .reset_index(name='count')  # Name the count column explicitly
        .rename(columns={'index': 'pattern'})
        .sort_values('count', ascending=False)
    )

    return pattern_counts


pattern_counts = analyze_card_action_patterns(final_df_3)
print(pattern_counts)

import matplotlib.pyplot as plt
import numpy as np

def plot_agent_data_wide(final_df, hierarchical=False):
    # Define number of subplots
    if not hierarchical:
        n_subplots = 22
        fig, axes = plt.subplots(nrows=n_subplots, ncols=1, figsize=(18, 40), sharex=True)
    else:
        n_subplots = 26
        fig, axes = plt.subplots(nrows=n_subplots, ncols=1, figsize=(18, 48), sharex=True)
    fig.subplots_adjust(hspace=0.7)

    timesteps = range(len(final_df))

    def setup_probability_plot(ax, title):
        ax.set_ylim(0, 1)
        ax.set_xticks(np.arange(0, len(timesteps), 2))
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_ylabel('Probability')

    def setup_action_plot(ax, title, actions, color_map):
        ax.set_xticks(np.arange(0, len(timesteps), 2))
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_yticks([])
        colors = [color_map[action] for action in actions]
        ax.scatter(timesteps, [1]*len(timesteps), c=colors)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',
                                      label=action, markerfacecolor=color, markersize=10)
                           for action, color in color_map.items()]
        ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))

    def setup_obs_plot(ax, title, obss, color_map):
        ax.set_xticks(np.arange(0, len(timesteps), 3))
        ax.grid(True, alpha=0.3)
        ax.set_title(title)
        ax.set_yticks([])
        colors = [color_map[obs] for obs in obss]
        ax.scatter(timesteps, [1]*len(timesteps), c=colors)
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',
                                      label=obs, markerfacecolor=color, markersize=5)
                           for obs, color in color_map.items()]
        ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))

    # 0-4: Prior beliefs (D)
    setup_probability_plot(axes[0], 'Prior Probability of Trust')
    prior_trust = [d[0][0] for d in final_df['D']]
    axes[0].plot(timesteps, prior_trust, label='Prior Trust', color='blue')
    axes[0].legend()

    setup_probability_plot(axes[1], 'Prior Probability of Blue Card')
    prior_blue = [d[1][0] for d in final_df['D']]
    axes[1].plot(timesteps, prior_blue, label='Prior Blue Card', color='blue')
    axes[1].legend()

    setup_probability_plot(axes[2], 'Prior Probability of Angry State')
    prior_angry = [d[2][0] for d in final_df['D']]
    axes[2].plot(timesteps, prior_angry, label='Prior Angry', color='red')
    axes[2].legend()

    setup_probability_plot(axes[3], 'Prior Choice Probabilities')
    prior_blue_choice = [d[3][0] for d in final_df['D']]
    prior_green_choice = [d[3][1] for d in final_df['D']]
    axes[3].plot(timesteps, prior_blue_choice, label='Prior Blue Choice', color='blue')
    axes[3].plot(timesteps, prior_green_choice, label='Prior Green Choice', color='green')
    axes[3].legend()

    setup_probability_plot(axes[4], 'Prior Stage Probabilities')
    prior_null = [d[4][0] for d in final_df['D']]
    prior_advice = [d[4][1] for d in final_df['D']]
    prior_decision = [d[4][2] for d in final_df['D']]
    axes[4].plot(timesteps, prior_null, label='Prior Null', color='grey')
    axes[4].plot(timesteps, prior_advice, label='Prior Advice', color='orange')
    axes[4].plot(timesteps, prior_decision, label='Prior Decision', color='purple')
    axes[4].legend()

    # 5-9: Posterior beliefs (qs_hist)
    setup_probability_plot(axes[5], 'Posterior Probability of Trust')
    trust_probs = [qs[0][0] for qs in final_df['qs_hist']]
    axes[5].plot(timesteps, trust_probs, label='Trust', color='blue')
    axes[5].legend()

    setup_probability_plot(axes[6], 'Posterior Probability of Blue Card')
    blue_probs = [qs[1][0] for qs in final_df['qs_hist']]
    axes[6].plot(timesteps, blue_probs, label='Blue Card', color='blue')
    axes[6].legend()

    setup_probability_plot(axes[7], 'Posterior Probability of Angry State')
    angry_probs = [qs[2][0] for qs in final_df['qs_hist']]
    axes[7].plot(timesteps, angry_probs, label='Angry', color='red')
    axes[7].legend()

    setup_probability_plot(axes[8], 'Posterior Choice Probabilities')
    blue_choice_probs = [qs[3][0] for qs in final_df['qs_hist']]
    green_choice_probs = [qs[3][1] for qs in final_df['qs_hist']]
    null_choice_probs = [qs[3][2] for qs in final_df['qs_hist']]
    axes[8].plot(timesteps, blue_choice_probs, label='Blue Choice', color='blue')
    axes[8].plot(timesteps, green_choice_probs, label='Green Choice', color='green')
    axes[8].plot(timesteps, null_choice_probs, label='Null Choice', color='gray')
    axes[8].legend()

    setup_probability_plot(axes[9], 'Posterior Stage Probabilities')
    null_probs = [qs[4][0] for qs in final_df['qs_hist']]
    advice_probs = [qs[4][1] for qs in final_df['qs_hist']]
    decision_probs = [qs[4][2] for qs in final_df['qs_hist']]
    axes[9].plot(timesteps, null_probs, label='Null', color='grey')
    axes[9].plot(timesteps, advice_probs, label='Advice', color='orange')
    axes[9].plot(timesteps, decision_probs, label='Decision', color='purple')
    axes[9].legend()

    # 10: Trust actions
    setup_action_plot(axes[10], 'Trust Actions', final_df['trust_action'],
                      {'trust': 'blue', 'distrust': 'red'})

    # 11: Card actions
    setup_action_plot(axes[11], 'Card Actions', final_df['card_action'],
                      {'blue': 'blue', 'green': 'green', 'null': 'grey'})

    # 12: True trustworthiness
    setup_action_plot(axes[12], 'True Trustworthiness', final_df['true_trustworthiness'],
                      {'trustworthy': 'blue', 'untrustworthy': 'red'})

    # 13: True color
    setup_action_plot(axes[13], 'True Color', final_df['true_color'],
                      {'blue': 'blue', 'green': 'green'})

    # 14: Expected free energies (efe)
    axes[14].set_xticks(np.arange(0, len(timesteps), 2))
    axes[14].grid(True, alpha=0.3)
    axes[14].set_title('Expected Free Energy')
    axes[14].set_ylabel('EFE')
    efe_colors = ['green', 'blue', 'red', 'orange', 'purple', 'grey']
    efe_labels = ['Trust+Green', 'Trust+Blue', 'Distrust+Blue', 'Distrust+Green', 'trust+null', 'Distrust+null']
    for i in range(6):
        efe_values = [e[i] for e in final_df['efe']]
        axes[14].plot(timesteps, efe_values, label=efe_labels[i], color=efe_colors[i])
    axes[14].legend()

    # 15: Policy probabilities (q_pi)
    setup_probability_plot(axes[15], 'Policy Probabilities')
    for i in range(6):
        policy_probs = [q[i] for q in final_df['q_pi']]
        axes[15].plot(timesteps, policy_probs, label=efe_labels[i], color=efe_colors[i])
    axes[15].legend()

    # 16: Advice likelihood probabilities (A[0])
    setup_probability_plot(axes[16], 'Advice Likelihood')
    # Initialize arrays for aggregation
    A_TrueAdvice_Trust = np.zeros(len(final_df))
    A_FalseAdvice_Distrust = np.zeros(len(final_df))
    A_FalseAdvice_Trust = np.zeros(len(final_df))
    A_TrueAdvice_Distrust = np.zeros(len(final_df))
    for affect in range(len(affect_states)):
        for choice in range(len(choice_states)):
            # Each term is summed independently
            A_TrueAdvice_Trust += np.array([a[0][0][0][0][affect][choice][1] for a in final_df['A']])
            A_TrueAdvice_Trust += np.array([a[0][1][0][1][affect][choice][1] for a in final_df['A']])
            A_FalseAdvice_Distrust += np.array([a[0][0][1][1][affect][choice][1] for a in final_df['A']])
            A_FalseAdvice_Distrust += np.array([a[0][1][1][0][affect][choice][1] for a in final_df['A']])
            A_FalseAdvice_Trust += np.array([a[0][0][0][1][affect][choice][1] for a in final_df['A']])
            A_FalseAdvice_Trust += np.array([a[0][1][0][0][affect][choice][1] for a in final_df['A']])
            A_TrueAdvice_Distrust += np.array([a[0][1][1][1][affect][choice][1] for a in final_df['A']])
            A_TrueAdvice_Distrust += np.array([a[0][0][1][0][affect][choice][1] for a in final_df['A']])
    divider = (len(affect_states) * len(choice_states) * 2)
    A_TrueAdvice_Trust /= divider
    A_FalseAdvice_Distrust /= divider
    A_FalseAdvice_Trust /= divider
    A_TrueAdvice_Distrust /= divider
    axes[16].plot(timesteps, A_TrueAdvice_Trust, label='A_TrueAdvice_Trust', color=efe_colors[0])
    axes[16].plot(timesteps, A_FalseAdvice_Distrust, label='A_FalseAdvice_Distrust', color=efe_colors[1])
    axes[16].plot(timesteps, A_FalseAdvice_Trust, label='A_FalseAdvice_Trust', color=efe_colors[2])
    axes[16].plot(timesteps, A_TrueAdvice_Distrust, label='A_TrueAdvice_Distrust', color=efe_colors[3])
    axes[16].legend()

    # 17: Feedback likelihood probabilities (A[1])
    setup_probability_plot(axes[17], 'Feedback Likelihood')
    A_Correct_Correct = np.zeros(len(final_df))
    A_Incorrect_Incorrect = np.zeros(len(final_df))
    A_Incorrect_Correct = np.zeros(len(final_df))
    A_Correct_Incorrect = np.zeros(len(final_df))
    for trust in range(len(trustworthiness_states)):
        for affect in range(len(affect_states)):
            A_Correct_Correct += np.array([a[1][0][trust][0][affect][0][2] for a in final_df['A']])
            A_Correct_Correct += np.array([a[1][0][trust][1][affect][1][2] for a in final_df['A']])
            A_Incorrect_Incorrect += np.array([a[1][1][trust][0][affect][1][2] for a in final_df['A']])
            A_Incorrect_Incorrect += np.array([a[1][1][trust][1][affect][0][2] for a in final_df['A']])
            A_Incorrect_Correct += np.array([a[1][1][trust][0][affect][0][2] for a in final_df['A']])
            A_Incorrect_Correct += np.array([a[1][1][trust][1][affect][1][2] for a in final_df['A']])
            A_Correct_Incorrect += np.array([a[1][0][trust][0][affect][1][2] for a in final_df['A']])
            A_Correct_Incorrect += np.array([a[1][0][trust][1][affect][0][2] for a in final_df['A']])
    divider = (len(affect_states) * len(trustworthiness_states) * 2)
    A_Correct_Correct /= divider
    A_Incorrect_Incorrect /= divider
    A_Incorrect_Correct /= divider
    A_Correct_Incorrect /= divider
    axes[17].plot(timesteps, A_Correct_Correct, label='A_Correct_Correct', color=efe_colors[0])
    axes[17].plot(timesteps, A_Incorrect_Incorrect, label='A_Incorrect_Incorrect', color=efe_colors[1])
    axes[17].plot(timesteps, A_Incorrect_Correct, label='A_Incorrect_Correct', color=efe_colors[2])
    axes[17].plot(timesteps, A_Correct_Incorrect, label='A_Correct_Incorrect', color=efe_colors[3])
    axes[17].legend()

    # 18: Observed advice
    obs_advice = [row[0] for row in final_df['obs_label']]
    setup_obs_plot(axes[18], 'Observed advice', obs_advice,
                   {'blue': 'blue', 'green': 'green', 'null': 'grey'})

    # 19: Observed feedback
    obs_feedback = [row[1] for row in final_df['obs_label']]
    setup_obs_plot(axes[19], 'Observed feedback', obs_feedback,
                   {'correct': 'black', 'incorrect': 'orange', 'null': 'grey'})

    # 20: Inferred Correct Card
    state_cc = [row[1] for row in final_df['qs_hist']]
    state_cc_label = []
    for i in range(len(state_cc)):
        idx = state_cc[i].index(max(state_cc[i]))
        state_cc_label.append(correct_card_states[idx])
    setup_obs_plot(axes[20], 'Inferred Correct Card', state_cc_label,
                   {'blue': 'blue', 'green': 'green'})

    # 21: Stage transition probabilities (B[0])
    setup_probability_plot(axes[21], 'Stage transition')
    stage_labels = ['null<-null','null<-advice','null<-decision','advice<-null','advice<-advice','advice<-decision','decision<-null','decision<-advice','decision<-decision']
    stage_colors = ['grey','orange','purple','blue','green','black','red','yellow','pink']
    i = 0
    for next_stage in range(len(stage_states)):
        for current_stage in range(len(stage_states)):
            stage_trans = [b[4][next_stage][current_stage][0] for b in final_df['B']]
            axes[21].plot(timesteps, stage_trans, label=stage_labels[i], color=stage_colors[i])
            i += 1
    axes[21].legend()

    # Hierarchical plots
    if hierarchical:
        # 22: Hierarchical Prior Safety Beliefs
        setup_probability_plot(axes[22], 'Hierarchical Prior Safety Beliefs P(D)')
        safety_self_high = [d[0][0] for d in final_df['D2']]
        safety_world_high = [d[1][0] for d in final_df['D2']]
        safety_other_high = [d[2][0] for d in final_df['D2']]
        axes[22].plot(timesteps, safety_self_high, label='Safe (Self)', color='grey')
        axes[22].plot(timesteps, safety_world_high, label='Safe (World)', color='orange')
        axes[22].plot(timesteps, safety_other_high, label='Safe (Others)', color='purple')
        axes[22].legend()

        # 23: Hierarchical Posterior Observations Q(o)
        setup_probability_plot(axes[23], 'Hierarchical Posterior Observations Q(o)')
        trustworthiness_qo = [qo[0][0] for qo in final_df['qo_pi_high']]
        affect_qo = [qo[2][0] for qo in final_df['qo_pi_high']]
        correct_qo = [qo[1][0] for qo in final_df['qo_pi_high']]
        axes[23].plot(timesteps, trustworthiness_qo, label='Trustworthy Expectation', color='grey')
        axes[23].plot(timesteps, affect_qo, label='Angry Expectation', color='orange')
        axes[23].plot(timesteps, correct_qo, label='Blue Correct Expectation', color='purple')
        axes[23].legend()

        # 24: Hierarchical Posterior Safety Beliefs Q(s)
        setup_probability_plot(axes[24], 'Hierarchical Posterior Safety Beliefs Q(s)')
        safety_self_high = [qs[0][0] for qs in final_df['qs_high_hist']]
        safety_world_high = [qs[1][0] for qs in final_df['qs_high_hist']]
        safety_other_high = [qs[2][0] for qs in final_df['qs_high_hist']]
        axes[24].plot(timesteps, safety_self_high, label='Safe (Self)', color='grey')
        axes[24].plot(timesteps, safety_world_high, label='Safe (World)', color='orange')
        axes[24].plot(timesteps, safety_other_high, label='Safe (Others)', color='purple')
        axes[24].legend()

        # 25: Hierarchical Likelihood Q(A)
        setup_probability_plot(axes[25], 'Hierarchical Likelihood Q(A)')
        trust_allSafe_A2 = [A2[0][0][0][0][0] for A2 in final_df['A2']]
        angry_allSafe_A2 = [A2[1][0][0][0][0] for A2 in final_df['A2']]
        axes[25].plot(timesteps, trust_allSafe_A2, label='P(trustworthy|self=safe,world=safe,others=safe)', color='grey')
        axes[25].plot(timesteps, angry_allSafe_A2, label='P(calm|self=safe,world=safe,others=safe)', color='orange')
        axes[25].legend()

    # Set x-label for bottom subplot
    axes[-1].set_xlabel('Timestep')

    return fig, axes

# Process simulation 2 results as data (DataFrame objects) and plot
print(f"Simulation 3 (results_3):")
final_df_3 = process_simulation_results(results_3)
#print(final_df_3.head(10).to_dict())
#print(final_df_3.D)
with pd.option_context('display.max_rows',None):
  display(final_df_3.tail(5).T)   # print first 10 rows

print("Simulation 3 Plot")
fig, axes = plot_agent_data_wide(final_df_3, hierarchical=True)
plt.tight_layout()
plt.show()

print(final_df_3.columns)
print("qo_pi_high")
print(final_df_3.loc[0,'qo_pi_high'])
print(final_df_3.loc[len(final_df_3)-1,'qo_pi_high'])
print("A2")
print(final_df_3.loc[0,'A2'])
print(final_df_3.loc[len(final_df_3)-1,'A2'])
print("B2")
print(final_df_3.loc[0,'B2'])
print(final_df_3.loc[len(final_df_3)-1,'B2'])

with pd.option_context('display.max_rows',None):
  display(final_df_3[['timestep','A2','B2','qo_pi_high']])

def plot_agent_data(final_df, hierarchical=False):
    import matplotlib.pyplot as plt
    from matplotlib.lines import Line2D
    import numpy as np

    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))
    plt.subplots_adjust(hspace=0.4, wspace=0.3)
    timesteps = range(len(final_df))

    def setup_probability_plot(ax, title):
        ax.set_ylim(-0.1, 1.1)
        ax.set_yticks(np.linspace(0, 1, 11))
        ax.set_ylabel('Probability')
        ax.grid(True, alpha=0.3)
        ax.set_title(title)

    # --- TOP LEFT ---
    ax1 = axes[0, 0]
    setup_probability_plot(ax1, 'Card Probability & Advice Observations')
    blue_probs = [qs[1][0] for qs in final_df['qs_hist']]
    ax1.plot(timesteps, blue_probs, label='Posterior Blue Probability', color='blue')
    obs_advice = [row[0] for row in final_df['obs_label']]
    advice_y = [1.0 if advice=='blue' else 0.0 if advice=='green' else 0.5 for advice in obs_advice]
    advice_colors = ['blue' if advice=='blue' else 'green' if advice=='green' else 'grey' for advice in obs_advice]
    ax1.scatter(timesteps, advice_y, c=advice_colors, s=30, alpha=0.7, edgecolor='black')
    legend_elements1 = [
        Line2D([0], [0], marker='o', color='w', label='Blue Advice', markerfacecolor='blue', markersize=8),
        Line2D([0], [0], marker='o', color='w', label='Green Advice', markerfacecolor='green', markersize=8),
        Line2D([0], [0], color='blue', label='Posterior Blue Probability')
    ]
    ax1.legend(handles=legend_elements1)  # Back inside

    # --- TOP RIGHT ---
    ax2 = axes[0, 1]
    setup_probability_plot(ax2, 'Anger State Probabilities with Arousal Markers')
    prior_angry = [d[2][0] for d in final_df['D']]
    angry_probs = [qs[2][0] for qs in final_df['qs_hist']]
    ax2.plot(timesteps, prior_angry, label='Prior Anger', color='black', linestyle='--', linewidth=2.5)
    ax2.plot(timesteps, angry_probs, label='Posterior Anger', color='#cc0000', linewidth=2.5, alpha=0.5)
    arousal_states = [row[2] for row in final_df['obs_label']]
    arousal_y = [1.0 if arousal=='high' else 0.0 for arousal in arousal_states]
    arousal_colors = ['#ff7f0e' if arousal=='high' else '#2ca02c' for arousal in arousal_states]
    ax2.scatter(timesteps, arousal_y, c=arousal_colors, marker='o', s=30, alpha=0.7, edgecolor='black')
    legend_elements2 = [
        Line2D([0], [0], color='black', linestyle='--', label='Prior Anger'),
        Line2D([0], [0], color='#cc0000', label='Posterior Anger', alpha=0.5),
        Line2D([0], [0], marker='o', color='w', label='High Arousal', markerfacecolor='#ff7f0e', markersize=8),
        Line2D([0], [0], marker='o', color='w', label='Low Arousal', markerfacecolor='#2ca02c', markersize=8)
    ]
    ax2.legend(handles=legend_elements2)  # Back inside

    # --- BOTTOM LEFT ---
    ax3 = axes[1, 0]
    setup_probability_plot(ax3, 'Policy Selection Probabilities')
    policy_labels = ['Trust + Green', 'Trust + Blue', 'Distrust + Blue', 'Distrust + Green', 'Trust + Withdraw', 'Distrust + Withdraw']
    colors = ['green', 'blue', 'red', 'orange', 'purple', 'grey']
    for i in range(6):
        policy_probs = [q[i] for q in final_df['q_pi']]
        ax3.plot(timesteps, policy_probs, label=policy_labels[i], color=colors[i], linewidth=2.5)
    ax3.legend()  # Back inside, no trust/distrust dots or legend

    # --- BOTTOM RIGHT ---
    ax4 = axes[1, 1]
    if hierarchical:
        setup_probability_plot(ax4, 'Hierarchical Posterior Observation Beliefs Q(o), Real-Time Trust Beliefs, True Trustworthiness')
        trustworthiness_qo = [qo[0][0] for qo in final_df['qo_pi_high']]
        affect_qo = [qo[2][0] for qo in final_df['qo_pi_high']]
        correct_qo = [qo[1][0] for qo in final_df['qo_pi_high']]
        ax4.plot(timesteps, trustworthiness_qo, label='Trust Expectation', color='grey', linewidth=2.5)
        ax4.plot(timesteps, affect_qo, label='Calm Expectation', color='orange', linewidth=2.5)
        ax4.plot(timesteps, correct_qo, label='Blue Correct Expectation', color='purple', linewidth=2.5)
        prior_trust = [d[0][0] for d in final_df['D']]
        posterior_trust = [qs[0][0] for qs in final_df['qs_hist']]
        ax4.plot(timesteps, prior_trust, label='Prior Probability of Trust', color='blue', linestyle='--', linewidth=2)
        ax4.plot(timesteps, posterior_trust, label='Posterior Probability of Trust', color='blue', linewidth=2)
        true_trust = final_df['true_trustworthiness']
        true_y = [1.0 if t == 'trustworthy' else 0.0 for t in true_trust]
        true_colors = ['blue' if t == 'trustworthy' else 'red' for t in true_trust]
        ax4.scatter(timesteps, true_y, c=true_colors, marker='o', s=30, alpha=0.7, edgecolor='black', label='True Trustworthiness')
        legend_elements4 = [
            Line2D([0], [0], color='grey', label='Trust Expectation'),
            Line2D([0], [0], color='orange', label='Calm Expectation'),
            Line2D([0], [0], color='purple', label='Blue Correct Expectation'),
            Line2D([0], [0], color='blue', linestyle='--', label='Prior Probability of Trust'),
            Line2D([0], [0], color='blue', label='Posterior Probability of Trust'),
            Line2D([0], [0], marker='o', color='w', label='True Trustworthiness (Trustworthy)', markerfacecolor='blue', markersize=8),
            Line2D([0], [0], marker='o', color='w', label='True Trustworthiness (Untrustworthy)', markerfacecolor='red', markersize=8)
        ]
        ax4.legend(handles=legend_elements4, loc='lower right', bbox_to_anchor=(1.35, -0.05))  # Remains outside
    else:
        setup_probability_plot(ax4, 'Hierarchical Observations (Disabled)')
        ax4.text(0.5, 0.5, 'Enable with hierarchical=True', ha='center', va='center', fontsize=12, transform=ax4.transAxes)

    # Remove x-axis numbers from all plots but keep ticks and label
    for ax in axes.flatten():
        ax.set_xlabel('Timestep', fontsize=10)
        ax.set_xticks(np.arange(0, len(timesteps), 2))
        ax.set_xticklabels([])

    return fig, axes

plot_agent_data(final_df_3, hierarchical=True)

