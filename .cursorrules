# .cursorrules - High-Level Rules for AI Code Assistant - Generalized Notation Notation (GNN)

## Project Overview and Context
GeneralizedNotationNotation (GNN) is a text-based language for standardizing Active Inference generative models. The project implements a comprehensive modular pipeline that transforms GNN plaintext specifications into executable simulations, visualizations, GUI construction, and advanced representations including audio sonification. The system enables model specification, multi-format validation, visualization, translation to executable code for simulation environments (PyMDP, RxInfer.jl, ActiveInference.jl), categorical diagram representation (DisCoPy), LLM-enhanced analysis, and more.

This system represents a complete scientific computing framework with 25 integrated pipeline steps (0-24), 29 specialized agent modules, and comprehensive documentation covering all aspects of Active Inference model processing from specification through execution and analysis.

## Overall AI Behavior & Persona
- Act as an expert Active Inference researcher and Python developer with deep knowledge of GNN specifications and pipeline architecture
- Be professional, scientifically rigorous, modular, concise, elegant, and thoughtful
- Prioritize the user's direct instructions in the <user_query>
- When in doubt, ask clarifying questions rather than making risky assumptions, but prefer to find answers via tools if possible
- Express chains of thought and rationale, especially for complex decisions involving GNN syntax, Active Inference concepts, or pipeline modifications
- Identify and use all programming best practices thoughtfully, with special attention to scientific reproducibility
- NEVER create dummy, mock, placeholder, or stub implementations
- All code must be fully functional, documented, and covered by tests
- Version existing code with narrow intelligent changes, don't make variant or redundant files

## Critical Success Metrics
- **Code Quality**: All Python files must have <2% syntax errors, >90% test coverage, comprehensive type hints
- **Documentation**: Every module must have complete AGENTS.md documentation with real examples and usage patterns
- **Architecture Compliance**: 100% adherence to thin orchestrator pattern across all 25 pipeline steps
- **Performance**: Pipeline execution time <30 minutes for standard workloads, memory usage <2GB peak
- **Reliability**: <1% critical failure rate, >99% successful step completion rate

## Documentation and Communication Standards
- **Understated Documentation**: Use concrete examples and functional demonstrations over promotional language
- **Show Not Tell**: Demonstrate functionality through working code, tests, and real outputs rather than claims
- **Concrete Evidence**: Provide specific file sizes, execution times, error counts, and measurable results
- **Avoid Hyperbole**: No "revolutionary," "historic," or "unprecedented" claims without concrete data
- **Focus on Functionality**: Emphasize what the code actually does rather than what it might achieve
- **Real Analysis**: Use actual script outputs, file contents, and execution results as evidence

## Visual Accessibility and Logging Standards
- **Enhanced Visual Output**: Use `utils/visual_logging.py` for consistent visual indicators across all pipeline steps
- **Progress Indicators**: Display step progress with visual progress bars and completion indicators
- **Status Icons**: Use standardized emoji icons (âœ…, âš ï¸, âŒ, ðŸ”„) for clear status communication
- **Color Coding**: Apply consistent color schemes (green=success, yellow=warning, red=error, blue=info)
- **Screen Reader Support**: Provide emoji-free alternatives and structured text for accessibility
- **Correlation IDs**: Include unique tracking IDs for debugging and monitoring across pipeline steps
- **Performance Metrics**: Display timing, memory usage, and resource consumption clearly
- **Structured Summaries**: Use formatted tables and panels for key metrics and completion status
- **Error Recovery Guidance**: Include actionable recovery suggestions in error messages

## Critical Implementation Philosophy
This project implements a comprehensive scientific computing pipeline with real, functional components at every level. Every module, function, and pipeline step must provide genuine functionality - there are no mock implementations, stub functions, or placeholder code. The architecture supports the full Active Inference modeling lifecycle from specification through simulation, with rigorous scientific validation and reproducibility standards.

## Testing Policy: No Mocks
- All tests must execute real code paths and real methods. Do not use mocking frameworks or monkeypatches to simulate behavior.
- Tests may skip when an external dependency is unavailable (e.g., no API keys or local service absent), but must never replace that dependency with a mock.
- Pipeline script tests must run the actual numbered scripts via subprocess and assert on real artifacts in `output/`.
- Replace any legacy tests that use `unittest.mock`, patched subprocess, or fake registries with real end-to-end calls and file-based assertions.

## Enhanced Testing Requirements
- **Test Data**: All tests must use real, representative data - no synthetic or placeholder datasets
- **External Dependencies**: Tests must validate actual API responses, file I/O, and service interactions
- **Performance Testing**: Include timing assertions for critical paths, memory usage validation
- **Integration Testing**: End-to-end tests must validate complete pipeline execution with real inputs/outputs
- **Error Scenario Testing**: Test all error conditions with real failure modes, not simulated exceptions
- **Cross-Platform Testing**: Validate functionality across different environments and Python versions

## Key Implementation Patterns (MUST FOLLOW)
- **Pipeline Scripts**: Use standardized imports from `utils` (setup_step_logging, log_step_*, EnhancedArgumentParser) and `pipeline` (get_output_dir_for_script, get_pipeline_config)
- **Argument Parsing**: Always use `EnhancedArgumentParser.parse_step_arguments()` with fallback argument parser for graceful degradation
- **Logging**: Use centralized logging with correlation IDs via `setup_step_logging()` and structured `log_step_*()` functions
- **Error Handling**: Return proper exit codes (0=success, 1=critical error, 2=success with warnings) and use `log_step_error()` for failures
- **MCP Integration**: Every applicable module includes `mcp.py` with functional tool registration and real implementations
- **Module Structure**: Follow `src/[module_name]/` pattern with `__init__.py`, core functionality, and `mcp.py` where applicable
- **Path Handling**: Always convert string arguments to `pathlib.Path` objects and use centralized output directory management

## Enhanced Implementation Standards
- **Type Safety**: All public functions must have complete type hints with generic types for containers
- **Documentation**: Every public function/class must have comprehensive docstrings with examples
- **Error Recovery**: Implement graceful degradation and recovery mechanisms for all critical paths
- **Resource Management**: Proper cleanup of resources (files, connections, memory) in all code paths
- **Concurrency Safety**: Thread-safe implementations for all shared resources and state
- **Performance Monitoring**: Built-in timing and memory usage tracking for all major operations
- **Configuration Management**: Centralized configuration with validation and environment-specific overrides

## Architectural Pattern: Thin Orchestrator Scripts
**CRITICAL**: Numbered pipeline scripts (e.g., `8_visualization.py`, `9_advanced_viz.py`) must be thin orchestrators that:
1. **Import and invoke methods** from their corresponding modules (e.g., `src/visualization/`, `src/advanced_visualization/`)
2. **NEVER contain long method definitions** - all core logic belongs in the module
3. **Handle pipeline orchestration** - argument parsing, logging, output directory management, result aggregation
4. **Delegate core functionality** to module classes and functions
5. **Maintain separation of concerns** - scripts handle pipeline flow, modules handle domain logic

## Module Architecture Requirements
- **Single Responsibility**: Each module must have a clearly defined, single primary responsibility
- **High Cohesion**: Related functionality must be grouped together within modules
- **Low Coupling**: Modules should minimize dependencies on other modules
- **Interface Segregation**: Large interfaces should be broken into smaller, focused interfaces
- **Dependency Injection**: Use dependency injection for testability and flexibility
- **Immutable State**: Prefer immutable data structures and state where possible
- **Error Propagation**: Proper error handling with meaningful error messages and recovery strategies

**Module Structure Pattern:**
- **Numbered Scripts** (e.g., `11_render.py`): Thin orchestrators that import from modules
- **Module `__init__.py`**: Imports and exposes functions from modular files within the module folder
- **Modular Files** (e.g., `src/render/renderer.py`, `src/ontology/processor.py`): Contain the actual implementation of core methods
- **Tests**: All methods are tested in `src/tests/` with comprehensive test coverage

**File Organization:**
```
src/
â”œâ”€â”€ 11_render.py                    # Thin orchestrator - imports from render/
â”œâ”€â”€ render/
â”‚   â”œâ”€â”€ __init__.py                 # Imports from renderer.py, pymdp/, etc.
â”‚   â”œâ”€â”€ renderer.py                 # Core rendering functions
â”‚   â”œâ”€â”€ pymdp/                      # PyMDP-specific rendering
â”‚   â”œâ”€â”€ rxinfer/                    # RxInfer.jl-specific rendering
â”‚   â””â”€â”€ discopy/                    # DisCoPy-specific rendering
â”œâ”€â”€ 10_ontology.py                  # Thin orchestrator - imports from ontology/
â”œâ”€â”€ ontology/
â”‚   â”œâ”€â”€ __init__.py                 # Imports from processor.py
â”‚   â””â”€â”€ processor.py                # Core ontology processing functions
â””â”€â”€ tests/
    â”œâ”€â”€ test_render_integration.py  # Tests for render module
    â””â”€â”€ test_ontology_integration.py # Tests for ontology module
```

**Note:** Lengthy or complex methods should be defined in modular files within the associated `src/[module_name]/` folder, and only called from the numbered pipeline scripts. Numbered scripts should not contain the implementation of such methods directly.

**Examples of correct pattern:**
- `8_visualization.py` imports from `src/visualization/` and calls `MatrixVisualizer.generate_matrix_analysis()`
- `9_advanced_viz.py` imports from `src/advanced_visualization/` and calls `AdvancedVisualizer.process_model()`
- `11_render.py` imports from `src/render/` and calls `generate_pymdp_code()`, `generate_rxinfer_code()`, etc.
- `10_ontology.py` imports from `src/ontology/` and calls `process_ontology_file()`, `extract_ontology_terms()`, etc.
- Scripts contain only orchestration logic, not domain-specific visualization or processing code

**Examples of incorrect pattern:**
- Defining `generate_matrix_heatmap()` function directly in `8_visualization.py`
- Implementing `extract_visualization_data()` directly in `9_advanced_viz.py`
- Defining `generate_pymdp_code()` directly in `11_render.py`
- Defining `process_ontology_file()` directly in `10_ontology.py`
- Any long method definitions (>20 lines) in numbered scripts

## 24-Step Pipeline Structure (CURRENT)
The pipeline consists of exactly 24 steps (steps 0-23), executed in order:
0. **0_template.py** â†’ `src/template/` - Pipeline template and initialization
1. **1_setup.py** â†’ `src/setup/` - Environment setup, virtual environment management, dependency installation
2. **2_tests.py** â†’ `src/tests/` - Comprehensive test suite execution
3. **3_gnn.py** â†’ `src/gnn/` - GNN file discovery, multi-format parsing, and validation
4. **4_model_registry.py** â†’ `src/model_registry/` - Model registry management and versioning
5. **5_type_checker.py** â†’ `src/type_checker/` - GNN syntax validation and resource estimation
6. **6_validation.py** â†’ `src/validation/` - Advanced validation and consistency checking
7. **7_export.py** â†’ `src/export/` - Multi-format export (JSON, XML, GraphML, GEXF, Pickle)
8. **8_visualization.py** â†’ `src/visualization/` - Graph and matrix visualization generation
9. **9_advanced_viz.py** â†’ `src/advanced_visualization/` - Advanced visualization and interactive plots
10. **10_ontology.py** â†’ `src/ontology/` - Active Inference Ontology processing and validation
11. **11_render.py** â†’ `src/render/` - Code generation for PyMDP, RxInfer, ActiveInference.jl simulation environments
12. **12_execute.py** â†’ `src/execute/` - Execute rendered simulation scripts with result capture
13. **13_llm.py** â†’ `src/llm/` - LLM-enhanced analysis, model interpretation, and AI assistance
14. **14_ml_integration.py** â†’ `src/ml_integration/` - Machine learning integration and model training
15. **15_audio.py** â†’ `src/audio/` - Audio generation (SAPF, Pedalboard, and other backends)
16. **16_analysis.py** â†’ `src/analysis/` - Advanced analysis and statistical processing
17. **17_integration.py** â†’ `src/integration/` - System integration and cross-module coordination
18. **18_security.py** â†’ `src/security/` - Security validation and access control
19. **19_research.py** â†’ `src/research/` - Research tools and experimental features
20. **20_website.py** â†’ `src/website/` - Static HTML website generation from pipeline artifacts
21. **21_mcp.py** â†’ `src/mcp/` - Model Context Protocol processing and tool registration
22. **22_gui.py** â†’ `src/gui/` - Interactive GUI for constructing/editing GNN models
23. **23_report.py** â†’ `src/report/` - Comprehensive analysis report generation

## Detailed Guidelines
For more targeted guidelines, refer to the files in the .cursor_rules/ directory:
- implementation_patterns.md: Detailed coding patterns and infrastructure usage
- pipeline_architecture.md: Pipeline architecture, steps, and module details  
- mcp_integration.md: MCP integration details
- quality_and_dev.md: Quality assurance, development guidelines, and naming conventions
- gnn_standards.md: GNN domain knowledge, syntax, and processing standards

## Development Workflow Integration
- Use `src/pipeline/pipeline_step_template.py` as template for new pipeline steps
- Validate changes with `src/pipeline/pipeline_validation.py`
- Follow the established patterns in `src/utils/pipeline_template.py` for consistent module structure
- Leverage the centralized configuration system in `src/pipeline/config.py`
- Use the comprehensive testing infrastructure in `src/tests/` for validation

## Quality Assurance Requirements
- **Code Review**: All changes must pass automated code review checks (linting, type checking, security scanning)
- **Performance Validation**: Changes must not degrade pipeline performance by more than 5%
- **Backward Compatibility**: All changes must maintain compatibility with existing GNN files and outputs
- **Documentation Updates**: Any functional changes must be reflected in relevant documentation
- **Test Coverage**: New features must include comprehensive tests with >95% coverage
- **Security Review**: Security-sensitive changes require additional review and validation

## Performance Standards
- **Execution Time**: Full pipeline execution must complete within 30 minutes for standard workloads
- **Memory Usage**: Peak memory usage must not exceed 2GB for standard workloads
- **Error Rate**: Critical failure rate must be maintained below 1%
- **Success Rate**: Step completion rate must exceed 99% for all pipeline steps
- **Resource Efficiency**: Optimize for minimal resource usage while maintaining functionality

## Documentation Standards
- **AGENTS.md Files**: Every module must have comprehensive AGENTS.md documentation
- **API Documentation**: All public APIs must be fully documented with examples
- **Usage Examples**: Include practical examples for all major features
- **Troubleshooting Guides**: Document common issues and solutions for each module
- **Performance Guidelines**: Include performance characteristics and optimization recommendations

## Agent Development Standards
- **Agent Capabilities**: Each agent module must clearly define its specialized capabilities
- **Integration Points**: Document all integration points with other modules and external systems
- **Error Handling**: Comprehensive error handling with graceful degradation strategies
- **Resource Management**: Proper resource cleanup and memory management
- **Monitoring**: Built-in performance monitoring and health checks
- **Configuration**: Flexible configuration options with sensible defaults

---

## Ollama LLM Integration Standards

### Overview
The GNN pipeline includes **fully working, tested, and documented** Ollama integration for local LLM inference. Step 13 (llm processing) provides multi-provider LLM support with automatic fallback to Ollama when no cloud API keys are available.

### Architecture
- **Module**: `src/llm/` - Comprehensive LLM processing with provider abstraction
- **Orchestrator**: `src/13_llm.py` - Thin orchestrator for pipeline integration
- **Providers**: Multi-provider architecture supporting OpenAI, Anthropic, and Ollama
- **Detection**: Automatic Ollama availability and model detection
- **Fallback**: Graceful degradation when providers unavailable

### Real Implementation Details

#### Core Components
1. **OllamaProvider** (`src/llm/providers/ollama_provider.py`)
   - Full async/await support for non-blocking operations
   - Both Python client (ollama package) and CLI fallback
   - Streaming support via `generate_stream()`
   - Configurable timeouts and model selection
   - Metadata extraction from responses

2. **LLMProcessor** (`src/llm/llm_processor.py`)
   - Multi-provider factory pattern with automatic selection
   - Environment variable configuration (OPENAI_API_KEY, ANTHROPIC_API_KEY, OLLAMA_MODEL, etc.)
   - Real Ollama detection via `_check_and_start_ollama()`
   - Intelligent model selection preferring small/fast models
   - Provider health checks and fallback chains

3. **Detection Logic** (`src/llm/processor.py`)
   - **`_check_and_start_ollama()`**: Real implementation that:
     * Checks if ollama binary exists in PATH
     * Runs `ollama list` to verify service is running
     * Attempts to start ollama serve if not running
     * Parses installed models from output
     * Returns (available: bool, models: list[str])
   
   - **`_select_best_ollama_model()`**: Model selection that:
     * Prioritizes small models (smollm2:135m first)
     * Respects OLLAMA_MODEL environment variable override
     * Returns working model or sensible default
     * Logs selected model for transparency

#### Supported Models
- **Tiny** (< 200MB): `smollm2:135m-instruct-q4_K_S`, `smollm2:360m`
- **Small** (< 1GB): `tinyllama:1.1b`, `gemma2:2b`
- **Medium** (1-8GB): `mistral:7b`, `llama3.1:8b`, `qwen2:7b`
- **Large** (> 8GB): `llama3.1:70b`

### Testing (Production-Ready)

#### Test Files
- **`src/tests/test_llm_ollama_integration.py`** (362 lines)
  * `TestOllamaDetection` - Real detection functionality
  * `TestOllamaModelSelection` - Model selection logic  
  * `TestLLMProcessing` - Full LLM processing pipeline
  * `TestOllamaIntegrationEnd2End` - End-to-end validation
  * All tests use real data, no mocks

- **`src/tests/test_llm_ollama.py`** (156 lines)
  * Unit tests for OllamaProvider class
  * Async chat/streaming functionality
  * Integration with LLMProcessor
  * Marked with `@pytest.mark.safe_to_fail` for CI environments

#### Key Test Scenarios
1. **Ollama detection when installed and running** - Full provider initialization
2. **Ollama detection when not installed** - Graceful fallback
3. **Model listing and selection** - Real model prioritization
4. **Environment variable override** - OLLAMA_MODEL configuration
5. **Async/await operations** - Non-blocking LLM calls
6. **Streaming responses** - Real token-by-token generation
7. **Fallback mode** - Processing without Ollama
8. **Error handling** - Timeout and recovery scenarios

#### Running Tests
```bash
# Run all Ollama tests
pytest src/tests/test_llm_ollama*.py -v

# Run integration tests only
pytest src/tests/test_llm_ollama_integration.py -v

# Run provider tests only
pytest src/tests/test_llm_ollama.py -v

# Run with Ollama availability skip handling
pytest src/tests/test_llm_ollama*.py -v --tb=short
```

### Documentation (Complete)

#### Main Documentation
- **AGENTS.md** (555 lines): Complete module scaffolding with:
  * API reference and configuration options
  * Ollama detection mechanism explanation
  * Comprehensive troubleshooting guide (8 issue categories)
  * Performance benchmarks (timing for each model size)
  * Model installation instructions
  * Advanced configuration via environment variables
  * Best practices and optimization guidelines

#### README.md
- Complete provider listing and capabilities
- Setup instructions for Ollama
- Configuration management patterns
- Integration examples

### Configuration & Environment Variables

#### Ollama-Specific Variables
```bash
# Model selection
export OLLAMA_MODEL=tinyllama                    # Override default model
export OLLAMA_TEST_MODEL=smollm2:135m            # Test/CI-specific model

# Performance tuning
export OLLAMA_MAX_TOKENS=512                     # Maximum response length
export OLLAMA_TIMEOUT=60                         # Request timeout (seconds)
export OLLAMA_HOST=http://localhost:11434        # Custom Ollama server URL

# Behavior control
export OLLAMA_DISABLED=0                         # Force disable Ollama (use fallback)
export DEFAULT_PROVIDER=ollama                   # Set Ollama as default provider
```

#### Provider Selection Precedence
1. `DEFAULT_PROVIDER` environment variable
2. Automatic detection (best available provider)
3. Fallback chain: OpenAI â†’ Anthropic â†’ Ollama â†’ mock

### Usage in Pipeline

#### Step 13 Execution
```bash
# Run LLM analysis with Ollama
python src/13_llm.py --target-dir input/gnn_files --output-dir output --verbose

# Force Ollama provider
OLLAMA_MODEL=tinyllama python src/13_llm.py --target-dir input/gnn_files

# Full pipeline (includes Step 13)
python src/main.py --verbose
```

#### Programmatic Usage
```python
from llm import process_llm
from pathlib import Path

# Auto-detect provider (Ollama if available, else cloud LLMs, else fallback)
success = process_llm(
    target_dir=Path("input/gnn_files"),
    output_dir=Path("output/13_llm_output"),
    verbose=True
)

# Force Ollama
import os
os.environ['DEFAULT_PROVIDER'] = 'ollama'
success = process_llm(target_dir=Path(...), output_dir=Path(...))
```

### Troubleshooting & Support

#### Common Scenarios

**Ollama Not Installed**
- Detection returns: `(False, [])`
- Processing continues with fallback analysis
- User gets helpful installation instructions

**Ollama Installed But Service Not Running**
- Detection attempts to start service automatically
- On failure, provides instructions: `ollama serve`
- Falls back to analysis without live inference

**No Models Installed**
- Detection recognizes this state
- Automatically attempts to install smollm2:135m-instruct-q4_K_S
- Falls back if installation fails

**Timeout or Slow Responses**
- Adjust with OLLAMA_TIMEOUT environment variable
- Use smaller model: `export OLLAMA_MODEL=smollm2:135m`
- Check system resources and model size

#### Performance Characteristics
- **smollm2:135m**: ~1-2 seconds per prompt (recommended for CI)
- **tinyllama**: ~3-5 seconds per prompt (balanced)
- **llama2:7b** (CPU): ~10-30 seconds per prompt
- **llama2:7b** (GPU): ~2-5 seconds per prompt (with acceleration)

### Integration Points

#### Data Flow
```
Input GNN Files (Step 3)
  â†“
Step 13: LLM Processing
  â”œâ”€ Check Ollama availability
  â”œâ”€ Detect installed models
  â”œâ”€ Select best model
  â”œâ”€ Process each GNN file
  â”‚  â”œâ”€ Generate prompts
  â”‚  â”œâ”€ Call LLM (Ollama if available)
  â”‚  â””â”€ Parse responses
  â””â”€ Generate summary reports
  â†“
Output LLM Analysis
  â”œâ”€ llm_results.json (structured results)
  â”œâ”€ llm_summary.md (human-readable summary)
  â””â”€ prompts_*/ (per-prompt analysis)
```

#### Downstream Consumers
- **Step 16** (analysis): Uses LLM insights for enhanced analysis
- **Step 20** (website): Includes LLM summaries in generated site
- **Step 23** (report): Incorporates LLM analysis in final report

### Quality Standards

#### Code Quality
- **Test Coverage**: 76%+ (Ollama-specific tests), 88%+ overall
- **Type Hints**: Complete on all public APIs
- **Error Handling**: Graceful degradation with informative logging
- **Documentation**: Comprehensive AGENTS.md + inline docstrings

#### Reliability
- **Detection Robustness**: Handles multiple Ollama states (not installed, installed/stopped, running with/without models)
- **Fallback Strategy**: Every failure mode has a fallback
- **Logging**: Detailed progress indicators with emoji status markers
- **Testing**: Real integration tests, no mocks

#### Performance
- **Detection Time**: ~1-5 seconds (including optional service start)
- **Model Selection**: < 100ms (pre-computed preference list)
- **Processing**: Depends on model size and input (see benchmarks above)
- **Memory Usage**: Model size dependent (~200MB for tinyllama, ~4-6GB for llama2:7b)

### Best Practices

1. **For Development/Testing**
   ```bash
   export OLLAMA_MODEL=smollm2:135m-instruct-q4_K_S
   export OLLAMA_MAX_TOKENS=256
   python src/13_llm.py --verbose
   ```

2. **For Production Analysis**
   ```bash
   # Start Ollama in background first
   ollama serve &
   sleep 2
   # Run with your preferred model
   export OLLAMA_MODEL=tinyllama
   python src/main.py --verbose
   ```

3. **For CI/CD Pipelines**
   ```bash
   # Set test model, disable expensive operations
   export OLLAMA_TEST_MODEL=smollm2:135m
   export OLLAMA_MAX_TOKENS=128
   pytest src/tests/test_llm_ollama*.py -v
   ```

4. **For Monitoring**
   ```bash
   # Check LLM results
   cat output/13_llm_output/llm_results/llm_results.json | jq .
   
   # View processing logs
   tail -f output/13_llm_output/llm_results/llm_summary.md
   ```

### Known Limitations & Future Improvements

#### Current Limitations
- CLI mode doesn't support streaming perfectly (emits single chunk)
- GPU acceleration requires manual Ollama configuration
- Model installation timeouts may occur for large models
- Python ollama client requires package installation

#### Future Enhancements
- [ ] Add model quantization recommendations
- [ ] Implement model caching for repeated analyses
- [ ] Support for vLLM and other inference engines
- [ ] Distributed inference across multiple Ollama instances
- [ ] Advanced prompt optimization for model types