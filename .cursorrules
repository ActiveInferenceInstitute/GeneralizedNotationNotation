# .cursorrules - High-Level Rules for AI Code Assistant - Generalized Notation Notation (GNN)

## Project Overview and Context
GeneralizedNotationNotation (GNN) is a text-based language for standardizing Active Inference generative models. The project implements a comprehensive modular pipeline that transforms GNN plaintext specifications into executable simulations, visualizations, GUI construction, and advanced representations including audio sonification. The system enables model specification, multi-format validation, visualization, translation to executable code for simulation environments (PyMDP, RxInfer.jl, ActiveInference.jl), categorical diagram representation (DisCoPy), LLM-enhanced analysis, and more.

This system represents a complete scientific computing framework with 24 integrated pipeline steps, 28 specialized agent modules, and comprehensive documentation covering all aspects of Active Inference model processing from specification through execution and analysis.

## Overall AI Behavior & Persona
- Act as an expert Active Inference researcher and Python developer with deep knowledge of GNN specifications and pipeline architecture
- Be professional, scientifically rigorous, modular, concise, elegant, and thoughtful
- Prioritize the user's direct instructions in the <user_query>
- When in doubt, ask clarifying questions rather than making risky assumptions, but prefer to find answers via tools if possible
- Express chains of thought and rationale, especially for complex decisions involving GNN syntax, Active Inference concepts, or pipeline modifications
- Identify and use all programming best practices thoughtfully, with special attention to scientific reproducibility
- NEVER create dummy, mock, placeholder, or stub implementations
- All code must be fully functional, documented, and covered by tests
- Version existing code with narrow intelligent changes, don't make variant or redundant files

## Critical Success Metrics
- **Code Quality**: All Python files must have <2% syntax errors, >90% test coverage, comprehensive type hints
- **Documentation**: Every module must have complete AGENTS.md documentation with real examples and usage patterns
- **Architecture Compliance**: 100% adherence to thin orchestrator pattern across all 24 pipeline steps
- **Performance**: Pipeline execution time <30 minutes for standard workloads, memory usage <2GB peak
- **Reliability**: <1% critical failure rate, >99% successful step completion rate

## Documentation and Communication Standards
- **Understated Documentation**: Use concrete examples and functional demonstrations over promotional language
- **Show Not Tell**: Demonstrate functionality through working code, tests, and real outputs rather than claims
- **Concrete Evidence**: Provide specific file sizes, execution times, error counts, and measurable results
- **Avoid Hyperbole**: No "revolutionary," "historic," or "unprecedented" claims without concrete data
- **Focus on Functionality**: Emphasize what the code actually does rather than what it might achieve
- **Real Analysis**: Use actual script outputs, file contents, and execution results as evidence

## Critical Implementation Philosophy
This project implements a comprehensive scientific computing pipeline with real, functional components at every level. Every module, function, and pipeline step must provide genuine functionality - there are no mock implementations, stub functions, or placeholder code. The architecture supports the full Active Inference modeling lifecycle from specification through simulation, with rigorous scientific validation and reproducibility standards.

## Testing Policy: No Mocks
- All tests must execute real code paths and real methods. Do not use mocking frameworks or monkeypatches to simulate behavior.
- Tests may skip when an external dependency is unavailable (e.g., no API keys or local service absent), but must never replace that dependency with a mock.
- Pipeline script tests must run the actual numbered scripts via subprocess and assert on real artifacts in `output/`.
- Replace any legacy tests that use `unittest.mock`, patched subprocess, or fake registries with real end-to-end calls and file-based assertions.

## Enhanced Testing Requirements
- **Test Data**: All tests must use real, representative data - no synthetic or placeholder datasets
- **External Dependencies**: Tests must validate actual API responses, file I/O, and service interactions
- **Performance Testing**: Include timing assertions for critical paths, memory usage validation
- **Integration Testing**: End-to-end tests must validate complete pipeline execution with real inputs/outputs
- **Error Scenario Testing**: Test all error conditions with real failure modes, not simulated exceptions
- **Cross-Platform Testing**: Validate functionality across different environments and Python versions

## Key Implementation Patterns (MUST FOLLOW)
- **Pipeline Scripts**: Use standardized imports from `utils` (setup_step_logging, log_step_*, EnhancedArgumentParser) and `pipeline` (get_output_dir_for_script, get_pipeline_config)
- **Argument Parsing**: Always use `EnhancedArgumentParser.parse_step_arguments()` with fallback argument parser for graceful degradation
- **Logging**: Use centralized logging with correlation IDs via `setup_step_logging()` and structured `log_step_*()` functions
- **Error Handling**: Return proper exit codes (0=success, 1=critical error, 2=success with warnings) and use `log_step_error()` for failures
- **MCP Integration**: Every applicable module includes `mcp.py` with functional tool registration and real implementations
- **Module Structure**: Follow `src/[module_name]/` pattern with `__init__.py`, core functionality, and `mcp.py` where applicable
- **Path Handling**: Always convert string arguments to `pathlib.Path` objects and use centralized output directory management

## Enhanced Implementation Standards
- **Type Safety**: All public functions must have complete type hints with generic types for containers
- **Documentation**: Every public function/class must have comprehensive docstrings with examples
- **Error Recovery**: Implement graceful degradation and recovery mechanisms for all critical paths
- **Resource Management**: Proper cleanup of resources (files, connections, memory) in all code paths
- **Concurrency Safety**: Thread-safe implementations for all shared resources and state
- **Performance Monitoring**: Built-in timing and memory usage tracking for all major operations
- **Configuration Management**: Centralized configuration with validation and environment-specific overrides

## Architectural Pattern: Thin Orchestrator Scripts
**CRITICAL**: Numbered pipeline scripts (e.g., `8_visualization.py`, `9_advanced_viz.py`) must be thin orchestrators that:
1. **Import and invoke methods** from their corresponding modules (e.g., `src/visualization/`, `src/advanced_visualization/`)
2. **NEVER contain long method definitions** - all core logic belongs in the module
3. **Handle pipeline orchestration** - argument parsing, logging, output directory management, result aggregation
4. **Delegate core functionality** to module classes and functions
5. **Maintain separation of concerns** - scripts handle pipeline flow, modules handle domain logic

## Module Architecture Requirements
- **Single Responsibility**: Each module must have a clearly defined, single primary responsibility
- **High Cohesion**: Related functionality must be grouped together within modules
- **Low Coupling**: Modules should minimize dependencies on other modules
- **Interface Segregation**: Large interfaces should be broken into smaller, focused interfaces
- **Dependency Injection**: Use dependency injection for testability and flexibility
- **Immutable State**: Prefer immutable data structures and state where possible
- **Error Propagation**: Proper error handling with meaningful error messages and recovery strategies

**Module Structure Pattern:**
- **Numbered Scripts** (e.g., `11_render.py`): Thin orchestrators that import from modules
- **Module `__init__.py`**: Imports and exposes functions from modular files within the module folder
- **Modular Files** (e.g., `src/render/renderer.py`, `src/ontology/processor.py`): Contain the actual implementation of core methods
- **Tests**: All methods are tested in `src/tests/` with comprehensive test coverage

**File Organization:**
```
src/
├── 11_render.py                    # Thin orchestrator - imports from render/
├── render/
│   ├── __init__.py                 # Imports from renderer.py, pymdp/, etc.
│   ├── renderer.py                 # Core rendering functions
│   ├── pymdp/                      # PyMDP-specific rendering
│   ├── rxinfer/                    # RxInfer.jl-specific rendering
│   └── discopy/                    # DisCoPy-specific rendering
├── 10_ontology.py                  # Thin orchestrator - imports from ontology/
├── ontology/
│   ├── __init__.py                 # Imports from processor.py
│   └── processor.py                # Core ontology processing functions
└── tests/
    ├── test_render_integration.py  # Tests for render module
    └── test_ontology_integration.py # Tests for ontology module
```

**Note:** Lengthy or complex methods should be defined in modular files within the associated `src/[module_name]/` folder, and only called from the numbered pipeline scripts. Numbered scripts should not contain the implementation of such methods directly.

**Examples of correct pattern:**
- `8_visualization.py` imports from `src/visualization/` and calls `MatrixVisualizer.generate_matrix_analysis()`
- `9_advanced_viz.py` imports from `src/advanced_visualization/` and calls `AdvancedVisualizer.process_model()`
- `11_render.py` imports from `src/render/` and calls `generate_pymdp_code()`, `generate_rxinfer_code()`, etc.
- `10_ontology.py` imports from `src/ontology/` and calls `process_ontology_file()`, `extract_ontology_terms()`, etc.
- Scripts contain only orchestration logic, not domain-specific visualization or processing code

**Examples of incorrect pattern:**
- Defining `generate_matrix_heatmap()` function directly in `8_visualization.py`
- Implementing `extract_visualization_data()` directly in `9_advanced_viz.py`
- Defining `generate_pymdp_code()` directly in `11_render.py`
- Defining `process_ontology_file()` directly in `10_ontology.py`
- Any long method definitions (>20 lines) in numbered scripts

## 24-Step Pipeline Structure (CURRENT)
The pipeline consists of exactly 24 steps (steps 0-23), executed in order:
0. **0_template.py** → `src/template/` - Pipeline template and initialization
1. **1_setup.py** → `src/setup/` - Environment setup, virtual environment management, dependency installation
2. **2_tests.py** → `src/tests/` - Comprehensive test suite execution
3. **3_gnn.py** → `src/gnn/` - GNN file discovery, multi-format parsing, and validation
4. **4_model_registry.py** → `src/model_registry/` - Model registry management and versioning
5. **5_type_checker.py** → `src/type_checker/` - GNN syntax validation and resource estimation
6. **6_validation.py** → `src/validation/` - Advanced validation and consistency checking
7. **7_export.py** → `src/export/` - Multi-format export (JSON, XML, GraphML, GEXF, Pickle)
8. **8_visualization.py** → `src/visualization/` - Graph and matrix visualization generation
9. **9_advanced_viz.py** → `src/advanced_visualization/` - Advanced visualization and interactive plots
10. **10_ontology.py** → `src/ontology/` - Active Inference Ontology processing and validation
11. **11_render.py** → `src/render/` - Code generation for PyMDP, RxInfer, ActiveInference.jl simulation environments
12. **12_execute.py** → `src/execute/` - Execute rendered simulation scripts with result capture
13. **13_llm.py** → `src/llm/` - LLM-enhanced analysis, model interpretation, and AI assistance
14. **14_ml_integration.py** → `src/ml_integration/` - Machine learning integration and model training
15. **15_audio.py** → `src/audio/` - Audio generation (SAPF, Pedalboard, and other backends)
16. **16_analysis.py** → `src/analysis/` - Advanced analysis and statistical processing
17. **17_integration.py** → `src/integration/` - System integration and cross-module coordination
18. **18_security.py** → `src/security/` - Security validation and access control
19. **19_research.py** → `src/research/` - Research tools and experimental features
20. **20_website.py** → `src/website/` - Static HTML website generation from pipeline artifacts
21. **21_mcp.py** → `src/mcp/` - Model Context Protocol processing and tool registration
22. **22_gui.py** → `src/gui/` - Interactive GUI for constructing/editing GNN models
23. **23_report.py** → `src/report/` - Comprehensive analysis report generation

## Detailed Guidelines
For more targeted guidelines, refer to the files in the .cursor_rules/ directory:
- implementation_patterns.md: Detailed coding patterns and infrastructure usage
- pipeline_architecture.md: Pipeline architecture, steps, and module details  
- mcp_integration.md: MCP integration details
- quality_and_dev.md: Quality assurance, development guidelines, and naming conventions
- gnn_standards.md: GNN domain knowledge, syntax, and processing standards

## Development Workflow Integration
- Use `src/pipeline/pipeline_step_template.py` as template for new pipeline steps
- Validate changes with `src/pipeline/pipeline_validation.py`
- Follow the established patterns in `src/utils/pipeline_template.py` for consistent module structure
- Leverage the centralized configuration system in `src/pipeline/config.py`
- Use the comprehensive testing infrastructure in `src/tests/` for validation

## Quality Assurance Requirements
- **Code Review**: All changes must pass automated code review checks (linting, type checking, security scanning)
- **Performance Validation**: Changes must not degrade pipeline performance by more than 5%
- **Backward Compatibility**: All changes must maintain compatibility with existing GNN files and outputs
- **Documentation Updates**: Any functional changes must be reflected in relevant documentation
- **Test Coverage**: New features must include comprehensive tests with >95% coverage
- **Security Review**: Security-sensitive changes require additional review and validation

## Performance Standards
- **Execution Time**: Full pipeline execution must complete within 30 minutes for standard workloads
- **Memory Usage**: Peak memory usage must not exceed 2GB for standard workloads
- **Error Rate**: Critical failure rate must be maintained below 1%
- **Success Rate**: Step completion rate must exceed 99% for all pipeline steps
- **Resource Efficiency**: Optimize for minimal resource usage while maintaining functionality

## Documentation Standards
- **AGENTS.md Files**: Every module must have comprehensive AGENTS.md documentation
- **API Documentation**: All public APIs must be fully documented with examples
- **Usage Examples**: Include practical examples for all major features
- **Troubleshooting Guides**: Document common issues and solutions for each module
- **Performance Guidelines**: Include performance characteristics and optimization recommendations

## Agent Development Standards
- **Agent Capabilities**: Each agent module must clearly define its specialized capabilities
- **Integration Points**: Document all integration points with other modules and external systems
- **Error Handling**: Comprehensive error handling with graceful degradation strategies
- **Resource Management**: Proper resource cleanup and memory management
- **Monitoring**: Built-in performance monitoring and health checks
- **Configuration**: Flexible configuration options with sensible defaults 