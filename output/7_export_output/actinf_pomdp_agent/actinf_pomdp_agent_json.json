{
  "model_name": "Active Inference POMDP Agent",
  "version": "1.0",
  "annotation": "This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.",
  "variables": [
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "86e1553d-4188-4998-b977-75de5d5be9ab",
      "name": "A",
      "var_type": "likelihood_matrix",
      "dimensions": [
        3,
        3
      ],
      "data_type": "float",
      "description": "Likelihood mapping hidden states to observations",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "bd4cff9c-5a2d-49ff-803d-47cbaa4161ce",
      "name": "B",
      "var_type": "transition_matrix",
      "dimensions": [
        3,
        3,
        3
      ],
      "data_type": "float",
      "description": "State transitions given previous state and action",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "989da8d6-e683-4150-b9c0-50da46dccba2",
      "name": "C",
      "var_type": "preference_vector",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Log-preferences over observations",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "271983fb-26bc-43c0-825f-35686533eb86",
      "name": "D",
      "var_type": "prior_vector",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Prior over initial hidden states",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "eda23274-61c2-426c-8cd7-6ae50b1ca694",
      "name": "E",
      "var_type": "policy",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Initial policy prior (habit) over actions",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "dcafe8e0-86be-4f63-90a5-e72de7470b6c",
      "name": "s",
      "var_type": "hidden_state",
      "dimensions": [
        3,
        1
      ],
      "data_type": "float",
      "description": "Current hidden state distribution",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "8f9af1b0-56c0-44e2-83f1-4a5a36f37155",
      "name": "s_prime",
      "var_type": "hidden_state",
      "dimensions": [
        3,
        1
      ],
      "data_type": "float",
      "description": "Next hidden state distribution",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "85610b6b-ed5a-4236-915d-d0b07f0cb757",
      "name": "F",
      "var_type": "hidden_state",
      "dimensions": [
        1
      ],
      "data_type": "float",
      "description": "Variational Free Energy for belief updating from observations",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "390d08c6-19f2-4d37-af09-702aa33bd08f",
      "name": "o",
      "var_type": "observation",
      "dimensions": [
        3,
        1
      ],
      "data_type": "integer",
      "description": "Current observation (integer index)",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "eb6a0f54-08fe-46b4-a68b-e9513bf6f752",
      "name": "π",
      "var_type": "policy",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Policy (distribution over actions), no planning",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "e67498ba-d202-4595-a39b-126914b27d66",
      "name": "u",
      "var_type": "action",
      "dimensions": [
        1
      ],
      "data_type": "integer",
      "description": "Action taken",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "91854089-9417-4c42-9fd9-1e4a20202ff8",
      "name": "G",
      "var_type": "policy",
      "dimensions": [
        1
      ],
      "data_type": "float",
      "description": "Expected Free Energy (per policy)",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "fa95bb64-0ff1-4dce-9c4b-40c2e7435013",
      "name": "t",
      "var_type": "hidden_state",
      "dimensions": [
        1
      ],
      "data_type": "integer",
      "description": "Discrete time step",
      "constraints": {}
    }
  ],
  "connections": [
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "75fe482f-91fc-4995-8eb6-9524ca935f06",
      "source_variables": [
        "D"
      ],
      "target_variables": [
        "s"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "89b23be7-cd43-4566-87c0-5181afef822d",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "A"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "51b257fe-8c0d-42e4-bdcb-2e1d348ac7b1",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "s_prime"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "c83bf925-4d4b-40ca-92a6-2fb3d76cb66e",
      "source_variables": [
        "A"
      ],
      "target_variables": [
        "o"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "a116b76f-891d-4942-8b98-47d92bf215af",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "B"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "46b1b1e0-3dfe-496b-ada3-318823626365",
      "source_variables": [
        "C"
      ],
      "target_variables": [
        "G"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "41322d8c-6999-43d6-a994-901d05a366cc",
      "source_variables": [
        "E"
      ],
      "target_variables": [
        "π"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "d1a77914-8004-4720-9b39-edf2950f4f7a",
      "source_variables": [
        "G"
      ],
      "target_variables": [
        "π"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "932f02b0-53f0-4528-a6dc-4a8f803285e8",
      "source_variables": [
        "π"
      ],
      "target_variables": [
        "u"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "c354963f-5ca6-4d1f-8f59-39337849aedf",
      "source_variables": [
        "B"
      ],
      "target_variables": [
        "u"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "67acd2af-5200-40bb-ba5f-c4a3c33c860b",
      "source_variables": [
        "u"
      ],
      "target_variables": [
        "s_prime"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    }
  ],
  "parameters": [
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "42c7b770-cecf-4e8e-83ba-4b19e9ba3f9e",
      "name": "A",
      "value": [
        [
          0.9,
          0.05,
          0.05
        ],
        [
          0.05,
          0.9,
          0.05
        ],
        [
          0.05,
          0.05,
          0.9
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "b9ae7608-a2b5-4a52-bd1b-f8c0943c989a",
      "name": "B",
      "value": [
        [
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ]
        ],
        [
          [
            0.0,
            1.0,
            0.0
          ],
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ]
        ],
        [
          [
            0.0,
            0.0,
            1.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            1.0,
            0.0,
            0.0
          ]
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "b293df80-cdbd-47d2-afe9-e972daf1a6bf",
      "name": "C",
      "value": [
        [
          0.1,
          0.1,
          1.0
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "d51be8a7-eb26-43b9-876c-68f645e2cebb",
      "name": "D",
      "value": [
        [
          0.33333,
          0.33333,
          0.33333
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "206385a3-5a96-48e8-8e5c-b8198727b6dc",
      "name": "E",
      "value": [
        [
          0.33333,
          0.33333,
          0.33333
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "b30081d4-3c73-4f49-8465-fbd725ac18e5",
      "name": "num_actions: 3       # B actions_dim",
      "value": "3 (controlled by π)\nnum_timesteps: 30    # Number of simulation timesteps for all frameworks",
      "type_hint": null,
      "description": null
    }
  ],
  "equations": [],
  "time_specification": {
    "node_type": "TimeSpecification",
    "source_location": null,
    "metadata": {},
    "id": "acf08924-159f-4cd8-acdd-6c80c10bf665",
    "time_type": "Dynamic",
    "discretization": null,
    "horizon": "Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.",
    "step_size": null
  },
  "ontology_mappings": [
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "495703ec-31e0-41cf-8c1e-bd6db1d82384",
      "variable_name": "A",
      "ontology_term": "LikelihoodMatrix",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "11821e97-6a1d-475c-8645-4c03b827dfb8",
      "variable_name": "B",
      "ontology_term": "TransitionMatrix",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "1f95959b-00fa-4ccc-8e1b-adc8b45043b7",
      "variable_name": "C",
      "ontology_term": "LogPreferenceVector",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "5426f1c8-e687-4ee3-b752-e4e8f37b5567",
      "variable_name": "D",
      "ontology_term": "PriorOverHiddenStates",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "f71d40e1-2fc1-4699-9fd2-2f8cf63afdbe",
      "variable_name": "E",
      "ontology_term": "Habit",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "064c4a72-12b9-4b1f-b3a4-d7277b9dffaf",
      "variable_name": "F",
      "ontology_term": "VariationalFreeEnergy",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "283257fa-b84d-421d-bae4-c68c8669a36b",
      "variable_name": "G",
      "ontology_term": "ExpectedFreeEnergy",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "c0ee2384-874f-43e4-aae2-401ae608d3ed",
      "variable_name": "s",
      "ontology_term": "HiddenState",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "1e8872bf-bea5-4c0a-9e24-b6dec7da562a",
      "variable_name": "s_prime",
      "ontology_term": "NextHiddenState",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "b209fc19-f997-40ad-932d-5789f025a56c",
      "variable_name": "o",
      "ontology_term": "Observation",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "91bcaaaa-3964-4eba-af65-0d6be1d961b5",
      "variable_name": "π",
      "ontology_term": "PolicyVector # Distribution over actions",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "25c23265-4f8e-41ef-8749-d647abb803c3",
      "variable_name": "u",
      "ontology_term": "Action       # Chosen action",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "b14999e1-9c32-45c0-8875-a7f515d5cf36",
      "variable_name": "t",
      "ontology_term": "Time",
      "description": null
    }
  ],
  "source_format": null,
  "created_at": "2026-01-24T15:01:04.885713",
  "modified_at": "2026-01-24T15:01:04.885714",
  "checksum": null,
  "extensions": {
    "gnn_section": "ActInfPOMDP",
    "footer": "Active Inference POMDP Agent v1 - GNN Representation. \nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting.",
    "signature": "Cryptographic signature goes here"
  },
  "raw_sections": {
    "GNNSection": "ActInfPOMDP",
    "GNNVersionAndFlags": "GNN v1",
    "ModelName": "Active Inference POMDP Agent",
    "ModelAnnotation": "This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.",
    "StateSpaceBlock": "# Likelihood matrix: A[observation_outcomes, hidden_states]\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\n\n# Transition matrix: B[states_next, states_previous, actions]\nB[3,3,3,type=float]   # State transitions given previous state and action\n\n# Preference vector: C[observation_outcomes]\nC[3,type=float]       # Log-preferences over observations\n\n# Prior vector: D[states]\nD[3,type=float]       # Prior over initial hidden states\n\n# Habit vector: E[actions]\nE[3,type=float]       # Initial policy prior (habit) over actions\n\n# Hidden State\ns[3,1,type=float]     # Current hidden state distribution\ns_prime[3,1,type=float] # Next hidden state distribution\nF[π,type=float]       # Variational Free Energy for belief updating from observations\n\n# Observation\no[3,1,type=int]     # Current observation (integer index)\n\n# Policy and Control\nπ[3,type=float]       # Policy (distribution over actions), no planning\nu[1,type=int]         # Action taken\nG[π,type=float]       # Expected Free Energy (per policy)\n\n# Time\nt[1,type=int]         # Discrete time step",
    "Connections": "D>s\ns-A\ns>s_prime\nA-o\ns-B\nC>G\nE>π\nG>π\nπ>u\nB>u\nu>s_prime",
    "InitialParameterization": "# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\nA={\n  (0.9, 0.05, 0.05),\n  (0.05, 0.9, 0.05),\n  (0.05, 0.05, 0.9)\n}\n\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\nB={\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\n}\n\n# C: 3 observations. Preference in terms of log-probabilities over observations.\nC={(0.1, 0.1, 1.0)}\n\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\nD={(0.33333, 0.33333, 0.33333)}\n\n# E: 3 actions. Uniform habit used as initial policy prior.\nE={(0.33333, 0.33333, 0.33333)}",
    "Equations": "# Standard Active Inference update equations for POMDPs:\n# - State inference using Variational Free Energy with infer_states()\n# - Policy inference using Expected Free Energy = with infer_policies()\n# - Action selection from policy posterior: action = sample_action()\n# - Belief updating using Variational Free Energy with update_beliefs()",
    "Time": "Time=t\nDynamic\nDiscrete\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.",
    "ActInfOntologyAnnotation": "A=LikelihoodMatrix\nB=TransitionMatrix\nC=LogPreferenceVector\nD=PriorOverHiddenStates\nE=Habit\nF=VariationalFreeEnergy\nG=ExpectedFreeEnergy\ns=HiddenState\ns_prime=NextHiddenState\no=Observation\nπ=PolicyVector # Distribution over actions\nu=Action       # Chosen action\nt=Time",
    "ModelParameters": "num_hidden_states: 3  # s[3]\nnum_obs: 3           # o[3]\nnum_actions: 3       # B actions_dim=3 (controlled by π)\nnum_timesteps: 30    # Number of simulation timesteps for all frameworks",
    "Footer": "Active Inference POMDP Agent v1 - GNN Representation. \nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting.",
    "Signature": "Cryptographic signature goes here"
  }
}