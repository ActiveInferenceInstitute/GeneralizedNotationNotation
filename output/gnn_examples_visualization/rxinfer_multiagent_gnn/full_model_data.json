{
  "_HeaderComments": "# GNN Example: RxInfer Multi-agent Trajectory Planning\n# Format: Markdown representation of a Multi-agent Trajectory Planning model for RxInfer.jl\n# Version: 1.0\n# This file is machine-readable and represents the configuration for the RxInfer.jl multi-agent trajectory planning example.",
  "ModelName": "Multi-agent Trajectory Planning",
  "GNNSection": "RxInferMultiAgentTrajectoryPlanning",
  "GNNVersionAndFlags": "GNN v1",
  "ModelAnnotation": "This model represents a multi-agent trajectory planning scenario in RxInfer.jl.\nIt includes:\n- State space model for agents moving in a 2D environment\n- Obstacle avoidance constraints\n- Goal-directed behavior\n- Inter-agent collision avoidance\nThe model can be used to simulate trajectory planning in various environments with obstacles.",
  "StateSpaceBlock": "# Model parameters\ndt[1,type=float]               # Time step for the state space model\ngamma[1,type=float]            # Constraint parameter for the Halfspace node\nnr_steps[1,type=int]           # Number of time steps in the trajectory\nnr_iterations[1,type=int]      # Number of inference iterations\nnr_agents[1,type=int]          # Number of agents in the simulation\nsoftmin_temperature[1,type=float] # Temperature parameter for the softmin function\nintermediate_steps[1,type=int] # Intermediate results saving interval\nsave_intermediates[1,type=bool] # Whether to save intermediate results\n\n# State space matrices\nA[4,4,type=float]              # State transition matrix\nB[4,2,type=float]              # Control input matrix\nC[2,4,type=float]              # Observation matrix\n\n# Prior distributions\ninitial_state_variance[1,type=float]    # Prior on initial state\ncontrol_variance[1,type=float]          # Prior on control inputs\ngoal_constraint_variance[1,type=float]  # Goal constraints variance\ngamma_shape[1,type=float]               # Parameters for GammaShapeRate prior\ngamma_scale_factor[1,type=float]        # Parameters for GammaShapeRate prior\n\n# Visualization parameters\nx_limits[2,type=float]            # Plot boundaries (x-axis)\ny_limits[2,type=float]            # Plot boundaries (y-axis)\nfps[1,type=int]                   # Animation frames per second\nheatmap_resolution[1,type=int]    # Heatmap resolution\nplot_width[1,type=int]            # Plot width\nplot_height[1,type=int]           # Plot height\nagent_alpha[1,type=float]         # Visualization alpha for agents\ntarget_alpha[1,type=float]        # Visualization alpha for targets\ncolor_palette[1,type=string]      # Color palette for visualization\n\n# Environment definitions\ndoor_obstacle_center_1[2,type=float]    # Door environment, obstacle 1 center\ndoor_obstacle_size_1[2,type=float]      # Door environment, obstacle 1 size\ndoor_obstacle_center_2[2,type=float]    # Door environment, obstacle 2 center\ndoor_obstacle_size_2[2,type=float]      # Door environment, obstacle 2 size\n\nwall_obstacle_center[2,type=float]      # Wall environment, obstacle center\nwall_obstacle_size[2,type=float]        # Wall environment, obstacle size\n\ncombined_obstacle_center_1[2,type=float] # Combined environment, obstacle 1 center\ncombined_obstacle_size_1[2,type=float]   # Combined environment, obstacle 1 size\ncombined_obstacle_center_2[2,type=float] # Combined environment, obstacle 2 center\ncombined_obstacle_size_2[2,type=float]   # Combined environment, obstacle 2 size\ncombined_obstacle_center_3[2,type=float] # Combined environment, obstacle 3 center\ncombined_obstacle_size_3[2,type=float]   # Combined environment, obstacle 3 size\n\n# Agent configurations\nagent1_id[1,type=int]                   # Agent 1 ID\nagent1_radius[1,type=float]             # Agent 1 radius\nagent1_initial_position[2,type=float]   # Agent 1 initial position\nagent1_target_position[2,type=float]    # Agent 1 target position\n\nagent2_id[1,type=int]                   # Agent 2 ID\nagent2_radius[1,type=float]             # Agent 2 radius\nagent2_initial_position[2,type=float]   # Agent 2 initial position\nagent2_target_position[2,type=float]    # Agent 2 target position\n\nagent3_id[1,type=int]                   # Agent 3 ID\nagent3_radius[1,type=float]             # Agent 3 radius\nagent3_initial_position[2,type=float]   # Agent 3 initial position\nagent3_target_position[2,type=float]    # Agent 3 target position\n\nagent4_id[1,type=int]                   # Agent 4 ID\nagent4_radius[1,type=float]             # Agent 4 radius\nagent4_initial_position[2,type=float]   # Agent 4 initial position\nagent4_target_position[2,type=float]    # Agent 4 target position\n\n# Experiment configurations\nexperiment_seeds[2,type=int]            # Random seeds for reproducibility\nresults_dir[1,type=string]              # Base directory for results\nanimation_template[1,type=string]       # Filename template for animations\ncontrol_vis_filename[1,type=string]     # Filename for control visualization\nobstacle_distance_filename[1,type=string] # Filename for obstacle distance plot\npath_uncertainty_filename[1,type=string]  # Filename for path uncertainty plot\nconvergence_filename[1,type=string]       # Filename for convergence plot",
  "Connections": "# Model parameters\ndt > A\n(A, B, C) > state_space_model\n\n# Agent trajectories\n(state_space_model, initial_state_variance, control_variance) > agent_trajectories\n\n# Goal constraints\n(agent_trajectories, goal_constraint_variance) > goal_directed_behavior\n\n# Obstacle avoidance\n(agent_trajectories, gamma, gamma_shape, gamma_scale_factor) > obstacle_avoidance\n\n# Collision avoidance\n(agent_trajectories, nr_agents) > collision_avoidance\n\n# Complete planning system\n(goal_directed_behavior, obstacle_avoidance, collision_avoidance) > planning_system",
  "InitialParameterization": "# Model parameters\ndt=1.0\ngamma=1.0\nnr_steps=40\nnr_iterations=350\nnr_agents=4\nsoftmin_temperature=10.0\nintermediate_steps=10\nsave_intermediates=false\n\n# State space matrices\n# A = [1 dt 0 0; 0 1 0 0; 0 0 1 dt; 0 0 0 1]\nA={(1.0, 1.0, 0.0, 0.0), (0.0, 1.0, 0.0, 0.0), (0.0, 0.0, 1.0, 1.0), (0.0, 0.0, 0.0, 1.0)}\n\n# B = [0 0; dt 0; 0 0; 0 dt]\nB={(0.0, 0.0), (1.0, 0.0), (0.0, 0.0), (0.0, 1.0)}\n\n# C = [1 0 0 0; 0 0 1 0]\nC={(1.0, 0.0, 0.0, 0.0), (0.0, 0.0, 1.0, 0.0)}\n\n# Prior distributions\ninitial_state_variance=100.0\ncontrol_variance=0.1\ngoal_constraint_variance=0.00001\ngamma_shape=1.5\ngamma_scale_factor=0.5\n\n# Visualization parameters\nx_limits={(-20, 20)}\ny_limits={(-20, 20)}\nfps=15\nheatmap_resolution=100\nplot_width=800\nplot_height=400\nagent_alpha=1.0\ntarget_alpha=0.2\ncolor_palette=\"tab10\"\n\n# Environment definitions\ndoor_obstacle_center_1={(-40.0, 0.0)}\ndoor_obstacle_size_1={(70.0, 5.0)}\ndoor_obstacle_center_2={(40.0, 0.0)}\ndoor_obstacle_size_2={(70.0, 5.0)}\n\nwall_obstacle_center={(0.0, 0.0)}\nwall_obstacle_size={(10.0, 5.0)}\n\ncombined_obstacle_center_1={(-50.0, 0.0)}\ncombined_obstacle_size_1={(70.0, 2.0)}\ncombined_obstacle_center_2={(50.0, 0.0)}\ncombined_obstacle_size_2={(70.0, 2.0)}\ncombined_obstacle_center_3={(5.0, -1.0)}\ncombined_obstacle_size_3={(3.0, 10.0)}\n\n# Agent configurations\nagent1_id=1\nagent1_radius=2.5\nagent1_initial_position={(-4.0, 10.0)}\nagent1_target_position={(-10.0, -10.0)}\n\nagent2_id=2\nagent2_radius=1.5\nagent2_initial_position={(-10.0, 5.0)}\nagent2_target_position={(10.0, -15.0)}\n\nagent3_id=3\nagent3_radius=1.0\nagent3_initial_position={(-15.0, -10.0)}\nagent3_target_position={(10.0, 10.0)}\n\nagent4_id=4\nagent4_radius=2.5\nagent4_initial_position={(0.0, -10.0)}\nagent4_target_position={(-10.0, 15.0)}\n\n# Experiment configurations\nexperiment_seeds={(42, 123)}\nresults_dir=\"results\"\nanimation_template=\"{environment}_{seed}.gif\"\ncontrol_vis_filename=\"control_signals.gif\"\nobstacle_distance_filename=\"obstacle_distance.png\"\npath_uncertainty_filename=\"path_uncertainty.png\"\nconvergence_filename=\"convergence.png\"",
  "Equations": "# State space model:\n# x_{t+1} = A * x_t + B * u_t + w_t,  w_t ~ N(0, control_variance)\n# y_t = C * x_t + v_t,                v_t ~ N(0, observation_variance)\n#\n# Obstacle avoidance constraint:\n# p(x_t | obstacle) ~ N(d(x_t, obstacle), gamma)\n# where d(x_t, obstacle) is the distance from position x_t to the nearest obstacle\n#\n# Goal constraint:\n# p(x_T | goal) ~ N(goal, goal_constraint_variance)\n# where x_T is the final position\n#\n# Collision avoidance constraint:\n# p(x_i, x_j) ~ N(||x_i - x_j|| - (r_i + r_j), gamma)\n# where x_i, x_j are positions of agents i and j, r_i, r_j are their radii",
  "Time": "Dynamic\nDiscreteTime\nModelTimeHorizon=nr_steps",
  "ActInfOntologyAnnotation": "dt=TimeStep\ngamma=ConstraintParameter\nnr_steps=TrajectoryLength\nnr_iterations=InferenceIterations\nnr_agents=NumberOfAgents\nsoftmin_temperature=SoftminTemperature\nA=StateTransitionMatrix\nB=ControlInputMatrix\nC=ObservationMatrix\ninitial_state_variance=InitialStateVariance\ncontrol_variance=ControlVariance\ngoal_constraint_variance=GoalConstraintVariance",
  "ModelParameters": "nr_agents=4\nnr_steps=40\nnr_iterations=350",
  "Footer": "Multi-agent Trajectory Planning - GNN Representation for RxInfer.jl",
  "Signature": "Creator: AI Assistant for GNN\nDate: 2024-07-27\nStatus: Example for RxInfer.jl multi-agent trajectory planning"
}