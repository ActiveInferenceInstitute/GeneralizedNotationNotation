{
  "_HeaderComments": "# GNN Example: Dynamic Perception Model with Policy Selection\n# Format: Markdown representation of a Dynamic Perception model with Policy Selection in Active Inference format\n# Version: 1.0\n# This file is machine-readable",
  "ModelName": "Dynamic perception with Policy Selection v1",
  "GNNSection": "DynamicPerceptionWithPolicySelection",
  "ImageFromPaper": "image.png",
  "GNNVersionAndFlags": "GNN v1",
  "ModelAnnotation": "This model relates a single hidden state to a single observable modality. It is a dynamic model because it tracks changes in the hidden state through time. There is Action applied via policy selection (\u03c0).",
  "StateSpaceBlock": "A[2,2,type=float]\nD[2,1,type=float]\nB[2,len(\u03c0),2,type=float]\n\u03c0=[2]\nC=[2,1]\nG[len(\u03c0),type=float]\ns[2,1,type=float]\ns_prime[2,1,type=float] # Next state (s at t+1)\no[2,1,type=float]\nt[1,type=int]",
  "Connections": "D-s\ns-A\nA-o\ns-B\nB-s_prime\nC>G\nG>\u03c0",
  "InitialParameterization": "A={(0.7,0.3),(0.4,0.6)}\nD={(0.5),(0.5)}\nC={(0.8),(0.2)}\nB={( ( (0.9,0.1),(0.1,0.9) ), ( (0.1,0.9),(0.9,0.1) ) )} # B[policy_index][state_from][state_to]",
  "Equations": "s=sigma((1/2)(lnD+ln(B^dagger_{pi,tau}s_{pi,tau+1}))+lnA^T*o_tau) # for tau=1, policy \u03c0\ns=sigma((1/2)(ln(B_{pi,tau-1}s_{pi,tau-1})+ln(B^dagger_{pi,tau}s_{pi,tau+1}))+lnA^T*o_tau) # for tau>1, policy \u03c0\nG[\u03c0]=sum_tau(As_{pi,tau}(ln(A*s_{pi,tau})-lnC_tau)-diag(A^TlnA)*s_{pi,tau})\n\u03c0=sigma(-G)",
  "Time": "Dynamic\nDiscreteTime=s\nModelTimeHorizon=Unbounded",
  "ActInfOntologyAnnotation": "A=RecognitionMatrix\nB=TransitionMatrix\nC=Preference\nD=Prior\nG=ExpectedFreeEnergy\ns=HiddenState\ns_prime=NextHiddenState\no=Observation\n\u03c0=PolicyVector\nt=Time",
  "ModelParameters": "num_hidden_states_factors: [2]\nnum_obs_modalities: [2]\nnum_control_action_dims: [2] # From len(\u03c0) which is 2, used in B matrix",
  "Footer": "Dynamic perception with Policy Selection v1",
  "Signature": "NA"
}