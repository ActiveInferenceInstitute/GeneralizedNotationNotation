{
  "timestamp": "2025-07-25T09:58:38.896882",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "analysis_results": [
    {
      "file_path": "/Users/4d/Documents/GitHub/GeneralizedNotationNotation/input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4085,
      "line_count": 127,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 67
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 74
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 81
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 84
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 87
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 94
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 118
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 119
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 120
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 43
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 46
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 51
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 88
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 93
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 94
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 97
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 100
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 103
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 104
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 105
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 106
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 107
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 108
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 109
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 110
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 111
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 112
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 113
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 114
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 115
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 120
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 43
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 46
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 47
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 48
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 51
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 118
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 119
        }
      ],
      "connections": [],
      "sections": [
        {
          "title": "GNN Example: Active Inference POMDP Agent",
          "level": 1,
          "line": 1
        },
        {
          "title": "GNN Version: 1.0",
          "level": 1,
          "line": 2
        },
        {
          "title": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "level": 1,
          "line": 3
        },
        {
          "title": "GNNSection",
          "level": 2,
          "line": 5
        },
        {
          "title": "GNNVersionAndFlags",
          "level": 2,
          "line": 8
        },
        {
          "title": "ModelName",
          "level": 2,
          "line": 11
        },
        {
          "title": "ModelAnnotation",
          "level": 2,
          "line": 14
        },
        {
          "title": "StateSpaceBlock",
          "level": 2,
          "line": 22
        },
        {
          "title": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "level": 1,
          "line": 23
        },
        {
          "title": "Transition matrix: B[states_next, states_previous, actions]",
          "level": 1,
          "line": 26
        },
        {
          "title": "Preference vector: C[observation_outcomes]",
          "level": 1,
          "line": 29
        },
        {
          "title": "Prior vector: D[states]",
          "level": 1,
          "line": 32
        },
        {
          "title": "Habit vector: E[actions]",
          "level": 1,
          "line": 35
        },
        {
          "title": "Hidden State",
          "level": 1,
          "line": 38
        },
        {
          "title": "Observation",
          "level": 1,
          "line": 42
        },
        {
          "title": "Policy and Control",
          "level": 1,
          "line": 45
        },
        {
          "title": "Time",
          "level": 1,
          "line": 50
        },
        {
          "title": "Connections",
          "level": 2,
          "line": 53
        },
        {
          "title": "InitialParameterization",
          "level": 2,
          "line": 66
        },
        {
          "title": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "level": 1,
          "line": 67
        },
        {
          "title": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "level": 1,
          "line": 74
        },
        {
          "title": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "level": 1,
          "line": 81
        },
        {
          "title": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "level": 1,
          "line": 84
        },
        {
          "title": "E: 3 actions. Uniform habit used as initial policy prior.",
          "level": 1,
          "line": 87
        },
        {
          "title": "Equations",
          "level": 2,
          "line": 90
        },
        {
          "title": "Standard Active Inference update equations for POMDPs:",
          "level": 1,
          "line": 91
        },
        {
          "title": "- State inference using Variational Free Energy with infer_states()",
          "level": 1,
          "line": 92
        },
        {
          "title": "- Policy inference using Expected Free Energy = with infer_policies()",
          "level": 1,
          "line": 93
        },
        {
          "title": "- Action selection from policy posterior: action = sample_action()",
          "level": 1,
          "line": 94
        },
        {
          "title": "Time",
          "level": 2,
          "line": 96
        },
        {
          "title": "ActInfOntologyAnnotation",
          "level": 2,
          "line": 102
        },
        {
          "title": "ModelParameters",
          "level": 2,
          "line": 117
        },
        {
          "title": "Footer",
          "level": 2,
          "line": 122
        },
        {
          "title": "Signature",
          "level": 2,
          "line": 126
        }
      ],
      "semantic_analysis": {
        "model_type": "POMDP",
        "complexity_level": "complex",
        "domain": "unknown",
        "key_concepts": [
          "state",
          "action",
          "observation",
          "transition",
          "policy"
        ],
        "potential_issues": []
      },
      "complexity_metrics": {
        "variable_count": 70,
        "connection_count": 0,
        "connectivity_ratio": 0.0,
        "complexity_score": 0.0,
        "estimated_computation_time": 0.0,
        "memory_requirements": 560
      },
      "patterns": {
        "design_patterns": [
          "State-Based Design",
          "Action-Oriented Design"
        ],
        "anti_patterns": [
          "High Complexity"
        ],
        "recommendations": [
          "Consider breaking into smaller modules"
        ]
      },
      "analysis_timestamp": "2025-07-25T09:58:38.899204"
    }
  ],
  "model_insights": [
    {
      "file_path": "/Users/4d/Documents/GitHub/GeneralizedNotationNotation/input/gnn_files/actinf_pomdp_agent.md",
      "model_summary": "This is a complex POMDP model with 70 variables.",
      "strengths": [
        "Well-defined variable structure",
        "Reasonable coupling between components"
      ],
      "weaknesses": [],
      "improvement_opportunities": [
        "Consider modularization"
      ]
    }
  ],
  "code_suggestions": [
    {
      "file_path": "/Users/4d/Documents/GitHub/GeneralizedNotationNotation/input/gnn_files/actinf_pomdp_agent.md",
      "implementation_suggestions": [
        "Use PyMDP library for POMDP implementation",
        "Implement belief state updates"
      ],
      "optimization_opportunities": [],
      "testing_recommendations": [
        "Create unit tests for each variable",
        "Implement integration tests for connections"
      ]
    }
  ],
  "documentation_generated": [
    {
      "file_path": "/Users/4d/Documents/GitHub/GeneralizedNotationNotation/input/gnn_files/actinf_pomdp_agent.md",
      "model_description": "\n# actinf_pomdp_agent.md\n\nThis is a complex POMDP model with 70 variables.\n\n## Model Overview\n- **Type**: POMDP\n- **Complexity**: complex\n- **Variables**: 70\n- **Connections**: 0\n\n## Key Components\n",
      "api_documentation": [
        "- `Example`: Variable defined at line 1",
        "- `Version`: Variable defined at line 2",
        "- `matrix`: Variable defined at line 23",
        "- `matrix`: Variable defined at line 26",
        "- `vector`: Variable defined at line 29"
      ],
      "usage_examples": [
        "```python",
        "# Example usage will be generated here",
        "```"
      ],
      "troubleshooting": []
    }
  ]
}