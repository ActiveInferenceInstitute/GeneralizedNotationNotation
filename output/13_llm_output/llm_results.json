{
  "timestamp": "2026-02-15T14:27:36.942587",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "provider_matrix": {
    "ollama": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "openai": {
      "available": true,
      "models": [
        "gpt-4",
        "gpt-3.5-turbo"
      ],
      "selected_model": null
    },
    "anthropic": {
      "available": true,
      "models": [
        "claude-3",
        "claude-2"
      ],
      "selected_model": null
    }
  },
  "analysis_results": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4306,
      "line_count": 130,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122
        },
        {
          "name": "num_timesteps",
          "definition": "num_timesteps: 30",
          "line": 123
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121
        }
      ],
      "connections": [],
      "sections": [
        {
          "name": "GNN Example: Active Inference POMDP Agent",
          "line": 1
        },
        {
          "name": "GNN Version: 1.0",
          "line": 2
        },
        {
          "name": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "line": 3
        },
        {
          "name": "GNNSection",
          "line": 5
        },
        {
          "name": "GNNVersionAndFlags",
          "line": 8
        },
        {
          "name": "ModelName",
          "line": 11
        },
        {
          "name": "ModelAnnotation",
          "line": 14
        },
        {
          "name": "StateSpaceBlock",
          "line": 22
        },
        {
          "name": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "Transition matrix: B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "Preference vector: C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "Prior vector: D[states]",
          "line": 32
        },
        {
          "name": "Habit vector: E[actions]",
          "line": 35
        },
        {
          "name": "Hidden State",
          "line": 38
        },
        {
          "name": "Observation",
          "line": 43
        },
        {
          "name": "Policy and Control",
          "line": 46
        },
        {
          "name": "Time",
          "line": 51
        },
        {
          "name": "Connections",
          "line": 54
        },
        {
          "name": "InitialParameterization",
          "line": 67
        },
        {
          "name": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "line": 68
        },
        {
          "name": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "line": 75
        },
        {
          "name": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "line": 82
        },
        {
          "name": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "line": 85
        },
        {
          "name": "E: 3 actions. Uniform habit used as initial policy prior.",
          "line": 88
        },
        {
          "name": "Equations",
          "line": 91
        },
        {
          "name": "Standard Active Inference update equations for POMDPs:",
          "line": 92
        },
        {
          "name": "- State inference using Variational Free Energy with infer_states()",
          "line": 93
        },
        {
          "name": "- Policy inference using Expected Free Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "- Action selection from policy posterior: action = sample_action()",
          "line": 95
        },
        {
          "name": "- Belief updating using Variational Free Energy with update_beliefs()",
          "line": 96
        },
        {
          "name": "Time",
          "line": 98
        },
        {
          "name": "ActInfOntologyAnnotation",
          "line": 104
        },
        {
          "name": "ModelParameters",
          "line": 119
        },
        {
          "name": "Footer",
          "line": 125
        },
        {
          "name": "Signature",
          "line": 129
        }
      ],
      "semantic_analysis": {
        "variable_count": 73,
        "connection_count": 0,
        "complexity_score": 73,
        "semantic_patterns": [],
        "variable_types": {
          "Active": 1,
          "1": 1,
          "A": 1,
          "B": 1,
          "C": 1,
          "D": 1,
          "E": 1,
          "3": 8,
          "action": 1,
          "30": 1,
          "unknown": 56
        },
        "connection_types": {}
      },
      "complexity_metrics": {
        "total_elements": 73,
        "variable_complexity": 73,
        "connection_complexity": 0,
        "density": 0.0,
        "cyclomatic_complexity": -71
      },
      "patterns": {
        "patterns": [
          "High variable count - complex model"
        ],
        "anti_patterns": [
          "No connections defined"
        ],
        "suggestions": [
          "Consider breaking down into smaller modules"
        ]
      },
      "analysis_timestamp": "2026-02-15T14:27:36.946342",
      "llm_summary_error": "All providers failed for analysis",
      "status": "SUCCESS",
      "analysis": "LLM-assisted analysis complete",
      "documentation": {
        "file_path": "input/gnn_files/actinf_pomdp_agent.md",
        "model_overview": ""
      },
      "llm_prompt_outputs": {
        "summarize_content": "Your document is well-structured and easy to understand. Here's a concise overview of your model and key variables:\n\n1. **Key Variables**:\n   - Hidden states: A[3]\n   \n   - Observations: A_n\n      \n   - Actions/Controls: B\n      \n   - Actions/Habit: A_k\n     \n   - Actions/Actions/Hidden Actions/Habits (including actions and habit)\n\n   - Key variables: A, B, C, D\n2. **Critical Parameters**:\n   - The key parameters of the model include:\n   \n   - Hidden states: A[3]\n   \n   - Observations: A_n\n      \n   - Actions/Controls: A_k\n    \n   - Actions/Habit: A_l\n     \n   - Actions/Actions/Hidden Actions/Habits (including actions and habit)\n\n   - Key parameters: A, B, C, D\n3. **Notable Features**:\n   \n   - Special properties or constraints:\n   \n   - Unique aspects of this model design\n\n4. **Use Cases**: \n   - The action space is defined by actions_dim=3. This means the number of actions will be fixed and controlled by the policy vector. \n\n   - The parameterization with Variational Free Energy (VFE) allows for flexibility in adapting models to new data or domains based on prior knowledge, but can lead to a potential loss of predictive performance when parameters are not chosen appropriately. \n  \n  Note: You mention \"certain features\" are critical because the model is designed using probabilistic graphical methods that rely heavily on these key variables.",
        "explain_model": "Here's a concise overview of the model:\n1. **Model Purpose**: The model represents a classic Active Inference agent for discrete POMDPs that takes into account 3 different actions and their corresponding probabilities based on the available hidden states.\n\n2. **Core Components**:\n   - **Hidden States**: Each observation is assigned to one of three possible actions (0, 1) while there are three states (heads or tails).\n   - **Habit**: The policy assigns each action equally across all states and actions.\n   - **Activation Functions**\n\n3. **Model Dynamics**: It represents the agent's behavior during training with parameters estimated from available observations for each state-action pair, which captures how they update beliefs over time based on their probabilities of corresponding actions (in case of \"Hidden State\").\n   - **Forward Probabilities** \n\n4. **Active Inference Context**: The model uses a priori knowledge to compute beliefs and predictions about the actions at different timesteps based on the observed data. It maps these beliefs to observable actions, which are updated according to the policy in each subsequent iteration until it reaches its termination criterion (for Bayesian inference) or \"decision boundary\" reached (GNN-based).\n\n5. **Practical Implications**: This model can be applied to various domains and situations based on available data for specific actions and hidden states/actions, which help identify trends, detect changes in behavior, make predictions about future outcomes, manage uncertainty from action selection decisions, optimize policies across multiple time steps, predict behaviors through belief updating, etc.\n\n6. **Key Relationships**:\n   - **Action Selection**: Policy uses \"Habit\" to provide a policy prior, which is updated based on the observed data and probabilities of corresponding actions for each state-action pair (based on a priori knowledge).\n   - **Base Belief**: Base beliefs are initialized as random until new information arises.\n   - **Policy Prior**: Policy updates based on learned probability distributions across all states/actions in order to update posterior probabilities, which can then be used to learn new beliefs and predictions based on new data from the input observation (state-action pairs).\n\nPlease provide clear and concise explanations for these components so they are easy to understand.",
        "identify_components": "LLM analysis for identify_components was not available. Please ensure that Ollama is running and the required model is installed.",
        "analyze_structure": "You've outlined a comprehensive exploration of the model's structure and its characteristics, highlighting key aspects such as graph properties, connection patterns, and mathematical concepts like matrix dimensions, dependencies, and symmetry. Here are some additional insights:\n\n1. **Graph Properties**: The graph topology is highly connected (connected components), which suggests that the agent learns through a greedy algorithm with dynamic behavior. This could indicate an iterative optimization process rather than simple planning.\n\n2. **Variable Analysis**: Variable connections can be analyzed to identify relationships between variables, patterns within them, and potential dependencies. This analysis demonstrates how the graph structure reflects the domain being modeled.\n\n3. **Mathematical Structure**: The modeling pattern identified suggests a decision-making algorithm that adapts based on observed outcomes rather than relying solely on predefined policies or actions. This could indicate an iterative process of exploration and planning in the context of knowledge representation.\n\n4. **Complexity Assessment**: The model demonstrates how it handles uncertainty, ambiguity, and complexity (innovation). This indicates a proactive design approach where the agent anticipates changes over time, reflecting its ability to learn from experience rather than being reactive.\n\n5. **Design Patterns**: A key insight is that the graph structure facilitates exploration by allowing for discovery of unknown values in the policy space without having to guess their location or evaluate them sequentially before choosing an action. This suggests a pragmatic approach based on learning and iteration, reflecting the domain-specific understanding of knowledge representation frameworks employed here (innovation).\n\nThese insights collectively underscore the agent's design as being proactive, flexible, and able to learn from experience while adapting to uncertainty and complexity.",
        "extract_parameters": "Based on the information provided, the `gnn_model` model has the following components:\n\n1. **State Space**: It represents a 3D continuous distribution over space and time, with each state having one possible outcome (observation) and three possible states for the hidden states. The number of observations is represented by \"observation\", which can be finite or infinite.\n\n2. **Transition Matrix**: This matrix contains the probabilities for each transition from one observation to another. It assigns an empirical probability distribution over actions between all observed outcomes, allowing the agent to make predictions based on the current state and action. The transitions are bounded in space by 10 units (to account for global uncertainty) and can also be constrained within a single range (to ensure data convergence).\n\n3. **Preferred Policy**: This matrix represents the policy that best predicts the observed outcomes given initial observations, hidden states, actions, or control factors with high confidence. It assigns an empirical belief distribution over actions across all possible actions selected at each timestep and is bounded within a single action range (to account for global uncertainty).\n\n4. **Habit**: This matrix represents a state-dependent sequence of policies that are used to predict observed outcomes based on initial observations, hidden states, and actions with high confidence. It assigns an empirical prior distribution over these actions across all possible actions selected at each timestep and is bounded within a single action range (to account for global uncertainty).\n\n5. **Initial Policy**: This matrix represents the policy that best predicts current observed outcomes given initial observation/observation values and hidden states of current observations, with high confidence in its predictions. It assigns an empirical prior distribution over these policies across all possible actions selected at each timestep and is bounded within a single action range (to account for global uncertainty).\n\n6. **Habit**: This matrix represents the policy that best predicts observed outcomes based on initial observation/observation values, with high confidence in its predictions across all possible actions selected at each timestep. It assigns an empirical prior distribution over these policies across all possible actions selected at each timestep and is bounded within a single action range (to account for global uncertainty).\n\n7. **Initial Policy**: This matrix represents the policy that best predicts observed outcomes based on initial observation/observation values, with high confidence in its predictions across all possible actions selected at each timestep but constrained by initial state-dependent constraints (e.g., no pre-determined policy). It assigns an empirical prior distribution over these policies across all possible actions and is bounded within a single action range (to account for global uncertainty) based on the available initial state dependence matrix.\n8. **Initial Policy**: This matrix represents the policy that best predicts observed outcomes with given initial observation/observation values, but no pre-determined behavior constraint or prior distribution over policies across all possible actions selected at each timestep and is bounded within a single action range (to account for global uncertainty). It assigns an empirical prior distribution over these policies.\n9. **Action**: This matrix represents the policy used to predict observed outcomes based on initial observation/observation values, with high confidence in its predictions across all possible actions selected at each timestamp but constrained by temporal dependencies and window constraints (for actions involving no pre-determined behavior constraint) or prior distributions among action choices. It assigns an empirical prior distribution over these policies across all possible actions.\n10. **Initialization Strategies**: This matrix represents the initial policy used to make predictions based on observations/observation values, with high confidence in its predictions and bounded by initial state dependence matrices. It also includes parameterizations for action selection (via the choice of actions) within each modality.\n11. **Model Parameters**: This matrix contains parameters that are adjusted as the model is updated or modified using a feedback loop from the previous models. It represents the ensemble of beliefs derived from different mechanisms and can be customized according to specific goals, preferences, or learning objectives.\n12. **Configuration Summary**: This list summarizes all parameters with respect to their roles in modeling an action-based Markov Decision Process (MDP) based on observed actions/observations and a set of initial states chosen at each timestep from the model's history.",
        "practical_applications": "This section provides an overview of the active inference agent used for a discrete POMDP, covering key concepts such as:\n\n1. **Model Description**: A list of variables representing states, actions, preferences, biases, etc., along with their corresponding probabilities and values over different observation modalities (e.g., state-observation, action) during policy updates using Bayesian inference or GNN models.\n\n2. **Initialization**: Details about initial parameters (`A`, `B`, `C`) that guide the agent's actions towards each action in the POMDP as it is initialized.\n\n3. **Model Parameters**: A list of variables representing the state, action space, history, etc., while tracking the agent's preferences and biases throughout its behavior during update processes using GNN models or Bayesian inference.\n\n4. **Initialization Requirements**: An outline of requirements for initial parameter settings that allow the agent to start learning from prior data and behave accordingly.\n\n5. **Model Performance Expectations**: A list showcasing potential performance expectations, including metrics such as convergence rate (the difference between current state-action setpoints and goal states), accuracy metrics (e.g., probability over past actions or loss on last observed action).\n\n6. **Implementation Considerations**: Specific requirements for deployment scenarios with real-time capabilities, cost constraints related to hardware/software dependencies, maintenance and monitoring needs for the agent's use in various domains.\n\nPlease note that different implementations may differ in terms of computational resources, data availability, communication protocols (e.g., RESTful APIs), etc. For detailed information about specific implementation considerations, please refer to [System Support for OpenAI Community](https://openai.org/support/) or the [Implementations and Technologies](https://github.com/OpenAI/backend-contrib) repository.",
        "technical_description": "LLM analysis for custom prompt technical_description was not available. Please ensure that Ollama is running and the required model is installed.",
        "nontechnical_description": "LLM analysis for custom prompt nontechnical_description was not available. Please ensure that Ollama is running and the required model is installed.",
        "runtime_behavior": "LLM analysis for custom prompt runtime_behavior was not available. Please ensure that Ollama is running and the required model is installed."
      }
    }
  ],
  "model_insights": [
    {
      "model_complexity": "high",
      "recommendations": [
        "Consider breaking down into smaller modules",
        "High variable count - consider grouping related variables",
        "Consider breaking down into smaller modules"
      ],
      "strengths": [],
      "weaknesses": [
        "Complex model may be difficult to understand",
        "No connections defined"
      ],
      "generation_timestamp": "2026-02-15T14:29:38.929562"
    }
  ],
  "code_suggestions": [
    {
      "optimizations": [],
      "improvements": [
        "Many variables lack type annotations - consider adding explicit types"
      ],
      "best_practices": [
        "Use descriptive variable names",
        "Group related variables together"
      ],
      "generation_timestamp": "2026-02-15T14:29:38.936312"
    }
  ],
  "documentation_generated": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "model_overview": "\n# actinf_pomdp_agent.md Model Documentation\n\nThis GNN model contains 73 variables and 0 connections.\n\n## Model Structure\n- **Variables**: 73 defined\n- **Connections**: 0 defined\n- **Complexity**: 73 total elements\n\n## Key Components\n",
      "variable_documentation": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1,
          "description": "Variable defined at line 1"
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2,
          "description": "Variable defined at line 2"
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68,
          "description": "Variable defined at line 68"
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75,
          "description": "Variable defined at line 75"
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82,
          "description": "Variable defined at line 82"
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85,
          "description": "Variable defined at line 85"
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88,
          "description": "Variable defined at line 88"
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121,
          "description": "Variable defined at line 121"
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "num_timesteps",
          "definition": "num_timesteps: 30",
          "line": 123,
          "description": "Variable defined at line 123"
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69,
          "description": "Variable defined at line 69"
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76,
          "description": "Variable defined at line 76"
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83,
          "description": "Variable defined at line 83"
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86,
          "description": "Variable defined at line 86"
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89,
          "description": "Variable defined at line 89"
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99,
          "description": "Variable defined at line 99"
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102,
          "description": "Variable defined at line 102"
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105,
          "description": "Variable defined at line 105"
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106,
          "description": "Variable defined at line 106"
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107,
          "description": "Variable defined at line 107"
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108,
          "description": "Variable defined at line 108"
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109,
          "description": "Variable defined at line 109"
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110,
          "description": "Variable defined at line 110"
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111,
          "description": "Variable defined at line 111"
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112,
          "description": "Variable defined at line 112"
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113,
          "description": "Variable defined at line 113"
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114,
          "description": "Variable defined at line 114"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115,
          "description": "Variable defined at line 115"
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116,
          "description": "Variable defined at line 116"
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117,
          "description": "Variable defined at line 117"
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121,
          "description": "Variable defined at line 121"
        }
      ],
      "connection_documentation": [],
      "usage_examples": [
        {
          "title": "Variable Usage",
          "description": "Example variables: Example, Version, matrix",
          "code": "# Example variable usage\nExample = ...\nVersion = ...\nmatrix = ..."
        }
      ],
      "generation_timestamp": "2026-02-15T14:29:38.937926"
    }
  ]
}