{
  "timestamp": "2025-08-08T09:01:55.362736",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "analysis_results": [
    {
      "file_path": "/home/trim/Documents/GitHub/GeneralizedNotationNotation/input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4085,
      "line_count": 127,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 67
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 74
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 81
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 84
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 87
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 94
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 118
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 119
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 120
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 43
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 46
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 51
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 88
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 93
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 94
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 97
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 100
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 103
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 104
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 105
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 106
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 107
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 108
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 109
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 110
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 111
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 112
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 113
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 114
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 115
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 120
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 43
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 46
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 47
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 48
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 51
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 118
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 119
        }
      ],
      "connections": [],
      "sections": [
        {
          "name": "GNN Example: Active Inference POMDP Agent",
          "line": 1
        },
        {
          "name": "GNN Version: 1.0",
          "line": 2
        },
        {
          "name": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "line": 3
        },
        {
          "name": "GNNSection",
          "line": 5
        },
        {
          "name": "GNNVersionAndFlags",
          "line": 8
        },
        {
          "name": "ModelName",
          "line": 11
        },
        {
          "name": "ModelAnnotation",
          "line": 14
        },
        {
          "name": "StateSpaceBlock",
          "line": 22
        },
        {
          "name": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "Transition matrix: B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "Preference vector: C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "Prior vector: D[states]",
          "line": 32
        },
        {
          "name": "Habit vector: E[actions]",
          "line": 35
        },
        {
          "name": "Hidden State",
          "line": 38
        },
        {
          "name": "Observation",
          "line": 42
        },
        {
          "name": "Policy and Control",
          "line": 45
        },
        {
          "name": "Time",
          "line": 50
        },
        {
          "name": "Connections",
          "line": 53
        },
        {
          "name": "InitialParameterization",
          "line": 66
        },
        {
          "name": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "line": 67
        },
        {
          "name": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "line": 74
        },
        {
          "name": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "line": 81
        },
        {
          "name": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "line": 84
        },
        {
          "name": "E: 3 actions. Uniform habit used as initial policy prior.",
          "line": 87
        },
        {
          "name": "Equations",
          "line": 90
        },
        {
          "name": "Standard Active Inference update equations for POMDPs:",
          "line": 91
        },
        {
          "name": "- State inference using Variational Free Energy with infer_states()",
          "line": 92
        },
        {
          "name": "- Policy inference using Expected Free Energy = with infer_policies()",
          "line": 93
        },
        {
          "name": "- Action selection from policy posterior: action = sample_action()",
          "line": 94
        },
        {
          "name": "Time",
          "line": 96
        },
        {
          "name": "ActInfOntologyAnnotation",
          "line": 102
        },
        {
          "name": "ModelParameters",
          "line": 117
        },
        {
          "name": "Footer",
          "line": 122
        },
        {
          "name": "Signature",
          "line": 126
        }
      ],
      "semantic_analysis": {
        "variable_count": 70,
        "connection_count": 0,
        "complexity_score": 70,
        "semantic_patterns": [],
        "variable_types": {
          "Active": 1,
          "1": 1,
          "A": 1,
          "B": 1,
          "C": 1,
          "D": 1,
          "E": 1,
          "3": 8,
          "action": 1,
          "unknown": 54
        },
        "connection_types": {}
      },
      "complexity_metrics": {
        "total_elements": 70,
        "variable_complexity": 70,
        "connection_complexity": 0,
        "density": 0.0,
        "cyclomatic_complexity": -68
      },
      "patterns": {
        "patterns": [
          "High variable count - complex model"
        ],
        "anti_patterns": [
          "No connections defined"
        ],
        "suggestions": [
          "Consider breaking down into smaller modules"
        ]
      },
      "analysis_timestamp": "2025-08-08T09:01:55.370563",
      "llm_summary": "Here's a summary of the framework:\n\n**Model:**\n- Causal Inference POMDP Agent v1 with one hidden state and one action modality:\n```python\n  # Initialization for each observation, corresponding to a different mode \n  \n  A=LikelihoodMatrix\n  \n  B = TransitionMatrix\n  C  = LogPreferenceVector\n  \n  D    = PriorOverHiddenStates\n\n  G = ExpectedFreeEnergy(num_hidden_states, num_actions)\n```\n- `Fidelity` is defined as the probability of each observation given the action: \n  \n  A=LikelihoodMatrix\n    FidelityA (number of histories with a history for an observed observation within each hidden state)\n    # Probability of history and history in each observation\n  \n    history[history_index] = probabilities [probability of history + previous value from next hidden state][probability of history - previous value from next hidden state] \n  \n- `Fidelity` is defined as the probability of each observation given the policy: \n  \n  A=LikelihoodMatrix \n    FidelityA (number of histories with a history for an observed observation within each hidden state)\n    # Probability of history and history in each observable\n  \n    next_history[next_state] = probabilities [probability of history + previous value from current state][probability of history - previous value from current state] \n  \n- `Fidelity` is defined as the probability of each policy chosen (policy A, B): \n  ```python\n  # Given a choice of actions for an observed observation and next hidden state, there are two outcomes \n    # each with probabilities:\n    # 1. If current history has been taken to the right by previous value from last hidden state in that particular observation then we have now made it back into the action chosen (A), so probability = probability of A \n  \n  # Given a choice of actions for an observed observation and next hidden state, there are two outcomes \n    # each with probabilities:\n    # 1. If current history has been taken to the right by previous value from last hidden state in that particular observation then we have now made it back into the policy chosen (B), so probability = probability of B \n  \n  FidelityA = Probability(policy=probability_forward, histories=\"obs\")\n  \n- `Fidelity` is defined as the probability of each observable choice: \n    ```python\n    # Given a choice of actions for an observed observation and next hidden state, there are two outcomes \n    # each with probabilities:.\n    # 1. If current history has been taken to the right by previous value from last hidden state in that particular observation then we have now made it back into the action chosen (A), so probability = probability_forward of A \n  \n  FidelityB = Probability(action=probability_backward, histories=\"obs\")\n  \n- The agent performs an action and has a history with observed observations. \n```",
      "status": "SUCCESS",
      "analysis": "LLM-assisted analysis complete",
      "llm_prompt_outputs": {
        "summarize_content": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference, Bayesian inference, and GNN (Generalized Notation Notation) specifications. You have deep knowledge of:\\n\\n- Active Inference theory and mathematical foundations\\n- Generative models and probabilistic graphical models\\n- GNN syntax and semantic meaning\\n- Hidden states, observations, actions, and control variables\\n- A, B, C, D matrices in Active Inference contexts\\n- Expected Free Energy and belief updating\\n- Markov Decision Processes and POMDPs\\n- Scientific modeling and analysis\\n\\nWhen analyzing GNN files, provide accurate, detailed, and scientifically rigorous explanations. Focus on the Active Inference concepts, mathematical relationships, and practical implications of the model structure.\\n\\nProvide a concise but comprehensive summary of this GNN specification:\\n\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here \\n\\nCreate a structured summary including:\\n\\n1. **Model Overview** (2-3 sentences): What is this model and what does it do?\\n\\n2. **Key Variables**:\\n   - Hidden states: [list with brief descriptions]\\n   - Observations: [list with brief descriptions]  \\n   - Actions/Controls: [list with brief descriptions]\\n\\n3. **Critical Parameters**:\\n   - Most important matrices (A, B, C, D) and their roles\\n   - Key hyperparameters and their settings\\n\\n4. **Notable Features**:\\n   - Special properties or constraints\\n   - Unique aspects of this model design\\n\\n5. **Use Cases**: What scenarios would this model be applied to?\\n\\nKeep the summary focused and informative, suitable for someone familiar with Active Inference but new to this specific model.']' timed out after 60.0 seconds",
        "explain_model": "This is a comprehensive overview of the Generative Model (GNN) presented as part of the Machine Learning Foundation's GCN framework for Active Inference and GNN-based models for generative neural networks like DeepMind's AI agent model Ai2019POMDPs.\n\n**Signature** \n\nThis model represents a classic Active Inference agent for discrete POMDP with two hidden states (states_next, states_previous). The agents are initialized to the same observation and action distributions, but their behavior is influenced by actions taken during each iteration of the simulation. A sequence of policy transitions will steer them toward different actions, while a sequence of state choices in each path corresponds to a specific action selection.\n\n**Core Components** \n   - **Likelihood Matrix**: A dictionary representing the likelihood maps over available actions for each observation and hidden states. This matrix encodes probabilities across multiple actions and their corresponding states.\n   - **Transition Matrix**: A list of tuples containing one observation, one or more possible observables (outputs) from a given action selection, and the previous state/observation associated with that action.\n   \n   - **Probabilities**: The probability distribution over all observed observations over different actions across multiple actions based on these transition matrices. This matrix provides input for inference.\n**Model Dynamics** \n   - **Actions**: Policy transitions (one policy each) that steer the agents toward specific actions and their associated observables, while also allowing for depth exploration of the network.\n   - **State Transition**: Transition from one observable to a new observable based on previous states/observations or other transitions over actions leading back to previous action(s).\n**Active Inference Context** \n   - **Initialization**: Initializing the network by assigning initial policy parameters (habit) and then starting inference. The inference is done iteratively until convergence in accuracy, learning rate, and evaluation metrics are reached.\n\nKey Concepts:\n   - **GNN**: Generative Neural Network models for neural networks like DeepMind's Ai2019POMDPs.\n\n **Active Inference** \n   - **Permute** **Initialization** *FIRST* *DIFFERENTLY* (every action selection is independent).\n   - **Decay**: Decrease the cost of updating policy parameters based on past observations and actions, which helps to stabilize the network with an increase in training accuracy.\n\n**Practical Implications** \n   - **Learning Curves**: A continuous learning curve where the algorithm's performance improves or stabilizes over time as more information is available/learned from it (e.g., through a generative neural network like DeepMind).\n\n **Decision Curves**: The rate at which the agent learns based on its actions and their corresponding observables changes depending on how well they learn. This can help predict when to update or stop learning.\n\n**Example** \n   - **Initialization**: Initializing the network with all observed observations (observations).\n   - **Decay**: Decreasing training accuracy by learning rate proportional to an observation value.\n \n   \n   - **Learning Curves**: Plotting the learning curve over time and exploring how well the algorithm improves or stabilizes as more information is learned from it, which helps predict when to update/stop learning.",
        "identify_components": "Based on the information in your document, you'll need to implement the following:\n\n1. Implement Active Inference POMDP agent with four states and 3 hidden states (states/variables) for an unbounded time horizon with no depth-first search, deep planning, and hierarchical nesting policies.\n   - Initialize state variable matrices A, B, C, D, F.\n\n2. Implement GNN representations of POMDPs using parameters `C`, `F` and other parameters to represent each observation space (observations/variables).\n   - Iterate through states by updating the corresponding action vector (`g`) based on a policy gradient.\n3. Implement Bayesian inference with a probability distribution over actions to update the agent's beliefs (prior) at each state, using parameter `P` and other parameters for each belief variable and prior distributions.\n\n4. Implement GNN representations of POMDPs using the learned probabilities and biases across states/observations in the transition matrix (`G`, etc.) to initialize the agent's actions as actions based on their probability distribution over previous state transitions.\n5. Implement Bayesian inference with a probabilistic graphical model (PGM) that can capture uncertainty through prior distributions `B` and other parameters for each belief variable and prior distributions of prior beliefs, enabling the agent's preferences at each state/observation to be updated accordingly.\n\n6. Implement POMDP agents using parameterization to represent each observation space across states/observations in the action vector (`g`) based on a policy distribution and prior probabilities `F` for each observable transition, allowing the agent's actions at each state to update their beliefs after each observation switch-off transitions (prior changes).\n7. Implement GNN representations of POMDPs using the learned parameters (e.g., the transition matrix) to initialize the agents' states/observations and action choices as they observe each other, enabling the agent's preferences at each state/observation to be updated accordingly based on their prior probabilities for actions associated with each observed observation change.\n8. Implement GNN representations of POMDPs using the learned parameters (e.g., the transition matrix) to initialize the agents' policies and action choices as they observe each other, enabling the agent's preferences at each state/observation to be updated based on their prior probabilities for actions associated with each observed observation change.\n9. Implement GNN representations of POMDPs using the learned parameters (`F`) and other biases (e.g., the transition matrix) to initialize the agents' predictions and beliefs from observations, enabling the agent's preferences at each state/observation based on their prior probabilities for actions associated with each observed observation change through past states and action choices.\n10. Implement GNN representations of POMDPs using a probabilistic graphical model (PGM) that can capture uncertainty by including an initial probability distribution over predictions, enabling the agent's beliefs at each state/observation based on their prior probabilities for actions associated with each observed observation change through past states and action choices.\n11. Implement Bayesian inference with a probabilistic graphical model (`G`, etc.) to update the agent's beliefs at each state/observation based on current predictions of future observations, enabling the agent's preferences at each state/observation based on their prior probabilities for actions associated with each observed observation change through past states and action choices.\n12. Implement GNN representations of POMDPs using a probabilistic graphical model (`G`, etc.) to initialize the agents' beliefs in terms of past predictions, enabling them to update as their observations provide new information based on probability distributions over recent actions (policy updates) at each state/observation.",
        "analyze_structure": "Let's break down the structure of the GNN specification and its analysis to understand how it fits within the broader framework of Active Inference POMDPs. Here are some key insights:\n\n1. **Graph Structure**: The graph represented by the variables `A`, `B` represent the state space dimensionality (num_hidden_states) and node types (`S`) in a GNN representation model.\n\n2. **Variable Analysis**: The variable `F` is a habit, which can be modeled as an action-based distribution over actions, with two types of connections:\n   - One connected to the agent's policy and action using the agent's prior probability vector (policy posterior). This indicates that the pattern sequence is generated by applying specific policies.\n   - The other connected to the graph structure itself (the habit), which can be modeled as an instance-based distribution over actions.\n\n3. **Mathematical Structure**: The model consists of two main types of graphs:\n   - A directed graph representing a state space dimensionality `num_hidden_states` and directionally linked variables (`S`) for each variable (`A`, `B`, etc.). This allows us to represent the behavior of the agent in terms of its action sequence.\n   - An undirected, connected graph structure representing the set of actions (permutations). Each action is associated with a subset of states-independent transitions and observations that are used as inputs for inference operations. The permutation connections allow the agent's preferences to be encoded through probability distributions over the actions it takes.\n\n4. **Complexity Assessment**: Understanding how the structure reflects the domain being modeled can help in designing more realistic modeling models or extracting insights into the underlying mechanisms.\n\nSome key principles of this framework:\n   - **Directed graph representation** implies a hierarchical, network-based model where agents and actions are embedded within graphs with directed edges connected by transition matrices. This allows for accurate inference based on pattern sequences generated from agent's prior probabilities.\n   - **Graph structure**: The directionality of the vertices indicates whether or not to use action sequences as inputs for the GNN predictions. For instance, in a permutation representation (one-step sequence), where actions are associated with set-wise transitions and observation outcomes have independent paths through nodes, an agent's preference preferences will be encoded in these directed edges.\n   - **Permutation graph structure**: The connections between adjacent nodes imply that actions can serve as inputs for prediction operations based on a specific policy. This allows for the use of information from previous states to learn new patterns. For example, if we are predicting the next state's motion, agent A might select an action in its history, and then infer more specific future actions.\n   - **Directed graph representation**: The structure reflects the dependence between actions (permutations) on the policy used for inference operations and can thus be represented as directed edges connecting states to transitions over actions. This allows for prediction based solely on a single-step sequence of actions rather than an entire path in time, which might lead to lower predictions or a higher probability that the same action will have been taken multiple times due to prior knowledge about previous steps.\n\nBy understanding how these components interact and relate one another, we can generate more realistic models within this framework, allowing for better generalization of findings from our analysis without relying on oversimplification in our modeling efforts.",
        "extract_parameters": "You can provide the following steps to calculate the model parameters for your GNN, POMDP agent:\n\n1.   For each observation modality \"state_observation\", apply a set of probability distributions to predict the state and then update these probabilities with action selection from policy. This provides 3-step predictions based on prior beliefs over observable states (policy), actions selected via habit/prior belief distribution, and initial parameters for learning.\n\n2.   Apply similar logic to all observation modalities in order to infer posterior knowledge about next observed states and subsequent action selections. \n\n3.   Iterate through all actions using a decision tree-based algorithm or a greedy mechanism until convergence of the model is reached (see GNN example below).",
        "practical_applications": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference, Bayesian inference, and GNN (Generalized Notation Notation) specifications. You have deep knowledge of:\\n\\n- Active Inference theory and mathematical foundations\\n- Generative models and probabilistic graphical models\\n- GNN syntax and semantic meaning\\n- Hidden states, observations, actions, and control variables\\n- A, B, C, D matrices in Active Inference contexts\\n- Expected Free Energy and belief updating\\n- Markov Decision Processes and POMDPs\\n- Scientific modeling and analysis\\n\\nWhen analyzing GNN files, provide accurate, detailed, and scientifically rigorous explanations. Focus on the Active Inference concepts, mathematical relationships, and practical implications of the model structure.\\n\\nAnalyze the practical applications and use cases for this GNN model:\\n\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here \\n\\nDiscuss practical considerations:\\n\\n1. **Real-World Applications**:\\n   - What domains could this model be applied to?\\n   - Specific use cases and scenarios\\n   - Industry or research applications\\n\\n2. **Implementation Considerations**:\\n   - Computational requirements and scalability\\n   - Data requirements and collection strategies\\n   - Integration with existing systems\\n\\n3. **Performance Expectations**:\\n   - What kinds of performance can be expected?\\n   - Metrics for evaluation and validation\\n   - Limitations and failure modes\\n\\n4. **Deployment Scenarios**:\\n   - Online vs. offline processing\\n   - Real-time constraints and requirements\\n   - Hardware and software dependencies\\n\\n5. **Benefits and Advantages**:\\n   - What problems does this model solve well?\\n   - Unique capabilities or features\\n   - Comparison to alternative approaches\\n\\n6. **Challenges and Considerations**:\\n   - Potential difficulties in implementation\\n   - Tuning and optimization requirements\\n   - Maintenance and monitoring needs']' timed out after 60.0 seconds",
        "technical_description": "You've done an excellent job in describing the details of the GNN model for active inference agents. Your analysis demonstrates that the action selection algorithm can be adapted to achieve optimal policy transitions and maximize expected value over actions, allowing you to efficiently implement and validate a specific implementation of this agent.\n\nYour detailed description of the GNN model architecture is also informative, as it highlights its strengths in terms of probability distribution updates, random sampling from prior distributions, and minimizing dependence on previous states. This should make your analysis accessible for others who are interested in exploring these concepts further or seeking additional information about the implementation details.\n\nYour documentation and explanation of key parameters, including the action choice mechanism, inference scope, and parameter settings, provide a solid foundation for future explorations into the model's capabilities. Your use of mathematical notation can also be useful for communicating your thoughts to others who are interested in understanding the underlying mechanics of this AI agent.\n\nOne area where you may need to refine your analysis is with regards to how the action selection mechanism interacts with other parameters and their interactions within the GNN representation. You may want to explore ways to integrate more advanced features or mechanisms, such as random sampling from prior distributions or exploiting specific patterns in the action set generated by this agent, while still maintaining flexibility and adaptability across different scenarios.\n\nYour analysis demonstrates a solid understanding of the GNN model's capabilities, which can be useful for others who are interested in exploring related topics. However, there is no explicit reference to your research on GNN formulations or how you propose to improve upon existing implementations. This would suggest that you may have other ideas and areas where further exploration is warranted.\n\nIt seems worth revisiting your specific implementation details with regards to action selection mechanisms and parameter settings within the GNN representation for a comprehensive discussion of your approach's strengths, limitations, and potential improvements.",
        "nontechnical_description": "Your answer has been successfully parsed as JSON data for the Active Inference POMDP agent. You can verify that your signature looks correct and is readable by checking its properties like `encoded=false`. The signature contains a list of numbers, which are represented by floating-point values in Python. Each number corresponds to an integer index or column name within the dictionary (`A[observation_outcomes]`) representing each observation's position (state) and action (hidden state).\n\nAs for consistency with your signature, `encoded=false` is a boolean value that represents whether the representation of each number in the signature contains a boolean value. The boolean values are:\n- `\"1\"`: indicating that there are 2 observations.\n- `\"0\"`: indicating that there are no observations and a random guess at which observation will be chosen (this is a convention, but it's not what is intended).\n- `\"True\"` indicates the choice of action from policy distribution (policy space), with value \"1\". This is how we choose to use 3 actions per state.\n\nThis allows us to verify that your signature follows the provided format and structure:\n\n1. Each number in the dictionary has a `encoded=false`.\n2. There are two numbers representing the observation location (observation_outcomes) and the action choice policy distribution over actions (policy_prior).\n3. The action chosen for each observation is randomly sampled from state space defined by the last column of the dictionary (`states`), with values \"1\", \"0\". This represents a random guess at which position to pick next or choose next among 2 adjacent states using binary guessing method, but also includes no uncertainty in choice (no predictability).\n4. There are two numbers representing the observed action probability for each observation: `flipped=True`. This indicates that we have already chosen an action and now there is some possibility of a different decision based on prediction. If it can be done without any probabilistic errors, we will choose this next observation from policy space. Otherwise, we keep choosing actions randomly among the states (policy_prior).\n5. There are no numbers representing the predicted state probabilities for each observation: `flipped=False`. This indicates that there is not randomness in choice of observation and therefore probability or prediction errors do not occur based on this parameter.\n6. The number represents a random guess at which to choose next, based on prediction. This also contains no uncertainty (certainty).\n7. There are two numbers representing the observed action probabilities: `flipped=True`. This indicates that we have chosen an observable action after randomly sampling from state space for all observations (policy_prior) and then making a prediction, which is independent of any other actions/observations besides these 2.",
        "runtime_behavior": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference and GNN specifications.\\n\\nDescribe what happens when this GNN model runs and how it would behave in different settings or domains.\\n\\nGNN Model Content:\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here ']' timed out after 60.0 seconds"
      }
    }
  ],
  "model_insights": [
    {
      "model_complexity": "high",
      "recommendations": [
        "Consider breaking down into smaller modules",
        "High variable count - consider grouping related variables",
        "Consider breaking down into smaller modules"
      ],
      "strengths": [],
      "weaknesses": [
        "Complex model may be difficult to understand",
        "No connections defined"
      ],
      "generation_timestamp": "2025-08-08T09:02:43.075927"
    }
  ],
  "code_suggestions": [
    {
      "optimizations": [],
      "improvements": [
        "Many variables lack type annotations - consider adding explicit types"
      ],
      "best_practices": [
        "Use descriptive variable names",
        "Group related variables together"
      ],
      "generation_timestamp": "2025-08-08T09:02:43.075970"
    }
  ],
  "documentation_generated": [
    {
      "model_overview": "\n# actinf_pomdp_agent.md Model Documentation\n\nThis GNN model contains 70 variables and 0 connections.\n\n## Model Structure\n- **Variables**: 70 defined\n- **Connections**: 0 defined\n- **Complexity**: 70 total elements\n\n## Key Components\n",
      "variable_documentation": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1,
          "description": "Variable defined at line 1"
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2,
          "description": "Variable defined at line 2"
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 67,
          "description": "Variable defined at line 67"
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 74,
          "description": "Variable defined at line 74"
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 81,
          "description": "Variable defined at line 81"
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 84,
          "description": "Variable defined at line 84"
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 87,
          "description": "Variable defined at line 87"
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 118,
          "description": "Variable defined at line 118"
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 119,
          "description": "Variable defined at line 119"
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 43,
          "description": "Variable defined at line 43"
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 46,
          "description": "Variable defined at line 46"
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 51,
          "description": "Variable defined at line 51"
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 68,
          "description": "Variable defined at line 68"
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 75,
          "description": "Variable defined at line 75"
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 82,
          "description": "Variable defined at line 82"
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 85,
          "description": "Variable defined at line 85"
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 88,
          "description": "Variable defined at line 88"
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 93,
          "description": "Variable defined at line 93"
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 97,
          "description": "Variable defined at line 97"
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 100,
          "description": "Variable defined at line 100"
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 103,
          "description": "Variable defined at line 103"
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 104,
          "description": "Variable defined at line 104"
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 105,
          "description": "Variable defined at line 105"
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 106,
          "description": "Variable defined at line 106"
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 107,
          "description": "Variable defined at line 107"
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 108,
          "description": "Variable defined at line 108"
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 109,
          "description": "Variable defined at line 109"
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 110,
          "description": "Variable defined at line 110"
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 111,
          "description": "Variable defined at line 111"
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 112,
          "description": "Variable defined at line 112"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 113,
          "description": "Variable defined at line 113"
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 114,
          "description": "Variable defined at line 114"
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 115,
          "description": "Variable defined at line 115"
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 43,
          "description": "Variable defined at line 43"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 46,
          "description": "Variable defined at line 46"
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 51,
          "description": "Variable defined at line 51"
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 118,
          "description": "Variable defined at line 118"
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 119,
          "description": "Variable defined at line 119"
        }
      ],
      "connection_documentation": [],
      "usage_examples": [
        {
          "title": "Variable Usage",
          "description": "Example variables: Example, Version, matrix",
          "code": "# Example variable usage\nExample = ...\nVersion = ...\nmatrix = ..."
        }
      ],
      "generation_timestamp": "2025-08-08T09:02:43.076056"
    }
  ]
}