# EXTRACT_PARAMETERS

You've already outlined the key components of the model specification, including:

1. **Initial Parameters:** The initial parameters define the domain and structure of the POMDP agent. These are represented by `StateSpaceBlock` objects with dimensions `n_obs x n_actions`. Each block represents a single action chosen by the agent. They are annotated with their role as initial conditions, which describe how they will be initialized for each observation.
2. **Model Parameters:** The model defines the structure and interpretation of the POMDP agent through its matrices (`A`, `B`), blocks (which represent actions) and transition matrix(s). These models have dimensions based on the number of observations in the domain, with each observation represented by a single dimension. Each action selection is determined by its state distribution across the observed states ($S_{obs} = \{x_1\}$-$\{x_n\}) for one state and then updated iteratively through actions using an inference process.
3. **Variation Parameters:** These parameters define how the model evolves over time based on policy updates (`Î±`). A global parameterization of the agent is used, which represents a global change in policy (policy update) across all observations at each observation. This allows for updating predictions based on observed state distributions and behavior through action selection.
4. **Feature Hierarchies:** These represent different actions or actions sequences that are chosen by the agent based on their prior beliefs over previous states. Features hierarchies provide a way to identify patterns within an ensemble of actions, allowing the agent's actions to fit into predefined categories (e.g., "action sequence").
5. **Optimization Parameters:** These describe how the model learns from policy-based updates and inference processes. The types are defined as `state_observation` objects or `observations`, where they represent a single observation in time, location, actions, etc., based on the policy action selection. These parameters are annotated with their roles for each iteration until convergence to fit into predefined categories (e.g., "action sequence" is initialized).
6. **Optimization Hierarchies:** Each configuration summarizes how the model learns from an ensemble of policies and actions in turn, ultimately leading to predictions that agree on overall beliefs over time based on observed state distributions across all observations.