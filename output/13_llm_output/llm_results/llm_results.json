{
  "timestamp": "2026-01-07T06:30:28.220734",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "provider_matrix": {
    "ollama": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "openai": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "anthropic": {
      "available": false,
      "models": [],
      "selected_model": null
    }
  },
  "analysis_results": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4244,
      "line_count": 129,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121
        }
      ],
      "connections": [],
      "sections": [
        {
          "name": "GNN Example: Active Inference POMDP Agent",
          "line": 1
        },
        {
          "name": "GNN Version: 1.0",
          "line": 2
        },
        {
          "name": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "line": 3
        },
        {
          "name": "GNNSection",
          "line": 5
        },
        {
          "name": "GNNVersionAndFlags",
          "line": 8
        },
        {
          "name": "ModelName",
          "line": 11
        },
        {
          "name": "ModelAnnotation",
          "line": 14
        },
        {
          "name": "StateSpaceBlock",
          "line": 22
        },
        {
          "name": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "Transition matrix: B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "Preference vector: C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "Prior vector: D[states]",
          "line": 32
        },
        {
          "name": "Habit vector: E[actions]",
          "line": 35
        },
        {
          "name": "Hidden State",
          "line": 38
        },
        {
          "name": "Observation",
          "line": 43
        },
        {
          "name": "Policy and Control",
          "line": 46
        },
        {
          "name": "Time",
          "line": 51
        },
        {
          "name": "Connections",
          "line": 54
        },
        {
          "name": "InitialParameterization",
          "line": 67
        },
        {
          "name": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "line": 68
        },
        {
          "name": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "line": 75
        },
        {
          "name": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "line": 82
        },
        {
          "name": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "line": 85
        },
        {
          "name": "E: 3 actions. Uniform habit used as initial policy prior.",
          "line": 88
        },
        {
          "name": "Equations",
          "line": 91
        },
        {
          "name": "Standard Active Inference update equations for POMDPs:",
          "line": 92
        },
        {
          "name": "- State inference using Variational Free Energy with infer_states()",
          "line": 93
        },
        {
          "name": "- Policy inference using Expected Free Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "- Action selection from policy posterior: action = sample_action()",
          "line": 95
        },
        {
          "name": "- Belief updating using Variational Free Energy with update_beliefs()",
          "line": 96
        },
        {
          "name": "Time",
          "line": 98
        },
        {
          "name": "ActInfOntologyAnnotation",
          "line": 104
        },
        {
          "name": "ModelParameters",
          "line": 119
        },
        {
          "name": "Footer",
          "line": 124
        },
        {
          "name": "Signature",
          "line": 128
        }
      ],
      "semantic_analysis": {
        "variable_count": 72,
        "connection_count": 0,
        "complexity_score": 72,
        "semantic_patterns": [],
        "variable_types": {
          "Active": 1,
          "1": 1,
          "A": 1,
          "B": 1,
          "C": 1,
          "D": 1,
          "E": 1,
          "3": 8,
          "action": 1,
          "unknown": 56
        },
        "connection_types": {}
      },
      "complexity_metrics": {
        "total_elements": 72,
        "variable_complexity": 72,
        "connection_complexity": 0,
        "density": 0.0,
        "cyclomatic_complexity": -70
      },
      "patterns": {
        "patterns": [
          "High variable count - complex model"
        ],
        "anti_patterns": [
          "No connections defined"
        ],
        "suggestions": [
          "Consider breaking down into smaller modules"
        ]
      },
      "analysis_timestamp": "2026-01-07T06:30:28.223952",
      "llm_summary": "Here's the complete GNN model with some additional information and further analysis:\n```python\n# Classify the behavior based on the action selection method:\nclass F(object):\n    def __init__(self, state_observations, actions=None):\n        self.state = states[0]\n        self.actions = {\n            (1, 2),\n            (0, 1) and ((3 - 1 + i * 4)/num_states for i in range(len(s))\n                : actions is True if (i % num_actions == actions_index[i]) else None\n        }\n\n    def predict(self):\n       # Note: The action selection from policy prior does not affect the behavior\n         self.policy=policy1\n\n        # Initialize hidden states and return state observations\n    def sample_action(self, x_n=(0, 4), n_actions=(3)):\n     assert isinstance(x_, (int, tuple))\n       if len(s) == len(x_):\n           for i in range(len(x_)))\n               self.observation = x_[i]\n         else:\n            # The policy returns a sequence of actions\n             action1=policy1\n                 self.action1 = x_ [i-1 : (i+2)].index([0])\n     return x_.next()\n\n# Function to infer the next observation based on previous observations\n   class Fv(object):\n      def __init__(self, state_observations)\n       # Use a plan in case of planning horizon but no actions are available\n           self.actions = {\n                (1, 2),\n                 (0, 1) and ((3 - 1 + i * 4)/num_states for i in range(len(s))\n                    : action is True if (i % num_actions == action_index[i]) else None\n           }\n\n     def predict(self):\n          # Note: The action selection from policy prior does not affect the behavior\n         return self.policy(state=self.observation)\n\n\n# Define actions as follows\n   class Fv1(F ):\n      def __init__(self, state_observations)\n       # Use a plan in case of planning horizon but no actions are available\n           self.actions = {\n                (0, 2): ((3 - 0 + i * 4)/num_states for i in range(len(s))\n                    : action is True if (i % num_actions == action_index[i]) else None\n             }\n\n  def sample_action(self, x_, n_observations=None) -> str:\n      \"\"\"Sample a policy sequence of actions from policy prior\n           based on the current observation\n       Args:\n          state_observation : state observations and actions to sample.\n               The sequence is structured as (policy+action1),( action2),\n                 ...).\n              See Example for code.\n         n_observations = number of observed observations if available\n   Returns a tuple containing sampled policy, or None if no actions are available\n       Args:\n          x_  : current observation\n           n_observations   : number of observed observations\n     Returns the sampled policy value (policy sequence in order) and any errors\nA class defines an action sequence from which to sample actions. The function samples a set of actions that follow a given sequence based on the input parameters. Note: This is not supported with GNN, as it has no support other than type inference.\n   A class also supports applying policies in the sense of actions sequences (see Example).\nA class extends `Fv` and `Fv1`, which are used to infer policies from policy prior. It defines an instance of `Fv` that accepts a single input parameter, `state_observation`. When using GNN with policies, it finds patterns in the available observation data for each possible action selection and samples those sequences. Then it returns a tuple containing sampled actions (policy sequence) and any errors encountered during sampling.\nThe class is designed to support using policy prior instead of GNN, as it does not directly provide support other than type inference. It can be used with GNN model specifications and may serve as an example for how to use this implementation.",
      "status": "SUCCESS",
      "analysis": "LLM-assisted analysis complete",
      "documentation": {
        "file_path": "input/gnn_files/actinf_pomdp_agent.md",
        "model_overview": ""
      },
      "llm_prompt_outputs": {
        "summarize_content": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference, Bayesian inference, and GNN (Generalized Notation Notation) specifications. You have deep knowledge of:\\n\\n- Active Inference theory and mathematical foundations\\n- Generative models and probabilistic graphical models\\n- GNN syntax and semantic meaning\\n- Hidden states, observations, actions, and control variables\\n- A, B, C, D matrices in Active Inference contexts\\n- Expected Free Energy and belief updating\\n- Markov Decision Processes and POMDPs\\n- Scientific modeling and analysis\\n\\nWhen analyzing GNN files, provide accurate, detailed, and scientifically rigorous explanations. Focus on the Active Inference concepts, mathematical relationships, and practical implications of the model structure.\\n\\nProvide a concise but comprehensive summary of this GNN specification:\\n\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\nF[\u03c0,type=float]       # Variational Free Energy for belief updating from observations\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n# - Belief updating using Variational Free Energy with update_beliefs()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here \\n\\nCreate a structured summary including:\\n\\n1. **Model Overview** (2-3 sentences): What is this model and what does it do?\\n\\n2. **Key Variables**:\\n   - Hidden states: [list with brief descriptions]\\n   - Observations: [list with brief descriptions]  \\n   - Actions/Controls: [list with brief descriptions]\\n\\n3. **Critical Parameters**:\\n   - Most important matrices (A, B, C, D) and their roles\\n   - Key hyperparameters and their settings\\n\\n4. **Notable Features**:\\n   - Special properties or constraints\\n   - Unique aspects of this model design\\n\\n5. **Use Cases**: What scenarios would this model be applied to?\\n\\nKeep the summary focused and informative, suitable for someone familiar with Active Inference but new to this specific model.']' timed out after 60.0 seconds",
        "explain_model": "Based on the documentation, I have a comprehensive explanation of the Active Inference POMDP agent and its components:\n\n1. **Model Purpose**: This is about how to represent this POMDP agent that can learn from an unbounded time horizon with planning horizon and no precision modulation. Specifically, it represents the agent's goal as \"go ahead and explore\" - action 0 = learning new state in next observation (observable), action 1 = exploring new state in previous observation (hidden) and actions 2-4 are actions that can be taken from policy, exploration, or planning based on prior probability. It also includes an initial policy prior with preferences of LQE as inferred via the hidden states and Actions are learned using GNN.\n\n2. **Core Components**: This model represents the input space (states), output space (actions/hidden state distributions) and parameters (num_observation, num_action1). Each component is represented by a dictionary entry that maps to its corresponding action or policy transition based on the current state. There are no explicit computations within these components but they can be described using the following key relationships:\n   - **Action selection**: Actions(state=0) and actions(state=4)=ActionsSelectionModel(action, probability = True),\n   - **Observation**: Observations with observed = action == Action[1] then action is learned from policy.\n   - **Learning**: The algorithm learns to select next observation based on the probabilities of each state in previous states for each sequence of actions taken (actions selected) and explored among observations as input parameters,\n   - **Planning**: Planning model involves learning from policies chosen by actions selected but it has no explicit computations within these models.\n3. **Model Dynamics**: This is about how to implement Active Inference principles using GNN: \n   - **Initialization**: Initialization of learned state variables (s[n]), observed states, and learned policy probabilities are established based on the current time horizon and prior probability distribution over actions selected.\n   - **Learning**: The algorithm learns to select next observation by learning from previous observations as inputs for each sequence of policies taken with actions chosen and explored among observations in output space.\n\n4. **Active Inference Context**: This is about how to learn Active Inference principles using GNN: \n   - **Initialization**: Initializing learned state variables (s[n]) based on a learning process implemented as an agent interface,\n   - **Learning**: The algorithm learns to select next observation by updating observed and observable states in output space based on prior probabilities of actions taken.\n\n5. **Practical Implications**: This is about how to learn from Active Inference principles using GNN:\n  - **Action Selection**: Actions selection model can learn the learning context (observable state) or policy parameters for each action selected as input through its initial policy distribution, where a change in policy will involve a corresponding change of actions. \n\n  - **Planning**: The algorithm learns to select next observation based on learned policies and explored sequences using PolicyOptimization from PolicyGraphs.\n\n6. **Decision**: This is about how to learn from Active Inference principles using GNN:\n  - **Initializing Policy**: Initialized learning parameters are initialized as initial policy distribution for each action selected, where the new value of actions[k] would be used as a policy transition when given new state[n]. For instance, for actions(h) = next states=0 and h={{1}}. This is also shown by actions([action]) = {{2},{4}], 3-step sequence where action(s)=[1] are observed and visited from previous observation to observe new state[n+1].\n\nPlease summarize the key points in your response.",
        "identify_components": "You've already covered key concepts in the GNN specification:\n- A(hidden states) represents an object's state space with 3 observations, 2 hidden states (states), and 4 actions (actions). These are represented by sets of vectors (`H`) representing the joint probability distributions for each observation and action.\n\nTo provide a systematic breakdown:\n\n1. **State Variables**:\n   - Variable names/dimensions: Observation variables, hidden state variables, action variable variables.\n   - Action parameters can include actions specified in the specification or implemented using prior belief networks (pbpn). These correspond to 3 discrete actions in the agent's policy network. The reward and loss functions are represented by matrices (`P`) representing the joint transition dynamics, while the history and reward updates relate to the past observations/actions of the current observation(s), respectively.\n\n2. **Observation Variables**:\n   - Observation variables represent an object's state space with 3 states. These represent each action/observation pair or a combination of actions and their immediate consequences if it was chosen as initial action, which can be represented by vectors (`H`). The covariance matrix represents the joint probability distribution for each observation based on each choice across these actions.\n\n3. **Control Variables**:\n   - Control variables are representations of control policies/actions in the policy network (policy matrices) and decisions made within the policy networks. These represent specific actions or decision sets to be taken, which can vary with changes in state transitions of different actions/states for all observations. Control parameters relate directly to the action selection that was chosen based on previous states as well as predictions of future states and actions if a particular choice is taken along any path between initial observation(s) and final observed (action choices).\n\n4. **Parameters**:\n   - Parameter matrix represents the individual components of the parameterized model, which can include loss functions for each variable and learning rate/adaptation parameters based on changes in action selection across actions as well as prediction predictions over previous observations. This is similar to the concept used in Bayesian inference models where a set of states define an observable, whereas this is utilized in GNN-like methods using learned beliefs or prior probability distributions representing conditional probabilities for each observation and corresponding transition dynamics between observed (action choices) and hidden state transitions based on prediction predictions across actions.\n\n5. **Hyperparameter**:\n   - Initialization parameters represent initial values of all variables that define the system, which can be implemented as default settings in a probabilistic graphical model/policy network to initialize or learn these prior distributions for subsequent iterations of this network. In contrast with Bayesian inference models, GNN-like methods like Bayesian inference do not rely on predefined initialization functions; instead they use learned beliefs from prior knowledge and predictions across actions that are derived using the probability distribution over observation sequences in the policy networks as well as future states (actions/observation pairs) based on prediction predictions.\n\nRegarding your question about parameterization, I have a summary of steps for GNN-like methods using learned belief distributions:\n\n1. **Initialization**:\n   - Initial state initialization involves generating parameters that are initialized randomly with each observed observation and action pair in the policy network (policy matrices) and decisions made within the policy networks (policies). These represent individual components from prior beliefs/prior probabilities for all observations as well as predictions of future actions based on prediction predictions across actions.\n\n2. **Generalized Notation Versus Bayesian Inference**:\n   - Bayesian inference models refer to them because they rely on prior distributions over model parameters or the probability distribution over observable states, whereas GNN provides a probabilistic graphical representation where each observable and action pair is represented by an individual variable vector which can also represent probabilities of each observation from previous observations/actions. In contrast, GNN-like methods use learned beliefs as predictions across actions that are derived using prior probabilities from prior distributions or belief representations used in the policy networks for prediction predictions (from actions choices) over predicted future observations based on learning predictions and prior beliefs from prior knowledge.\n\n3. **Model Parameterization**:\n   - Generalized Notation Versus Bayesian Inference methods use learned beliefs to represent conditional probability distributions for each observation/action pair as well as previous states, whereas GNN-like models only support learned belief representations (based solely on prior probabilities of observed observations) and prior knowledge from policy networks. In contrast with these generalizations, GNN-like methods allow one to generate parameters based on model information that is derived through learning beliefs used in policies for prediction predictions across actions from previous actions or predicted future actions modeled using the probability distribution over observable states in the policy networks as well as forward predictions of outcomes if an action choice was taken along a path.\n\nGiven your question about parameterization, I have provided a concise summary overview of GNN-like methods that use learned beliefs to represent conditional probabilities for observation/action pairs and prior knowledge associated with each actions choices across actions:\n\n1. **Generalized Notation Versus Bayesian Inference**:\n   - Generalized Notations allow individual components from prior distributions or probability distributions represented by individual variables in the policy networks (policy matrices) as well as predictions of future observations to be used for prediction, whereas GNN-like methods only support learned beliefs based on prior probabilities over actions and their corresponding previous states.\n\n2. **Generalized Notation Versus Bayesian Inference Methods**:\n   - Generalized Notations are based solely on the probability distributions represented by individual variables in the policy networks (policy matrices) or predictions of future observations modeled using a probabilistic graphical model/network, whereas GNN-like methods allow individuals to generate parameters and parameters derived from prior probabilities across actions while predicting outcomes based on learned beliefs.",
        "analyze_structure": "Here is a detailed analysis of the model implementation for the Active Inference POMDP agent:\n\n**Graph Structure:**\nThe graph structures used to represent the input and output data are:\n\n1. **Variable Analysis**:\n    - Each observation variable has 20 variables, each labeled as either \"observation\", \"action\", or \"policy\". These are stored in a `variable_set` structure.\n    - The number of variables is determined by the number of hidden states (6) and actions (3).\n\n2. **Matrix Analysis**:\n   - Each time step is represented via an 8-dimensional matrix containing two columns for each variable, corresponding to the action selected during the iteration process ($t=1$):\n    - The action vector $\\mathbf{A}=(\\mathbf{\\theta},\\mathbf{\\vbm})$, where $\\mathbf{\\theta}$ are random steps chosen from the policy prior (Habit), and $(\\vbm)$ is the probability of moving to a new state. Each row represents one observation, with each column representing one action ($action$.\n\n3. **Connection Patterns**:\n    - There are two types of connections between variables:\n     - **Forward Connection**: The network connection from an observed action (action) to its predicted output variable $u(x_i)$ is a directed edge. This type connects the input $\\mathbf{A}=(\\mathbf{\\theta}, \\vbm)$, and gives the probability distribution over the values of $x_1$ for action actions $(a, b)$. The forward connection propagates $(\\mathbf{z}_u^* = (\\mathbf{\\theta}^\\top_{action}(d(a), a))^{-t}$ across two new states.\n    - **Backward Connection**: The network connection from an observed observation ($x_i$) to its predicted output variable $y_o$. This type connects the input $\\vbm$ (probability distribution over actions) and its corresponding value at the next state, giving the conditional probability of moving to a new state $(a, b)$. The backward connection propagates $(\\mathbf{z}_x^* = (\\mathbf{\\theta}^\\top_{observation}(d(a), a))^{-t}$ across one observation from action actions ($action$.\n\n**Mathematical Structure:**\n\n1. **Matrix Analysis**:\n   - Each time step is represented via an 8-dimensional matrix containing two columns for each variable, corresponding to the action selected during the iteration process ($i=1$). For example:\n    $(\\mathbf{\\theta}^*_2 = (\\mathbf{a}, \\mathbf{z})$.\n\n2. **Variable Analysis**:\n   - Each observation is represented via an 8-dimensional vector containing two columns for each variable, corresponding to the action selected during the iteration process ($i=1$).\n    $(\\vbm)^t$ is a directed edge with its first parameter $(x_1)$ and second parameter $(d(a))$. The forward connection propagates $\\mathbf{z}_u^* = (\\mathbf{\\theta}^\\top_{action}(d(a), d(a)))^{-i}$ across two new states.\n   - Each observation is represented via an 8-dimensional vector containing the predicted output variable $(y_o)^t$.\n    $(\\vbm)^T$ and $\\mathbf{z}_x^* = (\\mathbf{\\theta}^\\top_{observation}(d(a), d(a)))^{-i}$ across one observation.\n\n3. **Connection Patterns**:\n   - Each time step is represented via an 8-dimensional matrix containing two columns for each variable, corresponding to the action selected during the iteration process ($i=1$).\n    $(\\vbm)^t$, etc., represent forward and backward connection connections between a sequence of input observations $(x_i)$.\n\n4. **Symmetries/Special Properties**:\n   - The graph structure is composed of 28 nodes representing variables (both fixed and variable), where each node has two neighboring nodes for its output value ($u(a)$). Each node can be connected to the previous or next node by a directed edge, and connected backwards. This corresponds to the idea that one input observation $x_1$ gives rise to multiple outputs $\\mathbf{z}_x^*$.\n   - There are two types of connections between variables:\n     - **Forward Connection**: The network connection from an observed action ($a) $to its predicted output variable $(y_o)^t)$ is a directed edge. This type connects the input vector (input observation $x_1$), followed by all nodes connected to the same input, and then returns to the original node for each subsequent action ($action$, represented as the value of the current state-observation sequence). The forward connection propagates $(\\mathbf{z}_u^* = (\\vbm)^t_{obs}$ across multiple observations $(x_1$), followed by all nodes connected to the same input, and then returns to itself.\n     - **Backward Connection**: The network connection from an observed observation ($a) $to its predicted output variable $(y_o)^T)$ is a directed edge. This type connects the input vector (input observation $x_1$), followed by all nodes connected to the same observable, and then returns back to itself after it has propagated through all observations before returning forward again. The backward connection propagates $(\\mathbf{z}_y^* = (\\vbm)^T_{obs}$ across one observation $(a$, represented as the value of the current state-observation sequence).\n   - There is a duality between these two types of connections, i.e., each input variable connects to all corresponding observations. This corresponds to the idea that it is possible for an action $u(x_1)$ to move from one observable observation ($x_i$) in one step (forward connected), and also to return back to itself after moving through multiple observations $(a, \\vbm)^T$.",
        "extract_parameters": "Your comprehensive list provides a clear overview of the information captured in your GNN model and its components:\n\nThis covers the main pieces:\n\n1. **Model Matrices**: `A`, `B`, `C`, `D` are matrices representing the model architecture, decision rules, policy distribution, habit preferences, and prior distributions over observed data and actions.\n2. **Pseudo-supervised learning protocols (`GNNVersionAndFlags`)**: `Gnn` is a function that implements Bayesian inference based on Active Inference principles, but it requires parameters to be defined beforehand (in this case, in the signature). The parameter settings depend on the implementation and can be easily adjusted.\n3. **Parameter evaluation metrics**: You mentioned `Dynamic`, which measures changes of the model during learning/training processes. It depends on the choice of update rate, change-rate protocol ($\\mathcal{M}_{g}) (default to ${\\bf G}$), initial state size (`n_states`).\n4. **Model evaluation metrics**: You mentioned `Discrete Time Horizon`, which is a feature type that measures actions executed during time in the model. It depends on the choice of update rate, action selection parameters ($\\mathcal{M}_{a})$) and other parameter settings based on the implementation (in this case, in the signature).\n5. **System Type**: You also mentioned `Dynamic` for inference and `Discrete Time Horizon` for prediction time horizons. These are different feature types but can be useful depending on your needs or data sources (like simulation scenarios).\n6. **Integration with other models**: You mentioned `Active Inference POMDP Agent`, which is a meta-model based on the model representations and their corresponding parameters (`A`), `Generative Model`(s) for GNN, etc., respectively. You might consider different interfaces between these models to adapt them towards specific modeling scenarios or use cases (like simulation simulations or inference backends).\n6. **Validation**: Your validation metrics are based on evaluation of the model in a realistic scenario (`Scenario`) and comparison with other variants that achieve similar performance for a given parameter set ($\\mathcal{M}_{g} \\in [0,1]$), while also accounting for data quality issues or any biases (bias) introduced by your implementation.",
        "practical_applications": "Let's analyze the given code snippet for Active Inference POMDP agent:\n\n1. **Classification of observations**: This line calculates the likelihood matrix based on a set of observation values. It finds two classes from the input array `A`. Then it checks if there is any class that can be classified as \"observation\" by comparing each row with the next row and its column, starting from the last row using boolean indexing (`i!= 0`, `j<= num_obs`).\n\n2. **State Transition Matrix**: This line computes the state transition matrix based on the input array `B`. It checks if there is any action that can be chosen as \"observation\" by comparing each row with the next row and its column, starting from the last row using boolean indexing (`i!= 0`, `j<= num_obs`).\n\n3. **Policy Vector**: The current policy vector obtained through a sequence of transition matrices and state transitions is stored in `D`. However, it is not initialized directly by applying the prior matrix. Instead, this line initializes the current parameterizations (`A`) with the previous ones. Then it applies an action to each slice using the transition matrix as input from the action vector.\n\n4. **Action Vector**: The action vector computed in step 3 is stored in `G`.\n\n5. **Estimated Free Energy (FHE):** Finally, this line computes the FHE by updating all possible values of the belief and belief parameters based on the current beliefs. It checks if there is any belief that can be chosen as \"observation\" using a boolean indexing (`i!= 0`) from `A`. Then it applies an action to each slice using its corresponding prior vector obtained in step 3 and their respective previous beliefs (in this case, the information from the state transition matrix). The goal of inference is to find out if there are any actions that can be chosen as \"observation\" based on predictions.\n\n6. **Benchmarking**: Finally, we compare the proposed model with existing active inference models like FNN and POMDPP (Generalized Notation Notation) models for classification accuracy and performance estimation. The evaluation metrics depend on several parameters of the implementation:\n   - **Accuracy**: It assesses how well the current framework fits into all possible predictions.\n   - **Performance**: It examines whether the proposed model achieves a better fit to any prediction pattern that can be validated in practice or research applications. In order for inference accuracy, performance is evaluated using evaluation metrics like loss and error rate.",
        "technical_description": "This section contains the signature for the Advanced Neural Network (ANN) version of Active Inference POMDP agent model. The signature is described in more detail below:\n```python\n@staticmethod @classmethod\ndef BaseClass(base_classes=[]):\n    super().__init__()\n\n    class Action(BaseA):\n        pass\n\n        def __init__(self, state=None, actions=np.empty((len(state),)), *args):\n            BaseA.__init__(*args)\n\n            self._actions = actions[:]\n\n    class Policy(BaseA):\n        pass\n\n        def __init__(self):\n           super().__init__()\n\n   def initialize_hidden_states(*args):\n     def forward(x: np.ndarray, x2: np.ndarray)=\n      state=x[len([i for i in range(num_hidden_states)]),0]\n\n                 # Initialize hidden states\n                   # = prev_history+prev_obs\n           self._actions,_next=_infer_histo()\n             # Forward pass\n                  #  # forward step\n       self.forward: callable\n      def return_state(*args):\n          state=x[len([i for i in range(num_hidden_states)]),0]\n\n                 # Backward pass\n                   # backward step\n           #   ----------------------------------------\n              if isinstance(prev_,np.ndarray) and\n                  is_supervised('Forward'):\n                    self._forward = (base_classes$nextstep)(x, x2, *args)\n                else: \n                    self._forward = base_classes$currentframe(*x**3)**0\n                    \n        @staticmethod\n    return policy\n```\nThe signature defines the action, state and hidden states of the POMDP agent. The `__init__` method is used to initialize all parameters after activation.",
        "nontechnical_description": "Here's a comprehensive analysis on how to implement the Active Inference POMDP agent in C++:\n\n1. **Initialization**:\n   - Initialize actions and parameters with predefined initial settings (`[0]`, `[3],` etc.).\n- Initialize hidden states, action distributions for each policy (`\u03c0`).\n\n2. **Optimization**\n  - Use backpropagation to update state probability matrix (P) based on the observed data (`A`) from left to right and vice versa (`B` and `C`, respectively), followed by gradient descent with a learning rate of 10-fold, and/or using a learning rate scheduler.\n   - Use GNN annotations to define an action space for each state.\n\n3. **Optimization**\n  - Use gradient descent with forward direction (gradient descent) as in your implementation:\n    - Initialize values of hidden states (`H[\u03c0]`) based on actions (`b`). Then use iterated backpropagation using `GNNAnnotations`:\n    - The output probability for each state is computed by multiplying the previous input state and action probabilities.\n    - Use forward direction to compute final state distribution (P), next state distribution, and so forth, followed by gradient descent with a learning rate of 10-fold, until stopping in any case).\n\n4. **Model estimation**\n  - Use POMDP estimation methods based on the learned actions (`A`) from left to right:\n    - Expected free energy (F) updates for each state according to the current probability distribution and policy prior.\n    - Forward direction forward momentum update with a learning rate of 10-fold, stopping in any case.\n\n5. **Model inference**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state based on the last learned action probabilities and values (with respect to the previous input state).\n\n    - Initialize the initial action distribution `(1,0)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next action is chosen or an incorrect choice takes place.\n\n6. **Model estimation**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state according to the last learned policy probabilities and values (with respect to the previous input state).\n\n    - Initialize the initial action distribution `(0,1)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next action is chosen or an incorrect choice takes place.\n\n7. **Model inference**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state according to the last learned policy probabilities and values (with respect to the previous input state).\n\n    - Initialize the action distribution `(0,1)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next actions are chosen or an incorrect choice takes place.\n\n8. **Model estimation**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state according to the last learned policy probabilities and values (with respect to the previous input state).\n\n    - Initialize the action distribution `(0,1)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next actions are chosen or an incorrect choice takes place.\n\n9. **Model estimation**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state according to the last learned policy probabilities and values (with respect to the previous input state).\n\n    - Initialize the action distribution `(0,1)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next actions are chosen or an incorrect choice takes place.\n\n10. **Model estimation**\n  - Use GNN annotations based on actions (`B`) from left to right:\n    - Initialization and step updates for each state according to the last learned policy probabilities and values (with respect to the previous input state).\n\n    - Initialize the action distribution `(0,1)`. Then use forward direction forward momentum update with a learning rate of 10-fold. Stop in any case if no next actions are chosen or an incorrect choice takes place.\n\nPlease note that these steps can be customized by specifying different types of annotations (e.g., action probabilities), and/or using more efficient algorithms for initialization and step updates (more on this later).\n\nNote: This implementation will not perform forward direction iteration in case the actions are not well-defined and do not follow a fixed path between states (see section 10.5 of ). It only executes forward direction forward momentum updates with a learning rate of 10-fold, stopping in any case if no next action is chosen or an incorrect choice takes place.\n\nTo get the current state distribution for each observation as well as future actions based on it (which will not be directly correlated to previous states), you can use the `GNNAnnotations()` class from the C++ Standard Library.",
        "runtime_behavior": "You've already provided the structure of your code and documentation. Here's a revised version with some minor edits for clarity and readability:\n\n```python\nimport smolLM as sml\n\n# Define the model parameters\nnum_hidden_states = 3\nactive_inference_paraments([\n    ('LikelihoodMatrix',\n        ([\n            ([\n                (\"InitialState\",\n                    [[\n                        (\"observation\",\n                            [\n                                (\"state\", \"0\"),\n                                    (\"action\"=True)\n                  ],\n                       # The policy is fully controllable via 1 discrete actions.\n                   (\"policy\")),\n          ])\n       ]),\n      (\n            3,\n           {\"Probabilities\":[[\n                ([\n                  ('Location',\n                     [\"observations\"]),\n                    [\"actions\"]\n                 ]]\n            ]])\n        ),\n  )\n], [\"History\"], [])\n```"
      }
    }
  ],
  "model_insights": [
    {
      "model_complexity": "high",
      "recommendations": [
        "Consider breaking down into smaller modules",
        "High variable count - consider grouping related variables",
        "Consider breaking down into smaller modules"
      ],
      "strengths": [],
      "weaknesses": [
        "Complex model may be difficult to understand",
        "No connections defined"
      ],
      "generation_timestamp": "2026-01-07T06:30:31.561048"
    }
  ],
  "code_suggestions": [
    {
      "optimizations": [],
      "improvements": [
        "Many variables lack type annotations - consider adding explicit types"
      ],
      "best_practices": [
        "Use descriptive variable names",
        "Group related variables together"
      ],
      "generation_timestamp": "2026-01-07T06:30:31.561084"
    }
  ],
  "documentation_generated": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "model_overview": "\n# actinf_pomdp_agent.md Model Documentation\n\nThis GNN model contains 72 variables and 0 connections.\n\n## Model Structure\n- **Variables**: 72 defined\n- **Connections**: 0 defined\n- **Complexity**: 72 total elements\n\n## Key Components\n",
      "variable_documentation": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1,
          "description": "Variable defined at line 1"
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2,
          "description": "Variable defined at line 2"
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68,
          "description": "Variable defined at line 68"
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75,
          "description": "Variable defined at line 75"
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82,
          "description": "Variable defined at line 82"
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85,
          "description": "Variable defined at line 85"
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88,
          "description": "Variable defined at line 88"
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121,
          "description": "Variable defined at line 121"
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69,
          "description": "Variable defined at line 69"
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76,
          "description": "Variable defined at line 76"
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83,
          "description": "Variable defined at line 83"
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86,
          "description": "Variable defined at line 86"
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89,
          "description": "Variable defined at line 89"
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99,
          "description": "Variable defined at line 99"
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102,
          "description": "Variable defined at line 102"
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105,
          "description": "Variable defined at line 105"
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106,
          "description": "Variable defined at line 106"
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107,
          "description": "Variable defined at line 107"
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108,
          "description": "Variable defined at line 108"
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109,
          "description": "Variable defined at line 109"
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110,
          "description": "Variable defined at line 110"
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111,
          "description": "Variable defined at line 111"
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112,
          "description": "Variable defined at line 112"
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113,
          "description": "Variable defined at line 113"
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114,
          "description": "Variable defined at line 114"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115,
          "description": "Variable defined at line 115"
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116,
          "description": "Variable defined at line 116"
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117,
          "description": "Variable defined at line 117"
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121,
          "description": "Variable defined at line 121"
        }
      ],
      "connection_documentation": [],
      "usage_examples": [
        {
          "title": "Variable Usage",
          "description": "Example variables: Example, Version, matrix",
          "code": "# Example variable usage\nExample = ...\nVersion = ...\nmatrix = ..."
        }
      ],
      "generation_timestamp": "2026-01-07T06:30:31.561102"
    }
  ]
}