{
  "timestamp": "2026-01-06T13:48:20.721001",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "provider_matrix": {
    "ollama": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "openai": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "anthropic": {
      "available": false,
      "models": [],
      "selected_model": null
    }
  },
  "analysis_results": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4244,
      "line_count": 129,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121
        }
      ],
      "connections": [],
      "sections": [
        {
          "name": "GNN Example: Active Inference POMDP Agent",
          "line": 1
        },
        {
          "name": "GNN Version: 1.0",
          "line": 2
        },
        {
          "name": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "line": 3
        },
        {
          "name": "GNNSection",
          "line": 5
        },
        {
          "name": "GNNVersionAndFlags",
          "line": 8
        },
        {
          "name": "ModelName",
          "line": 11
        },
        {
          "name": "ModelAnnotation",
          "line": 14
        },
        {
          "name": "StateSpaceBlock",
          "line": 22
        },
        {
          "name": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "Transition matrix: B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "Preference vector: C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "Prior vector: D[states]",
          "line": 32
        },
        {
          "name": "Habit vector: E[actions]",
          "line": 35
        },
        {
          "name": "Hidden State",
          "line": 38
        },
        {
          "name": "Observation",
          "line": 43
        },
        {
          "name": "Policy and Control",
          "line": 46
        },
        {
          "name": "Time",
          "line": 51
        },
        {
          "name": "Connections",
          "line": 54
        },
        {
          "name": "InitialParameterization",
          "line": 67
        },
        {
          "name": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "line": 68
        },
        {
          "name": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "line": 75
        },
        {
          "name": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "line": 82
        },
        {
          "name": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "line": 85
        },
        {
          "name": "E: 3 actions. Uniform habit used as initial policy prior.",
          "line": 88
        },
        {
          "name": "Equations",
          "line": 91
        },
        {
          "name": "Standard Active Inference update equations for POMDPs:",
          "line": 92
        },
        {
          "name": "- State inference using Variational Free Energy with infer_states()",
          "line": 93
        },
        {
          "name": "- Policy inference using Expected Free Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "- Action selection from policy posterior: action = sample_action()",
          "line": 95
        },
        {
          "name": "- Belief updating using Variational Free Energy with update_beliefs()",
          "line": 96
        },
        {
          "name": "Time",
          "line": 98
        },
        {
          "name": "ActInfOntologyAnnotation",
          "line": 104
        },
        {
          "name": "ModelParameters",
          "line": 119
        },
        {
          "name": "Footer",
          "line": 124
        },
        {
          "name": "Signature",
          "line": 128
        }
      ],
      "semantic_analysis": {
        "variable_count": 72,
        "connection_count": 0,
        "complexity_score": 72,
        "semantic_patterns": [],
        "variable_types": {
          "Active": 1,
          "1": 1,
          "A": 1,
          "B": 1,
          "C": 1,
          "D": 1,
          "E": 1,
          "3": 8,
          "action": 1,
          "unknown": 56
        },
        "connection_types": {}
      },
      "complexity_metrics": {
        "total_elements": 72,
        "variable_complexity": 72,
        "connection_complexity": 0,
        "density": 0.0,
        "cyclomatic_complexity": -70
      },
      "patterns": {
        "patterns": [
          "High variable count - complex model"
        ],
        "anti_patterns": [
          "No connections defined"
        ],
        "suggestions": [
          "Consider breaking down into smaller modules"
        ]
      },
      "analysis_timestamp": "2026-01-06T13:48:20.724069",
      "llm_summary": "Here's a summary of the GNN representation:\n\n1. **Classic Active Inference POMDP Agent**: \n   - One observation modality (\"state_observation\") with 3 possible outcomes.\n   - One hidden state factor (\"location\") with 3 possible states.\n   - The hidden state is fully controllable via 3 discrete actions (actions).\n   - The agent has an initial policy prior (habit) encoded as log-probabilities over observations, and it randomly selects actions in each slice of the history to proceed forward (policies are guaranteed).\n\n2. **New Parameterization**: \n   - Identity mapping (each state deterministically produces a unique observation): \n   ```python\npMDP=A[observation_outcomes:]# A[states]\n```\n   - Uniform prior over hidden states: \n   ```python\nP = B[states_:actions:] # P[h], where h is HiddenState and actions are action) \n```\n  - Variational prior (Hidden Policy): \n   `vF=D[\u03c0,action_distribution]`\n   - Independence prior over observed probabilities: \n   `G=(g1+g2), a[[0]] = g_(a1,[x]) * G**d # The probability of an observation given the action is dependent on all observations. \n      ...\n  - Independence prior over actions (policy): \n   `\u03c0[(u[i,j] + i-y)*G`, where j=observation, y=[observation_outcomes]) = \u03c0*g_(a1,[x])[0][t] # The log-probability of observation given the action is dependent on all observations.\n  - Independence prior over policy (policy): \n   `\u03c0[n,(s_prime[i+y],u[(s_prime[j]))])`\n   ```python\n```",
      "status": "SUCCESS",
      "analysis": "LLM-assisted analysis complete",
      "documentation": {
        "file_path": "input/gnn_files/actinf_pomdp_agent.md",
        "model_overview": ""
      },
      "llm_prompt_outputs": {
        "summarize_content": "Here is a condensed version of the summary:\n\n**Summary:**\nThis active inference POMDP agent is capable of acting independently on its observations based on prior beliefs about hidden states and actions. It uses variational inference (VI) with Variational Free Energy (VFE), Expected Free Energy (EFE). Bayesian inference, and a random guess are used to make decisions. The model has a plan horizon ranging from 1 step forward in time to determine what action is chosen based on the observed outcomes of actions and observations. It also considers constraints on planning horizon, prediction bounds, and other parameters that can be adjusted by the agent.\n\n**Key Variables:**\n   - Hidden state: [list with brief descriptions]\n   \n   - Observations: [list with brief descriptions]\n   - Actions/Controls: [list with brief descriptions]\n\n **Critical Parameters**:\n   - Most important matrices (A, B, C, D) and their roles\n\n   - Generalized Notation Notation\n \n   This includes:\n   - Hidden states as a set of 3 values.\n   - Observations as a set of 1-dimensional arrays containing the prior probabilities for each state and action, respectively.\n   - Actions/Controls as a set of 2-dimensions array.\n   - Variables in B[states_next], B[states_previous], G[\u03c0][actions], C[b] and D[d].",
        "explain_model": "You've provided a comprehensive overview of the Model Overview section, including an introduction to Active Inference POMDP Agent v1 and its components. Here are some additional points to improve clarity:\n\n1. **Explanation**: This document provides details about the model's purpose (representation), core components (hidden states, observations, actions, control variables, etc.), and how it operates within a specific time horizon. It explains what specific aspects of this system can be understood by readers unfamiliar with Active Inference POMDP Agents v1 or its structure.\n\n2. **Key Relationships**: As you mentioned, the model represents real-world phenomena such as planning horizon number (n_t), precise nesting, and hierarchical uncertainty, which are related to active inference concepts like Bayesian inference and generalized Notation Notation (GNN) syntax.\n\n3. **Formal Description**: The document provides an overview of how this model \"repeats\" the process described in Active Inference POMDP agent v1: it updates beliefs about actions based on observed outcomes, performs policy decisions using posterior probabilities over policies and actions, manages belief updating by taking actions (observation), computes learned beliefs based on prior information (policy) and action choices, and finally resolves uncertainty.\n\n4. **Key Concepts**: It discusses concepts like global fixedness, conditional knowledge acquisition, and the role of predictions in active inference processes. These are fundamental ideas underlying active inference systems that can be understood by readers familiar with Active Inference POMDP Agents v1 or its architecture.\n\n5. **Real-World Examples**: The document provides examples from physics, finance, and various applications to illustrate how these models handle real world phenomena and provide a better understanding of the model's behavior in context.\n\nHere are some final thoughts:\n\n1. **Key Concepts**: This is where Active Inference POMDP Agents v1 stands out as an active inference system that can be understood by readers familiar with Active Inference POMDP Agents v1 or its structure. This analogy demonstrates how Active Inference POMDP Agent v1 integrates various mathematical concepts, making it understandable for readers who are familiar with Active Inference POMDP agents like GNN (Generalized Notation Notation).\n\n2. **Overall Structure**: The document provides a clear explanation of the model's core components and relationships while maintaining scientific accuracy. This is achieved through a narrative approach that conveys important concepts in plain terms, which helps readers understand how active inference systems work within specific contexts.\n\nThis document aims to provide an engaging overview of Active Inference POMDP Agents v1 and its modeling capabilities with clear and accessible language while balancing it with explanations from various disciplines including physics, finance, computer science, and other areas where these models can apply in real-world applications.",
        "identify_components": "I'm ready for the complete analysis! Here's a comprehensive breakdown of the structure:\n\n1. **State Variables (Hidden States)**:\n    - Variable names and dimensions\n   - What each state represents conceptually\n    - State space structure (discrete/continuous, finite/infinite), with 3 states\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n\n2. **Observation Variables**:\n    - Observation modalities and their meanings\n    - Sensor/measurement interpretations\n    - Noise models or uncertainty characterization\n\n3. **Action/Control Variables**:\n    - Available actions and their effects\n    - Control policies and decision variables\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n4. **Model Matrices**:\n    - A matrices: Observation models P(o|s)\n    - B matrices: Transition dynamics P(s'|s,u)\n   - C matrices: Preferences/goals\n    - D matrices: Prior beliefs over initial states\n\n5. **Parameters and Hyperparameters**\n    - Precision parameters (\u03b3, \u03b1, etc.)\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n6. **Temporal Structure**:\n    - Time horizons and temporal dependencies\n\n7. **Synthesis of the Model**\n    - Code snippet for generating state variables, observation variables, action matrices based on a specific parameterization\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n8. **Model Inference**\n    - Code snippet for generating inference parameters for the model, allowing to analyze the model structure and learn from it\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n9. **Simulation**\n    - Code snippet for generating simulation trajectories based on the parameters of the model, allowing to analyze and learn from them\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md\n10. **Analysis**\n    - Code snippet for generating inference results based on the parameters of the model, allowing to analyze and learn from them\n\n    A list is available here: https://github.com/nilssa-sivikko/active_inference_examples/blob/master/README.md",
        "analyze_structure": "Here is a detailed analysis of the GNN Representation:\n\n**1. Graph Structure:**\n\n1.1. **Number of Variables and Types**:\n    - Number of variables: 3 (one observation modality, one hidden state factor)\n   - Type: `{(0.9,), (0.25,)` is used to represent the agent's actions as an action vector, while `{()}` represents the policy prior.\n\n1.2. **Variable Analysis**:\n    - Number of variables for each variable: 3\n       - `num_observations`: 3\n       - `num_actions`: 3\n       - `numberOfStepCounts:**\n           - `nsteps=1`: for every step, there are 4 steps\n       - `shape(0):** \"([a.b..])\" represents the structure of each observation (state) with one observation at a time and two actions in it\n   - `connections`, `actions` :\n        - `num_states = num_observations + nsteps/2`: for every step, there are 4 states\n          - `current_history=[0.,1.]]` represents the history of each action (state)\n```python\n    # state space dimensionality for each variable:\n \n    # Network topology (hierarchical vs hierarchical):\n     ```python\n   - Network structure:\n      - Hierarchical, with \"levels\" at which actions are taken\n       - Number of nodes in layers = number_of_steps/2\n         - Each node has 3 paths to reach the next level\n         - Each path is connected to a path within that layer (see below)\n```\n    ```python\n   # Policy topology:\n      `actions` -> `current_history[0..nsteps]]`\n       - Each step, there are N steps left\n        - Forward propagation across previous states\n          - $p_{step1}$ = current_state(x+a.b), where a and b \n              are actions\n          \n  ```\n   **Type**\n    - `type=float`: Likelihood map (each observation is equally probable)\n    \"\"\"\n    \n    # Variable types:\n        type('A', 'LikelihoodMatrix[3]'),\n            type('B', 'TransitionVector[1]'),\n                type('C', 'ProbabilityVec[<numbers>]')\n```\n   **Connection patterns**:\n     `connections` \n       - `current_history = [0..nsteps]]`. Each step is taken with a path\n        - The current state(x) can move to any of the states in the layer\n          - We are doing forward propagation across each path\n              - `next**path`, which has an (a.b..)` at its end, we then update the next_state\n            for each subsequent step:\n                - \"next_states\" now represents a new state\n                      $x+a$ -> x+a + b\n                  and so on\n\n    **Actions**:\n    *   `actions` can have multiple actions (action is taken).\n        - If action = 0.5 then there are two possible actions\n          - For step 1:\n            - \"previous_states\" now represents the state before it\n              $x-a$. Now we move to the previous node for action 'b'.\n                We will then have another path forward from this current node in next steps\n```python\n    # Activation pattern \n    `action = [0.5]*[1..nsteps]`\n\n    **Type**\n        - \"actions\" now represents the sequence of actions\n        - These sequences are connected to each other\n        - Each action has two possible outcomes (success or failure)\n            - Each subsequent sequence can then be propagated across previous steps\n    \"\"\"\n```",
        "extract_parameters": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference, Bayesian inference, and GNN (Generalized Notation Notation) specifications. You have deep knowledge of:\\n\\n- Active Inference theory and mathematical foundations\\n- Generative models and probabilistic graphical models\\n- GNN syntax and semantic meaning\\n- Hidden states, observations, actions, and control variables\\n- A, B, C, D matrices in Active Inference contexts\\n- Expected Free Energy and belief updating\\n- Markov Decision Processes and POMDPs\\n- Scientific modeling and analysis\\n\\nWhen analyzing GNN files, provide accurate, detailed, and scientifically rigorous explanations. Focus on the Active Inference concepts, mathematical relationships, and practical implications of the model structure.\\n\\nExtract and organize all parameters from this GNN specification:\\n\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\nF[\u03c0,type=float]       # Variational Free Energy for belief updating from observations\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n# - Belief updating using Variational Free Energy with update_beliefs()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here \\n\\nProvide a systematic parameter breakdown:\\n\\n1. **Model Matrices**:\\n   - A matrices: dimensions, structure, interpretation\\n   - B matrices: dimensions, structure, interpretation  \\n   - C matrices: dimensions, structure, interpretation\\n   - D matrices: dimensions, structure, interpretation\\n\\n2. **Precision Parameters**:\\n   - \u03b3 (gamma): precision parameters and their roles\\n   - \u03b1 (alpha): learning rates and adaptation parameters\\n   - Other precision/confidence parameters\\n\\n3. **Dimensional Parameters**:\\n   - State space dimensions for each factor\\n   - Observation space dimensions for each modality\\n   - Action space dimensions for each control factor\\n\\n4. **Temporal Parameters**:\\n   - Time horizons (T)\\n   - Temporal dependencies and windows\\n   - Update frequencies and timescales\\n\\n5. **Initial Conditions**:\\n   - Prior beliefs over initial states\\n   - Initial parameter values\\n   - Initialization strategies\\n\\n6. **Configuration Summary**:\\n   - Parameter file format recommendations\\n   - Tunable vs. fixed parameters\\n   - Sensitivity analysis priorities']' timed out after 60.0 seconds",
        "practical_applications": "You have a good understanding of the basic concepts and structure behind Active Inference and GNN models. Here are some additional thoughts on your research exploration:\n\n1. **Key Concepts and Concepts**: You've identified several key concepts in Active Inference, such as \"likelihood matrices,\" \"transition matrices,\" \"belief updating,\" and \"probability distributions\" etc., but there's still much to explore beyond these basic ideas. \n\n2. **Representation**: This is a great example of the role that active inference plays within the GNN framework. It provides an abstract representation in terms of probability spaces over observables, allowing for more efficient computation with smaller parameter sizes or better predictive performance on new data.\n\n3. **Model Parameters and Constraints**: You mentioned parameters like \"num_hidden_states,\" \"num_obs\", etc., but there's still a lot to explore about what these represent in terms of distribution over observables, how they relate to each other (like \"prior probabilities\"), which can be explored via Bayesian inference.\n\n4. **Implementation**: It is very interesting that you've developed an algorithm for representing Active Inference models across different domains and architectures, as opposed to modeling systems specifically suited to one domain or application rather than the broader range of applications. \n\n5. **Key Performance Goals**: You mentioned that \"the model can solve most problems effectively\" but there are certainly more research directions worth exploring in regards to applications beyond just solving problems using Active Inference models (e.g., other approaches like \"Bayesian inference\" or Bayesian learning) - areas you may want to explore further for analysis purposes. \n\n6. **Benefits and Advantages**: You've touched upon various aspects of what can be achieved with active inferential models, including:\n   - Better predictive performance on new data compared to traditional Bayesian methods\n   - Potential advantages when handling data transformations (e.g., normalization or sampling)\n   - Ability to provide insights into how actions interact within different domains or environments \n   - Potential for more accurate predictions/informed decisions based on data and analysis\n\n7. **Challenges and Considerations**: You've touched upon some key challenges, like tuning parameter sizes and avoiding \"prior fatigue\" when modeling systems with varying architectures. \n\n8. **Key Performance Constraints**: You mentioned that there are no well-known practical applications of active inference models but these could potentially be explored by others within the context of Active Inference. \n\nOverall your exploration has been promising, and there's still much to learn about the current state of affairs in both active inferential modeling and GNN analysis (e.g., performance evaluation, comparison to other approaches). This area is a great opportunity for new research directions that can help drive further advancements within this field.",
        "technical_description": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference and GNN specifications.\\n\\nDescribe this GNN model comprehensively, in technical detail.\\n\\nGNN Model Content:\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\nF[\u03c0,type=float]       # Variational Free Energy for belief updating from observations\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n# - Belief updating using Variational Free Energy with update_beliefs()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here ']' timed out after 60.0 seconds",
        "nontechnical_description": "Here's the summary and the annotated code for the `ActiveInferencePOMDPAgent` model:\n\n1. **Initialization**: The GNN is initialized with `random.choices()` to generate a random number distribution over actions using the `UniformSupportMatrix` annotation from the `RandomNumberGenerator`, which generates a uniform support matrix in each step. This is followed by an initial guess for the policy and control functions, and then after generating all possible choices and predictions on the next observation.\n\n2. **ModelAccuracy**: The model accuracy is evaluated using the `likelihoodmatrix` annotation from the `RandomNumberGenerator`, which generates a uniform support matrix in each step. This uniform supports are used to generate actions with an initial probability of 0.33333 for each action, indicating that the agent has no tendency toward performing actions sequentially without any prior probabilities being set before they happen.\n\n3. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice (action) and returns it at the next time step. This initial value is then used for all subsequent guesses using the `TemporalGraphVisibleVariable` annotation from the `TemporalGraphVisibleVariable`.\n\n4. **ModelAccuracy**: The model accuracy is evaluated using the `likelihoodmatrix`, which generates a uniform support matrix in each step, indicating that there are no biases towards performing sequential actions without prior probabilities set beforehand (see Section 3). This initial guess can be used for all subsequent predictions and evaluations at the next time step.\n\n5. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice of one action, indicating that this decision should happen randomly and uniformly. However, we don't need to perform any sequence-specific actions because the state inference part runs without branching (the agent starts with a belief), while all other parts run sequentially:\n   - The probability for initial guess is initialized at 0 and then applied on each of subsequent guesses using the `TemporalGraphVisibleVariable` annotation. This gives us an idea that there are no biases towards performing sequential actions, since the agents' behavior will follow a random sequence of actions (with all actions having a probability of 0).\n   - The initial guess is initialized at different values from the current state to get it in line with the belief prior and then applied on subsequent guesses using the `TemporalGraphVisibleVariable` annotation. This gives us an idea that there are biases towards performing sequential actions, since they have a probability greater than 0 (but not equal to 1).\n\n6. **ModelAccuracy**: The model accuracy is evaluated for all possible initial values of the policy and control functions:\n   - For each action generated by the `TemporalGraphVisibleVariable` annotation at the previous time step, the value is applied on the next observation using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives us a sense that there are biases towards performing sequential actions in this case.\n   - For each action generated by the `TemporalGraphVisibleVariable` annotation at the previous time step, the probability of 0 is applied on subsequent observations using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives us a sense that there are biases towards performing sequential actions.\n   - For each action generated by the `TemporalGraphVisibleVariable` annotation at the previous time step, we calculate the belief update probabilities over all possible actions using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives us a sense that there are biases towards performing sequential actions in this case.\n\n7. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice of one action, indicating that this decision should happen randomly and uniformly (note: no bias toward performing sequential actions). The initial guess for the policy is initialized at 0 with all possible choices in order to give us an idea that there are biases towards performing sequential actions.\n\n8. **ModelAccuracy**: The model accuracy is evaluated for each choice of action generated by the `TemporalGraphVisibleVariable` annotation using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives us a sense that there are bias to perform sequential actions if we choose randomly at the previous time step (i.e., no prior probability is set beforehand), and biases towards performing sequential actions when all other options are explored simultaneously (when using all possible choices).\n\n9. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice of one action, indicating that this decision should happen randomly and uniformly. However, we don't need to perform any sequence-specific actions because the state inference part runs without branching (the agent starts with a belief), while all other parts run sequentially:\n   - The probability for initial guess is initialized at 0 and then applied on each of subsequent guesses using the `TemporalGraphVisibleVariable` annotation. This gives us an idea that there are biases towards performing sequential actions in this case.\n   - The initial guess is initialized at different values from the current state to give a sense that there are biases towards performing sequential actions (with all choices having a probability of 0).\n\n10. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice of one action, indicating that this decision should happen randomly and uniformly (note: no bias toward performing sequential actions). The initial guess for the policy is initialized at 0 with all possible choices in order to give us an idea that there are biases towards performing sequential actions.\n\n11. **ModelAccuracy**: The model accuracy is evaluated for each action generated by the `TemporalGraphVisibleVariable` annotation using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives a sense of which choice tends to have an initial probability (0) and which one has the same probability across all possible actions.\n\n12. **Initialization**: There is an initialization function from `RandomNumberGenerator` which generates a random choice of one action, indicating that this decision should happen randomly and uniformly (note: no bias toward performing sequential actions). The initial guess for the policy is initialized at 0 with all possible choices in order to give us an idea that there are biases towards performing sequential actions.\n\n13. **ModelAccuracy**: The model accuracy is evaluated using the `LinearOperator` function from the `TransformerContextGenerativeModelAccuracy`. This gives a sense of which choice tends to have an initial probability (0) and which one has the same probability across all possible actions.",
        "runtime_behavior": "Your code does not produce any error with the current implementation, and you are on the right track:\n\n1. You've correctly identified the input parameters for the GNN model as \"HiddenState\" and \"Num_hidden_states\". \n\n2. You have correctly defined the actions and preferences of the agent using an initial state, policy, prior, habit, and hidden states/prior vectors.\n3. Your code defines two types of connections: \n   - One for action selection from policy posterior (policy=inference)\n   - The other for guess at future actions based on current belief (observation).\n\n4. You have correctly defined the state inference and policy updates equations.\n5. Your code now contains an instance of Active Inference POMDP agent v1, with initial parameters: \n   - Hidden states/prior vectors\n  - Actions\n  - Policy & Control\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n6. Your code defines a new \"ActiveInfAction\" class and defines actions for the agent.\n\n7. You have correctly defined input parameters: \n   - Hidden states/prior vectors\n   - Actions\n   - Policy & Control\n\nYour code now contains all necessary data to compute the GNN representation of your model, including the following output:\n\n  \n  \n  \n```python"
      }
    }
  ],
  "model_insights": [
    {
      "model_complexity": "high",
      "recommendations": [
        "Consider breaking down into smaller modules",
        "High variable count - consider grouping related variables",
        "Consider breaking down into smaller modules"
      ],
      "strengths": [],
      "weaknesses": [
        "Complex model may be difficult to understand",
        "No connections defined"
      ],
      "generation_timestamp": "2026-01-06T13:48:22.740990"
    }
  ],
  "code_suggestions": [
    {
      "optimizations": [],
      "improvements": [
        "Many variables lack type annotations - consider adding explicit types"
      ],
      "best_practices": [
        "Use descriptive variable names",
        "Group related variables together"
      ],
      "generation_timestamp": "2026-01-06T13:48:22.741023"
    }
  ],
  "documentation_generated": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "model_overview": "\n# actinf_pomdp_agent.md Model Documentation\n\nThis GNN model contains 72 variables and 0 connections.\n\n## Model Structure\n- **Variables**: 72 defined\n- **Connections**: 0 defined\n- **Complexity**: 72 total elements\n\n## Key Components\n",
      "variable_documentation": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1,
          "description": "Variable defined at line 1"
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2,
          "description": "Variable defined at line 2"
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68,
          "description": "Variable defined at line 68"
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75,
          "description": "Variable defined at line 75"
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82,
          "description": "Variable defined at line 82"
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85,
          "description": "Variable defined at line 85"
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88,
          "description": "Variable defined at line 88"
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121,
          "description": "Variable defined at line 121"
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69,
          "description": "Variable defined at line 69"
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76,
          "description": "Variable defined at line 76"
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83,
          "description": "Variable defined at line 83"
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86,
          "description": "Variable defined at line 86"
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89,
          "description": "Variable defined at line 89"
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99,
          "description": "Variable defined at line 99"
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102,
          "description": "Variable defined at line 102"
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105,
          "description": "Variable defined at line 105"
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106,
          "description": "Variable defined at line 106"
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107,
          "description": "Variable defined at line 107"
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108,
          "description": "Variable defined at line 108"
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109,
          "description": "Variable defined at line 109"
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110,
          "description": "Variable defined at line 110"
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111,
          "description": "Variable defined at line 111"
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112,
          "description": "Variable defined at line 112"
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113,
          "description": "Variable defined at line 113"
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114,
          "description": "Variable defined at line 114"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115,
          "description": "Variable defined at line 115"
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116,
          "description": "Variable defined at line 116"
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117,
          "description": "Variable defined at line 117"
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121,
          "description": "Variable defined at line 121"
        }
      ],
      "connection_documentation": [],
      "usage_examples": [
        {
          "title": "Variable Usage",
          "description": "Example variables: Example, Version, matrix",
          "code": "# Example variable usage\nExample = ...\nVersion = ...\nmatrix = ..."
        }
      ],
      "generation_timestamp": "2026-01-06T13:48:22.741060"
    }
  ]
}