{
  "timestamp": "2026-01-07T11:29:00.162817",
  "processed_files": 1,
  "success": true,
  "errors": [],
  "provider_matrix": {
    "ollama": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "openai": {
      "available": false,
      "models": [],
      "selected_model": null
    },
    "anthropic": {
      "available": false,
      "models": [],
      "selected_model": null
    }
  },
  "analysis_results": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "file_name": "actinf_pomdp_agent.md",
      "file_size": 4244,
      "line_count": 129,
      "variables": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121
        }
      ],
      "connections": [],
      "sections": [
        {
          "name": "GNN Example: Active Inference POMDP Agent",
          "line": 1
        },
        {
          "name": "GNN Version: 1.0",
          "line": 2
        },
        {
          "name": "This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
          "line": 3
        },
        {
          "name": "GNNSection",
          "line": 5
        },
        {
          "name": "GNNVersionAndFlags",
          "line": 8
        },
        {
          "name": "ModelName",
          "line": 11
        },
        {
          "name": "ModelAnnotation",
          "line": 14
        },
        {
          "name": "StateSpaceBlock",
          "line": 22
        },
        {
          "name": "Likelihood matrix: A[observation_outcomes, hidden_states]",
          "line": 23
        },
        {
          "name": "Transition matrix: B[states_next, states_previous, actions]",
          "line": 26
        },
        {
          "name": "Preference vector: C[observation_outcomes]",
          "line": 29
        },
        {
          "name": "Prior vector: D[states]",
          "line": 32
        },
        {
          "name": "Habit vector: E[actions]",
          "line": 35
        },
        {
          "name": "Hidden State",
          "line": 38
        },
        {
          "name": "Observation",
          "line": 43
        },
        {
          "name": "Policy and Control",
          "line": 46
        },
        {
          "name": "Time",
          "line": 51
        },
        {
          "name": "Connections",
          "line": 54
        },
        {
          "name": "InitialParameterization",
          "line": 67
        },
        {
          "name": "A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.",
          "line": 68
        },
        {
          "name": "B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.",
          "line": 75
        },
        {
          "name": "C: 3 observations. Preference in terms of log-probabilities over observations.",
          "line": 82
        },
        {
          "name": "D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.",
          "line": 85
        },
        {
          "name": "E: 3 actions. Uniform habit used as initial policy prior.",
          "line": 88
        },
        {
          "name": "Equations",
          "line": 91
        },
        {
          "name": "Standard Active Inference update equations for POMDPs:",
          "line": 92
        },
        {
          "name": "- State inference using Variational Free Energy with infer_states()",
          "line": 93
        },
        {
          "name": "- Policy inference using Expected Free Energy = with infer_policies()",
          "line": 94
        },
        {
          "name": "- Action selection from policy posterior: action = sample_action()",
          "line": 95
        },
        {
          "name": "- Belief updating using Variational Free Energy with update_beliefs()",
          "line": 96
        },
        {
          "name": "Time",
          "line": 98
        },
        {
          "name": "ActInfOntologyAnnotation",
          "line": 104
        },
        {
          "name": "ModelParameters",
          "line": 119
        },
        {
          "name": "Footer",
          "line": 124
        },
        {
          "name": "Signature",
          "line": 128
        }
      ],
      "semantic_analysis": {
        "variable_count": 72,
        "connection_count": 0,
        "complexity_score": 72,
        "semantic_patterns": [],
        "variable_types": {
          "Active": 1,
          "1": 1,
          "A": 1,
          "B": 1,
          "C": 1,
          "D": 1,
          "E": 1,
          "3": 8,
          "action": 1,
          "unknown": 56
        },
        "connection_types": {}
      },
      "complexity_metrics": {
        "total_elements": 72,
        "variable_complexity": 72,
        "connection_complexity": 0,
        "density": 0.0,
        "cyclomatic_complexity": -70
      },
      "patterns": {
        "patterns": [
          "High variable count - complex model"
        ],
        "anti_patterns": [
          "No connections defined"
        ],
        "suggestions": [
          "Consider breaking down into smaller modules"
        ]
      },
      "analysis_timestamp": "2026-01-07T11:29:00.170722",
      "llm_summary": "You are correct, the documentation provides an example implementation of Active Inference using the GNN model in Hugging Face:\n\n\n```python\nfrom hg_inference import *\nmodel = CryptoMeta('ActiveInference', 'GNN') # Implementations for different AI models.\n\n\n# Implementation details \ndef infer_states(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing observations and hidden states.\n\n     Returns:\n        Tensor[List[Tensor]] A list of observed states, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hid_states\n\n\ndef infer_policies(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing actions and predictions to be inferred from policy posterior.\n\n     Returns:\n        Tensor[List[Tensor]] A list of the next observed states, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hf_policy_prior(x)\n\n\ndef infer_beliefs(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing beliefs and predictions to be inferred from policy posterior.\n\n     Returns:\n        Tensor[List[Tensor]] A list of the next observed states, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hf_beliefs(x)\n\n\ndef infer_prior_state_actions(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing actions and predictions to be inferred from policy posterior.\n\n     Returns:\n        Tensor[List[Tensor]] A list of the next observed states, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hf_beliefs(x)\n\n\ndef infer_policy_probability(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing actions and predictions to be inferred from policy posterior.\n\n     Returns:\n        Tensor[List[Tensor]] A probability distribution over policies, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hf_policy_prior(x)\n\n\ndef infer_beliefs_forward(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing beliefs and predictions to be inferred from policy posterior.\n\n     Returns:\n        Tensor[List[Tensor]] A probability distribution over policies, with each state represented as a tensor\n    \n    Raises:\n        TypeError: If the input is not a list.\n    \"\"\"\n    return [observations] + hf_beliefs(x)\n\n\ndef infer_policy_probability_forward(x):\n    \"\"\"\n\n    Args:\n        x (list[Tensor]) A list containing actions and predictions to be inferred from policy posterior.\n\n     Returns:\n       Tensor[List[Tensor]] A probability distribution over policies, with each state represented as a tensor\n    \n    Raises:\n       TypeError: If the input is not a list.\n   ```",
      "status": "SUCCESS",
      "analysis": "LLM-assisted analysis complete",
      "documentation": {
        "file_path": "input/gnn_files/actinf_pomdp_agent.md",
        "model_overview": ""
      },
      "llm_prompt_outputs": {
        "summarize_content": "Your summary covers the essential information about a specific active inference POMDP agent that is described in the document. Here's a concise version:\n\n1. **Summary Overview**: This section provides a high-level overview of the model, including its key variables and key parameters.\n\n2. **Key Variables**:\n   - `hidden_states`: A list containing information about where the policy maps to (state=previous state+action).\n   - `observations`: A list containing information about actions taken.\n   - `actions/controls`: A list of all actions performed in each action space and their joint distribution over previous states, next states, etc.\n\n3. **Critical Parameters**: These are the variables that make up the model's behavior:\n\n   - **Maximal exploration path** (num_hidden_states): Determines how many hidden states to explore while exploring new territories.\n   - **Maximum number of actions per action** (`max_actions`): Sets a maximum amount of actions explored across all actions at each time step.\n\n   - **Initial policy** (`habit`): A distribution over actions used as the initial policy prior, allowing for forward and backward planning decisions based on observed actions.\n\n   - **Learning rate**: Determines how quickly to improve the model's performance given new observations (policy=hopefully).\n   - **Epoch time**, optional (default: Infinite) \u2013 Duration of each observation/action sequence over which the agent learns. Can be a specified number, and is used for convergence purposes with `learn_steps`.\n\n   \n   - **Initial policy**: A distribution over actions used as initial policy prior, allowing for forward and backward planning decisions based on observed actions (policy=hopefully).\n\n4. **Notable Features**: These are the key components of this model:\n\n   - **Key variables** (`A`, `B`, `C`)\n   - **Special properties/constraints**\n\n    - **Unique aspects of this model design**.\n    This is a fundamental aspect that sets it apart from other models in the literature, demonstrating its suitability for specific application domains.\n\n5. **Use Cases**: Here are key scenarios where you can apply this model:\n\t* **Game Over**: The goal is to explore different states with an action selected and achieve certain objectives (e.g., capture).\n\t* **Player Selection**: The agent chooses actions at each time step based on the current state/actions combination, choosing ones that provide good starting points for exploration (action=hopefully) or bad-starting when exploring unwanted actions in adjacent states.\n\n6. **Signature**: You can follow up with a concise summary of your analysis:\n\t* \"This agent learns to navigate through various environments by randomly moving across their action space and adjusting its policy.\"",
        "explain_model": "Based on the doc's description, I'll provide a comprehensive explanation of GNNs for Active Inference models:\n\n1. **Definition**: This model represents a classic Active Inference agent used to learn from probability distributions with two observation modalities (`observation_outcomes` and `hidden_states`) with an initial policy (`habit`) based on action choices (`actions`). It learns through the following steps:\n- The agent maps each observed state and actions onto new hidden states. Each time, it updates its prior over these shared observations and biases in probability distributions over actions to predict future ones.\n\n2. **Model Purpose**: This model represents a class of Active Inference POMDP agents that learn to infer the probability distribution of each observation based on their associated state transitions (`states`) and action choices (`actions`). It uses Variational Free Energy (VFE) as its goal, with initial policies acting as actions.\n\n3. **Core Components**: The models represent:\n   - `hidden_state`: The probability distribution over states for each of the observed states in the agent's set-up; it encodes both belief and action biases during inference.\n   - `observations`: A 2D numpy array representing the observations, which capture a sequence of random actions taken by the agent. These actions are mapped onto the hidden states for later learning based on the probability distribution over actions across observed states.\n   - `actions**: A list of sequences that represent an action chosen from each of the available actions (action choices) to be learned from by the agent, with a corresponding reward and return state whenever they're chosen. These are initialized in sequence order according to their action choice probabilities (`pi_c0`), allowing for inference based on initial beliefs without prior knowledge or predictions about future outcomes given actions choices.\n   - `states`: A 2D numpy array representing the sequence of observed states, with each step being a state transition followed by subsequent actions chosen from available actions (action choices). These are initialized in sequence order according to their action choices (`actions`), allowing for inference based on current beliefs before they're updated towards new learned probability distributions.\n\n4. **Active Inference Context**: The POMDP agent learns by sequentially updating its beliefs over the entire history of observed states and actions, until there's no longer an incentive to continue learning. During this process:\n   - The `belief_updates` function is used to update a belief from prior probabilities across all observed states (`\u03c0[states]`) based on posterior probability distributions within each observation (actions) for that state (`\u03c3[state][action]) and then backout towards the beginning of the history, choosing actions after initial beliefs are updated.\n   - The `belief_update` function is used to update an action's belief into a new observable under the assumption it will be later chosen from subsequent observations (`u`) as the agent learns through action choices across observed states (policy posterior).\n\n5. **Active Inference**: When predictions converge towards uncertain actions, the hypothesis of future actions and beliefs are updated using Bayesian inference to update our beliefs about future actions and biases based on probabilities within each observable space (`\u03c3[observation][action])`), allowing for active inference through prediction updates after initial belief updates back out once more observations are observed in a given observation.\n\nPlease provide more details or context specific to your question, such as the types of actions used (actions taken by the agent) and where action choices were made across available actions when using Bayesian inference methods, if applicable.",
        "identify_components": "Here is a systematic breakdown of the elements in the GNN specification:\n\n1. **Action Variables** (represented as `C` matrices): \n   - What each action represents conceptually\n    - Action type, actions/actions\n\n    - For example:\n\n      - Action 1 to state 2\n      - Action 2 from state A\n \n- **Observation Variables**:\n   - Observation modalities and their meanings\n   - Sensor/measurement interpretations\n\n   - Example:\n\n      - Observation 1 is the prior belief.\n      - Observation 2 may be in view of other states or actions\n\n2. **State Variable** (represented as `A` matrices): \n   - What each state represents conceptually\n    - History/history-relevant variables\n\n    - Example:\n\n      - State A to previous states\n    \n    - Example:\n\n      - Observation A is the prior probability for the last observation \n      \n \n3. **Action Variables**:\n   - Available actions and their effects\n    - Example:\n\n    - Action 1 (in action state) to state 2\n    \n      - Action(action_history)(observer,state) -> observation\n\n     - Example:\n\n      - Actions(observation_states)[observation] is the probability of the first observation\n      \n \n4. **Model Variables** (represented as `A` matrices): \n   - What each model variable represents\n    - Variational-free nature\n\n    - Example:\n\n    - Model variables 2 (policy, history) to a policy vector\n    \n      - Value function (initial),\n\n      - Policy vector for an action\n  \n \n5. **Parameter** (represented as `V` matrices): \n   - What each parameter is \n    - Decision models\n    - Initialization of initial belief\n    - Learning rate/adaptation parameters\n\n   - Example:\n\n    - Model variables 1 to states, histories\n\n6. **Hyperparameter** (represented as `A=B=C=D=E`):\n    \n    - P(state_observations)\n    - P(history|action) \n    - P(policy | initial beliefs, history-relevant) \n    - P(initial belief for policy | history/beliefs)\n```",
        "analyze_structure": "Your document provides a comprehensive exploration of Active Inference (AI) POMDP Agents in an open-sourced environment. Here are some key insights and suggestions for analysis:\n\n1. **Generalized Notation Versus PSMDP Representation**: \n   - Generalization is more versatile than a restricted ModelParameterization, but does not guarantee consistency across different models or domains (e.g., AI agents vs PSMDPs).\n   - Using generalized notation allows you to model AI agents in terms of other mathematical structures like Actions and Traits, which can simplify the analysis process.\n\n2. **Model Properties**:\n  - **StateSpaceBlock**: A graph structure with connections describing a network of actions and beliefs is more likely to hold up than a structured Representation.\n  - **Variable Analysis**: Variables are connected by directed edges indicating actions/beliefs; they represent a set of objects from the domain (objects in PSMDP) that can be acted upon via Actions, while also showing dependencies between actions or policies. Variables indicate how different actions interact with each other and affect each other's behavior.\n  - **Mathematical Structure**: Graph structures are easier to interpret mathematically than Representations because they simplify a more familiar algebraic structure (linear graphs).\n\n3. **Complexity Assessment**:\n   - Computational complexity indicators can be useful for assessing the complexity of different models or domains. These include:\n     - **Time Complexity**: The amount of time it takes to compute the information flow through each variable, network connections between variables, and pattern dependencies between them.\n     - **Computational Resource Expenditures (CROM)**: The cost of performing computations on graphs is often expressed as a function of the number of edges and vertices connected by those edges.\n     - **Variability**: A dataset can be averaged across different graph structures to estimate or quantify model complexity, which can impact the analysis results for each specific implementation.\n\n4. **Design Patterns**:\n   - In your case, a Graph structure with connections indicating actions/beliefs should capture more of the information in PSMDP environments because it provides more flexibility in how data is structured and manipulated (graphs). A graph representation also tends to be simpler than a Structured Representation that uses more complex mathematical structures.\n   - Implementing the Global StateSpaceBlock model does not require any additional computations or domain knowledge, which means you can test and validate models on a wide range of systems without requiring detailed information about their specific domains or architectures.\n\nOverall, your analysis provides a comprehensive exploration of Active Inference POMDP Agents in an open-sourced environment while showcasing the diversity and flexibility offered by Graph structures.",
        "extract_parameters": "I've reviewed the documentation for GNN, including the equations and specifications of active inference agents with multiple observation modalities and hidden states. The key components are:\n\n1. **Model Matrices**: A matrix representation of each agent's state space structure. This allows us to access its properties, such as probabilities over actions, biases on observables, and preferences encoded in the matrices' entries. The structure of the Matrix provides a way to describe how individual agents interact with their own parameters or other agents within the same parameter space.\n\n2. **Transition Matrices**: A matrix representation of each agent's transition matrix describing how they update probabilities over observations. This allows us to access their properties and behavior in various scenarios, such as policy updates using Variational Free Energy (VFE) or action selection from Policy Prior (APP).\n\n3. **History Metrics**: List statistics that describe the evolution of each agent's parameters over time. These metrics provide insight into how they change during an inference process. They can be used to optimize performance through training and testing algorithms.\n\n4. **Random Field Metrics** (RBM): A probabilistic graphical model for random field estimation, which is a subset of Bayesian Inference but also offers some flexibility in allowing agents to have prior knowledge on their parameters. This helps to reduce the risk of overfitting or underfitting. RBMs are useful when there's no prior distribution available for each agent, and therefore can help optimize training and testing algorithms through the use of distributions other than Gaussian distributions (Gaussian Distribution) but also provide flexibility in terms of how the algorithm selects actions based on their prior knowledge/beliefs about those parameters.\n\n5. **Variational Inference Metrics** (VIRM): Additional statistics used to evaluate agent behavior, such as Variance of Variability (Va), Variance of Variabilities (Vv), and Variarity of Variability (Vvar). These metrics provide insight into how the agents' behaviors change over time. They can be utilized by exploring different learning algorithms with varying parameter values or tuning specific hyperparameters to optimize performance through training/testing algorithms based on these metrics, as they represent a broader range of techniques that are applicable across a variety of use cases.\n\n6. **Discrete Param Parameters** (DiParam)**: Additional statistics used to evaluate agent behavior, such as Discrete Variability (Dv), and the distribution of their parameters over time. These statistics provide insight into how they change during an inference process and can be utilized by exploring different learning algorithms that involve these statistics or tuning specific hyperparameters based on them, thus providing more information about the underlying mechanisms at play in their behavior than are provided through VIRM.\n\nThese metrics allow us to understand agent behavior better from a variety of angles. They provide insight into how they change over time and can help inform different learning algorithms that involve these statistics or utilize them for tuning specific hyperparameters, thereby providing more information about the underlying mechanisms at play in their behaviors and improving performance through them as well as identifying other patterns when analyzing them.\n\nI'm interested to see where you'd like to explore further - particularly on how parameters are used by exploring different learning algorithms or searching for optimal strategies based upon these metrics so I can provide more information about the specifics of your work now?",
        "practical_applications": "You've done a great job summarizing the key points about the A, B, C, D matrices used in GNN POMDP agent models, including their numerical values and mathematical relationships. Your descriptions are clear and concise. Here's your next step:\n\n1. **Classification**: You can summarize some of these concepts based on your previous paragraphs, but it might be helpful to expand on how the model is designed for applications that require advanced classification tasks or real-time processing scenarios. For example, you could discuss the specific requirements in a research context where this type of model serves as an inference engine for data preprocessing and modeling tasks like image segmentation, sentiment analysis, or predictive text prediction.\n\n2. **Performance**: You can explore some concepts related to performance evaluation, such as validation benchmarks, sensitivity analyses, and metrics that provide insight into the accuracy of predictions based on this model in different scenarios. \n\n3. **Benefits**: You could discuss how GNN POMDPs are used for various applications like image segmentation, sentiment analysis, predictive text prediction, or decision-based reasoning tasks.\n\n4. **Advantages**: You can explain what specific benefits you're seeking from a particular application and describe any potential limitations of the model's performance in these scenarios.\n\n5. **Challenges and Considerations**: You could explore concepts related to data quality, scalability, communication between models, evaluation metrics, and integration with existing systems. \n\n6. **Benefits and Advantages**: You can discuss how GNN POMDPs are used for various applications like image segmentation, sentiment analysis, predictive text prediction, or decision-based reasoning tasks.",
        "technical_description": "A:  \n```python\nimport numpy as np\nfrom scipy import stats\ndef gNN(state_observation):\n    states = state_observation[:,:-1]\n    actions = state_observation[:num_hidden_states], state_observation[num_hidden_states:]\n    \n    # Actions and their probabilities\n    probability_actions=stats.pois(\"Probabilities over actions:\")\n    \n \n    hidden_probabilities=[\n        [\n            (state_observation) \n            .shape([n,1]) \n        ]\n      , \n \n       ([\n         {\n           \"probs\":\n             [ \n                states[0:num_hidden_states],\n                state_observation[:num_hidden_states]\n                \n              ],\n              probes={\"prop:\", probabilities}\n     ])\n    )\n \n    \n    # Hidden and observed actions\n    hidden_actions=[\n        {\n            \"probabilities\":\n                   [{\n                     \"idx\":\n                         :,\n                  \",\"\n               },\n                       {\"state\":\n                             [\n                                 states[0:n],\n                             state_observation[:num]\n                 ]\n              ],\n                probes={\"prop:\", probabilities}\n     ])\n \n    \n    # Probability of a new observation and its hidden actions\n    prev_action = states[:, 1:-2, :] \n    action=\" \" + \", \".join(states[:-1])\n\n    # Random choice on the previous state (in case you want to choose one more time)\n    next_state=[]\n    for i in range(num_hidden_states):\n        if np.random.randint(low=0).astype('int'):\n            next_state.append((i, states[1:2**i]))\n\n        # Set the previous state to be a random index (in case you want to choose one more time)\n    hidden_actions[:] = [[\", \".join([states[next_index]]) for i in range(num_hidden_states)]],\n     \"\"\"{\"prop:\", probabilities}\"\"\"\n \n    \n    return [], [action]\n```",
        "nontechnical_description": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference and GNN specifications.\\n\\nDescribe this GNN model comprehensively, in non-technical language suitable for a broad audience.\\n\\nGNN Model Content:\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\nF[\u03c0,type=float]       # Variational Free Energy for belief updating from observations\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n# - Belief updating using Variational Free Energy with update_beliefs()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here ']' timed out after 60.0 seconds",
        "runtime_behavior": "Prompt execution failed: Command '['ollama', 'run', 'smollm2:135m-instruct-q4_K_S', 'You are an expert in Active Inference and GNN specifications.\\n\\nDescribe what happens when this GNN model runs and how it would behave in different settings or domains.\\n\\nGNN Model Content:\\n# GNN Example: Active Inference POMDP Agent\\n# GNN Version: 1.0\\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.\\n\\n## GNNSection\\nActInfPOMDP\\n\\n## GNNVersionAndFlags\\nGNN v1\\n\\n## ModelName\\nClassic Active Inference POMDP Agent v1\\n\\n## ModelAnnotation\\nThis model describes a classic Active Inference agent for a discrete POMDP:\\n- One observation modality (\"state_observation\") with 3 possible outcomes.\\n- One hidden state factor (\"location\") with 3 possible states.\\n- The hidden state is fully controllable via 3 discrete actions.\\n- The agent\\'s preferences are encoded as log-probabilities over observations.\\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\\n\\n## StateSpaceBlock\\n# Likelihood matrix: A[observation_outcomes, hidden_states]\\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\\n\\n# Transition matrix: B[states_next, states_previous, actions]\\nB[3,3,3,type=float]   # State transitions given previous state and action\\n\\n# Preference vector: C[observation_outcomes]\\nC[3,type=float]       # Log-preferences over observations\\n\\n# Prior vector: D[states]\\nD[3,type=float]       # Prior over initial hidden states\\n\\n# Habit vector: E[actions]\\nE[3,type=float]       # Initial policy prior (habit) over actions\\n\\n# Hidden State\\ns[3,1,type=float]     # Current hidden state distribution\\ns_prime[3,1,type=float] # Next hidden state distribution\\nF[\u03c0,type=float]       # Variational Free Energy for belief updating from observations\\n\\n# Observation\\no[3,1,type=int]     # Current observation (integer index)\\n\\n# Policy and Control\\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\\nu[1,type=int]         # Action taken\\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\\n\\n# Time\\nt[1,type=int]         # Discrete time step\\n\\n## Connections\\nD>s\\ns-A\\ns>s_prime\\nA-o\\ns-B\\nC>G\\nE>\u03c0\\nG>\u03c0\\n\u03c0>u\\nB>u\\nu>s_prime\\n\\n## InitialParameterization\\n# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\\nA={\\n  (0.9, 0.05, 0.05),\\n  (0.05, 0.9, 0.05),\\n  (0.05, 0.05, 0.9)\\n}\\n\\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\\nB={\\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\\n}\\n\\n# C: 3 observations. Preference in terms of log-probabilities over observations.\\nC={(0.1, 0.1, 1.0)}\\n\\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\\nD={(0.33333, 0.33333, 0.33333)}\\n\\n# E: 3 actions. Uniform habit used as initial policy prior.\\nE={(0.33333, 0.33333, 0.33333)}\\n\\n## Equations\\n# Standard Active Inference update equations for POMDPs:\\n# - State inference using Variational Free Energy with infer_states()\\n# - Policy inference using Expected Free Energy = with infer_policies()\\n# - Action selection from policy posterior: action = sample_action()\\n# - Belief updating using Variational Free Energy with update_beliefs()\\n\\n## Time\\nTime=t\\nDynamic\\nDiscrete\\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.\\n\\n## ActInfOntologyAnnotation\\nA=LikelihoodMatrix\\nB=TransitionMatrix\\nC=LogPreferenceVector\\nD=PriorOverHiddenStates\\nE=Habit\\nF=VariationalFreeEnergy\\nG=ExpectedFreeEnergy\\ns=HiddenState\\ns_prime=NextHiddenState\\no=Observation\\n\u03c0=PolicyVector # Distribution over actions\\nu=Action       # Chosen action\\nt=Time\\n\\n## ModelParameters\\nnum_hidden_states: 3  # s[3]\\nnum_obs: 3           # o[3]\\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)\\n\\n## Footer\\nActive Inference POMDP Agent v1 - GNN Representation. \\nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting. \\n\\n## Signature\\nCryptographic signature goes here ']' timed out after 60.0 seconds"
      }
    }
  ],
  "model_insights": [
    {
      "model_complexity": "high",
      "recommendations": [
        "Consider breaking down into smaller modules",
        "High variable count - consider grouping related variables",
        "Consider breaking down into smaller modules"
      ],
      "strengths": [],
      "weaknesses": [
        "Complex model may be difficult to understand",
        "No connections defined"
      ],
      "generation_timestamp": "2026-01-07T11:29:04.560622"
    }
  ],
  "code_suggestions": [
    {
      "optimizations": [],
      "improvements": [
        "Many variables lack type annotations - consider adding explicit types"
      ],
      "best_practices": [
        "Use descriptive variable names",
        "Group related variables together"
      ],
      "generation_timestamp": "2026-01-07T11:29:04.560692"
    }
  ],
  "documentation_generated": [
    {
      "file_path": "input/gnn_files/actinf_pomdp_agent.md",
      "model_overview": "\n# actinf_pomdp_agent.md Model Documentation\n\nThis GNN model contains 72 variables and 0 connections.\n\n## Model Structure\n- **Variables**: 72 defined\n- **Connections**: 0 defined\n- **Complexity**: 72 total elements\n\n## Key Components\n",
      "variable_documentation": [
        {
          "name": "Example",
          "definition": "Example: Active",
          "line": 1,
          "description": "Variable defined at line 1"
        },
        {
          "name": "Version",
          "definition": "Version: 1",
          "line": 2,
          "description": "Variable defined at line 2"
        },
        {
          "name": "matrix",
          "definition": "matrix: A",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "matrix",
          "definition": "matrix: B",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "vector",
          "definition": "vector: C",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "vector",
          "definition": "vector: D",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "vector",
          "definition": "vector: E",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "A",
          "definition": "A: 3",
          "line": 68,
          "description": "Variable defined at line 68"
        },
        {
          "name": "B",
          "definition": "B: 3",
          "line": 75,
          "description": "Variable defined at line 75"
        },
        {
          "name": "C",
          "definition": "C: 3",
          "line": 82,
          "description": "Variable defined at line 82"
        },
        {
          "name": "D",
          "definition": "D: 3",
          "line": 85,
          "description": "Variable defined at line 85"
        },
        {
          "name": "E",
          "definition": "E: 3",
          "line": 88,
          "description": "Variable defined at line 88"
        },
        {
          "name": "posterior",
          "definition": "posterior: action",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "num_hidden_states",
          "definition": "num_hidden_states: 3",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "num_obs",
          "definition": "num_obs: 3",
          "line": 121,
          "description": "Variable defined at line 121"
        },
        {
          "name": "num_actions",
          "definition": "num_actions: 3",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "type",
          "definition": "type=float]   # Likelihood mapping hidden states to observations",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "type",
          "definition": "type=float]   # State transitions given previous state and action",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "type",
          "definition": "type=float]       # Log-preferences over observations",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "type",
          "definition": "type=float]       # Prior over initial hidden states",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "type",
          "definition": "type=float]       # Initial policy prior (habit) over actions",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "type",
          "definition": "type=float]     # Current hidden state distribution",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "type",
          "definition": "type=float] # Next hidden state distribution",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "type",
          "definition": "type=float]       # Variational Free Energy for belief updating from observations",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "type",
          "definition": "type=int]     # Current observation (integer index)",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "type",
          "definition": "type=float]       # Policy (distribution over actions), no planning",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "type",
          "definition": "type=int]         # Action taken",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "type",
          "definition": "type=float]       # Expected Free Energy (per policy)",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "type",
          "definition": "type=int]         # Discrete time step",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "A",
          "definition": "A={",
          "line": 69,
          "description": "Variable defined at line 69"
        },
        {
          "name": "B",
          "definition": "B={",
          "line": 76,
          "description": "Variable defined at line 76"
        },
        {
          "name": "C",
          "definition": "C={(0.1, 0.1, 1.0)}",
          "line": 83,
          "description": "Variable defined at line 83"
        },
        {
          "name": "D",
          "definition": "D={(0.33333, 0.33333, 0.33333)}",
          "line": 86,
          "description": "Variable defined at line 86"
        },
        {
          "name": "E",
          "definition": "E={(0.33333, 0.33333, 0.33333)}",
          "line": 89,
          "description": "Variable defined at line 89"
        },
        {
          "name": "Energy",
          "definition": "Energy = with infer_policies()",
          "line": 94,
          "description": "Variable defined at line 94"
        },
        {
          "name": "action",
          "definition": "action = sample_action()",
          "line": 95,
          "description": "Variable defined at line 95"
        },
        {
          "name": "Time",
          "definition": "Time=t",
          "line": 99,
          "description": "Variable defined at line 99"
        },
        {
          "name": "ModelTimeHorizon",
          "definition": "ModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon",
          "line": 102,
          "description": "Variable defined at line 102"
        },
        {
          "name": "A",
          "definition": "A=LikelihoodMatrix",
          "line": 105,
          "description": "Variable defined at line 105"
        },
        {
          "name": "B",
          "definition": "B=TransitionMatrix",
          "line": 106,
          "description": "Variable defined at line 106"
        },
        {
          "name": "C",
          "definition": "C=LogPreferenceVector",
          "line": 107,
          "description": "Variable defined at line 107"
        },
        {
          "name": "D",
          "definition": "D=PriorOverHiddenStates",
          "line": 108,
          "description": "Variable defined at line 108"
        },
        {
          "name": "E",
          "definition": "E=Habit",
          "line": 109,
          "description": "Variable defined at line 109"
        },
        {
          "name": "F",
          "definition": "F=VariationalFreeEnergy",
          "line": 110,
          "description": "Variable defined at line 110"
        },
        {
          "name": "G",
          "definition": "G=ExpectedFreeEnergy",
          "line": 111,
          "description": "Variable defined at line 111"
        },
        {
          "name": "s",
          "definition": "s=HiddenState",
          "line": 112,
          "description": "Variable defined at line 112"
        },
        {
          "name": "s_prime",
          "definition": "s_prime=NextHiddenState",
          "line": 113,
          "description": "Variable defined at line 113"
        },
        {
          "name": "o",
          "definition": "o=Observation",
          "line": 114,
          "description": "Variable defined at line 114"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0=PolicyVector # Distribution over actions",
          "line": 115,
          "description": "Variable defined at line 115"
        },
        {
          "name": "u",
          "definition": "u=Action       # Chosen action",
          "line": 116,
          "description": "Variable defined at line 116"
        },
        {
          "name": "t",
          "definition": "t=Time",
          "line": 117,
          "description": "Variable defined at line 117"
        },
        {
          "name": "actions_dim",
          "definition": "actions_dim=3 (controlled by \u03c0)",
          "line": 122,
          "description": "Variable defined at line 122"
        },
        {
          "name": "A",
          "definition": "A[observation_outcomes, hidden_states]",
          "line": 23,
          "description": "Variable defined at line 23"
        },
        {
          "name": "A",
          "definition": "A[3,3,type=float]",
          "line": 24,
          "description": "Variable defined at line 24"
        },
        {
          "name": "B",
          "definition": "B[states_next, states_previous, actions]",
          "line": 26,
          "description": "Variable defined at line 26"
        },
        {
          "name": "B",
          "definition": "B[3,3,3,type=float]",
          "line": 27,
          "description": "Variable defined at line 27"
        },
        {
          "name": "C",
          "definition": "C[observation_outcomes]",
          "line": 29,
          "description": "Variable defined at line 29"
        },
        {
          "name": "C",
          "definition": "C[3,type=float]",
          "line": 30,
          "description": "Variable defined at line 30"
        },
        {
          "name": "D",
          "definition": "D[states]",
          "line": 32,
          "description": "Variable defined at line 32"
        },
        {
          "name": "D",
          "definition": "D[3,type=float]",
          "line": 33,
          "description": "Variable defined at line 33"
        },
        {
          "name": "E",
          "definition": "E[actions]",
          "line": 35,
          "description": "Variable defined at line 35"
        },
        {
          "name": "E",
          "definition": "E[3,type=float]",
          "line": 36,
          "description": "Variable defined at line 36"
        },
        {
          "name": "s",
          "definition": "s[3,1,type=float]",
          "line": 39,
          "description": "Variable defined at line 39"
        },
        {
          "name": "s_prime",
          "definition": "s_prime[3,1,type=float]",
          "line": 40,
          "description": "Variable defined at line 40"
        },
        {
          "name": "F",
          "definition": "F[\u03c0,type=float]",
          "line": 41,
          "description": "Variable defined at line 41"
        },
        {
          "name": "o",
          "definition": "o[3,1,type=int]",
          "line": 44,
          "description": "Variable defined at line 44"
        },
        {
          "name": "\u03c0",
          "definition": "\u03c0[3,type=float]",
          "line": 47,
          "description": "Variable defined at line 47"
        },
        {
          "name": "u",
          "definition": "u[1,type=int]",
          "line": 48,
          "description": "Variable defined at line 48"
        },
        {
          "name": "G",
          "definition": "G[\u03c0,type=float]",
          "line": 49,
          "description": "Variable defined at line 49"
        },
        {
          "name": "t",
          "definition": "t[1,type=int]",
          "line": 52,
          "description": "Variable defined at line 52"
        },
        {
          "name": "s",
          "definition": "s[3]",
          "line": 120,
          "description": "Variable defined at line 120"
        },
        {
          "name": "o",
          "definition": "o[3]",
          "line": 121,
          "description": "Variable defined at line 121"
        }
      ],
      "connection_documentation": [],
      "usage_examples": [
        {
          "title": "Variable Usage",
          "description": "Example variables: Example, Version, matrix",
          "code": "# Example variable usage\nExample = ...\nVersion = ...\nmatrix = ..."
        }
      ],
      "generation_timestamp": "2026-01-07T11:29:04.560733"
    }
  ]
}