# EXPLAIN_MODEL

This GNN (Generalized Notation Notation) POMDP agent is designed to provide an active inference mechanism for a discrete Markov chain (Pomdp). The agent performs actions based on its policy and chooses which actions are taken by choosing different states/observations with probability of one. These decisions depend on the prior probabilities over the history, actions, and future observations.

The model comprises five components:

1. **Input**: Input data is provided for training purposes; the input consists of a set of observed data (observation_outcomes) and hidden state information used to initialize or update policies/prior distributions/beliefs/etc.

2. **Learning**: Each observation is represented as a vector containing its probability across different states, with some columns representing previously observed states and others indicating actions taken. The policy distribution represents these beliefs for each observation based on the previous observations of known states ("state_observation").

3. **Model Parameters**: The input parameters represent what data are used to compute outputs (policy/beliefs), while the learning parameter determines how well the model fits past behaviors into future predictions.

4. **Learning Model**: A set of rules for computing beliefs is presented to allow generating predictions based on observed probabilities. Each observation corresponds to a particular action, and each action has its corresponding probability distribution over previous observations. This enables generating predictions from existing behavior patterns towards new ones.

5. **Outputs**: Output data are represented as vector representations that define the actions being taken (observation_outcomes). These correspond to given observed state probabilities for all actions ("observation"), followed by their prior distributions and belief representations, respectively, based on previously observed states "states_next".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observations from which predictions are generated through observable behavior patterns (policies/prior probabilities).

2. **Learning Model**: The learning model comprises the following rules for generating actions based on previous observables and beliefs ("policy_beliefs", etc.):
   - Policy-based learning: Actions taken based on their past behaviors towards observed states "states" or prior distributions "state".
   - Belief-based learning: Actions used to make predictions from observable behavior patterns.
   - Action prediction policy: The agent's current belief is determined by its actions/prior distribution with probability of one (policy).

3. **Output**: Output data are represented as vector representations that define the beliefs for each observation ("observations"). These represent observed outcomes and can be used to generate predictions from observable behavior patterns towards new ones "observation_actions".

4. **Learning Model**: The learning model comprises the following rules for computing actions based on previous observations of known states, probabilities (policy), or prior distributions (priorities). For each observation/observations pairs ("p", p_current), there is a corresponding action-based probability distribution and belief representation that maps observed outcomes to their associated beliefs. This enables generating predictions from observable behavior patterns towards new ones "observation".

5. **Output**: The output data are represented as vector representations that define the actions being taken (observations). These represent observed observations and can be used to generate predictions from observable behavior patterns towards new ones "observation_actions".

The agent consists of five components:

1. **Input** - Input data is provided via a set of visible observables. This enables generating predictions based on observable behaviors towards known states ("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observed outcomes ("observations"). Each action-based probability distribution maps observed observations to their corresponding corresponding beliefs, respectively, with the probability of one being 1/n(probability). The policy pattern "policy" is represented by a set of policy distributions that are initialized based on known state probabilities.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors (states) and prior distribution probabilities for observed observables ("observation") or corresponding beliefs ("observations"). These represent observations from the current observation to "observations" which define the action-based probability distributions of past actions/prior, while the policy is represented by a set of policies.

4. **Learning Model** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observed outcomes ("observation"). Each action-based probability distribution maps observations to their corresponding observables or prior probabilities with probability 1/n(probability). The policy is represented by a set of policies, which represent each action based on its known state probabilities.

5. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation"). These define the actions being taken and can be used to generate predictions from observable behavior patterns towards new ones "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes ("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observed outcomes ("observation"). Each action-based probability distribution maps observations to their corresponding observables/priorities with probability 1/n(probability). The policy is represented by a set of policies, which represent each action based on its known state probabilities.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation"). These define the actions being taken and can be used to generate predictions from observable behavior patterns towards new ones "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes ("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observed outcomes("observation"). Each action-based probability distribution maps observations to their corresponding observables/priorities with probability 1/(n(probability)). The policy is represented by a set of policies, which represent each action based on its known state probabilities.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation"). These define the actions being taken and can be used to generate predictions from observable behavior patterns towards new ones "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observing observations with probability 1/(n(probability)). Each action-based probability distribution maps observations to their corresponding observables/priorities with probability 1/(n(probabilities)) by considering observed outcomes in a prior way.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation") and can be used to generate predictions from observing behaviors "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observing observations with probability 1/(n(probability)). Each action-based probability distribution maps observed observables to their corresponding observables/priorities with probability 1/(n(probabilities)) by considering observed outcomes in a prior way.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation"). These define the actions being taken and can be used to generate predictions from observing behaviors "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observing observations with probability 1/(n(probability)). Each action-based probability distribution maps observed observables to their corresponding observables/priorities with probability 1/(n(probabilities)) by considering observed outcomes in a prior way.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation") and can be used to generate predictions from observing behaviors "observations".

The agent consists of five components:

1. **Input**: Input data is provided via a set of visible observables. This enables generating predictions based on observed outcomes("states") or prior probabilities.

2. **Learning** - A learning rule computes the learned beliefs (belief) from observable behavior patterns and actions for observing observations with probability 1/(n(probability)). Each action-based probability distribution maps observed observables to their corresponding observables/priorities with probability 1/(n(probabilities)) by considering observed outcomes in a prior way.

3. **Output** - Output data are presented in vector representations representing actions taken towards observable behaviors ("observation") and can be used to generate predictions from observing behaviors "observations".