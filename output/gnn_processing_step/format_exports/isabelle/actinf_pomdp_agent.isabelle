(* GNN Model: Classic Active Inference POMDP Agent v1 *)
(* Generated from GNN markdown *)

theory GNNModel
imports Main
begin

datatype variable = Variable
  (name: string)
  (type: string)
  (dimensions: "nat list")
  (description: string)

datatype connection = Connection
  (source: string)
  (target: string)
  (type: string)
  (description: string)

datatype parameter = Parameter
  (name: string)
  (value: string)
  (description: string)

datatype gnn_model = GNNModel
  (model_name: string)
  (version: string)
  (annotation: string)
  (variables: "variable list")
  (connections: "connection list")
  (parameters: "parameter list")

definition model :: gnn_model where
  "model = GNNModel
    \<open>Classic Active Inference POMDP Agent v1\<close>
    \<open>1.0\<close>
    \<open>This model describes a classic Active Inference agent for a discrete POMDP:
- One observation modality ("state_observation") with 3 possible outcomes.
- One hidden state factor ("location") with 3 possible states.
- The hidden state is fully controllable via 3 discrete actions.
- The agent's preferences are encoded as log-probabilities over observations.
- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\<close>
    [      Variable \<open>A\<close> \<open>parameter_matrix\<close> [3, 3] \<open>Likelihood mapping hidden states to observations\<close>
      Variable \<open>B\<close> \<open>parameter_matrix\<close> [3, 3, 3] \<open>State transitions given previous state and action\<close>
      Variable \<open>C\<close> \<open>parameter_matrix\<close> [3] \<open>Log-preferences over observations\<close>
      Variable \<open>D\<close> \<open>parameter_matrix\<close> [3] \<open>Prior over initial hidden states\<close>
      Variable \<open>E\<close> \<open>parameter_matrix\<close> [3] \<open>Initial policy prior (habit) over actions\<close>
      Variable \<open>s\<close> \<open>hidden_state\<close> [3, 1] \<open>Current hidden state distribution\<close>
      Variable \<open>s_prime\<close> \<open>hidden_state\<close> [3, 1] \<open>Next hidden state distribution\<close>
      Variable \<open>o\<close> \<open>observation\<close> [3, 1] \<open>Current observation (integer index)\<close>
      Variable \<open>π\<close> \<open>hidden_state\<close> [3] \<open>Policy (distribution over actions), no planning\<close>
      Variable \<open>u\<close> \<open>action\<close> [1] \<open>Action taken\<close>
      Variable \<open>G\<close> \<open>hidden_state\<close> [] \<open>Expected Free Energy (per policy)\<close>
      Variable \<open>t\<close> \<open>hidden_state\<close> [1] \<open>Discrete time step\<close>    ]
    [      Connection \<open>D\<close> \<open>s\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>s\<close> \<open>A\<close> \<open>undirected\<close> \<open>\<close>
      Connection \<open>s\<close> \<open>s_prime\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>A\<close> \<open>o\<close> \<open>undirected\<close> \<open>\<close>
      Connection \<open>s\<close> \<open>B\<close> \<open>undirected\<close> \<open>\<close>
      Connection \<open>C\<close> \<open>G\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>E\<close> \<open>π\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>G\<close> \<open>π\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>π\<close> \<open>u\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>B\<close> \<open>u\<close> \<open>directed\<close> \<open>\<close>
      Connection \<open>u\<close> \<open>s_prime\<close> \<open>directed\<close> \<open>\<close>    ]
    [      Parameter \<open>A\<close> \<open>{\<close> \<open>\<close>
      Parameter \<open>B\<close> \<open>{\<close> \<open>\<close>
      Parameter \<open>C\<close> \<open>(0.1, 0.1, 1.0)\<close> \<open>\<close>
      Parameter \<open>D\<close> \<open>(0.33333, 0.33333, 0.33333)\<close> \<open>\<close>
      Parameter \<open>E\<close> \<open>(0.33333, 0.33333, 0.33333)\<close> \<open>\<close>    ]"

end
