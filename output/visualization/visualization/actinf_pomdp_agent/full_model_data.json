{
  "_HeaderComments": "# GNN Example: Classic Active Inference POMDP Agent\n# Format: Markdown representation of a single-modality, single-factor POMDP agent in Active Inference format\n# Version: 1.0\n# This file is machine-readable and specifies a classic Active Inference agent for a discrete POMDP with one observation modality and one hidden state factor. The model is suitable for rendering into various simulation or inference backends.",
  "GNNSection": "ClassicPOMDPAgent",
  "GNNVersionAndFlags": "GNN v1",
  "ModelName": "Classic Active Inference POMDP Agent v1",
  "ModelAnnotation": "This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.\n- All parameterizations are explicit and suitable for translation to code or simulation in any Active Inference framework.",
  "StateSpaceBlock": "# Likelihood matrix: A[observation_outcomes, hidden_states]\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\n\n# Transition matrix: B[states_next, states_previous, actions]\nB[3,3,3,type=float]   # State transitions given previous state and action\n\n# Preference vector: C[observation_outcomes]\nC[3,type=float]       # Log-preferences over observations\n\n# Prior vector: D[states]\nD[3,type=float]       # Prior over initial hidden states\n\n# Habit vector: E[actions]\nE[3,type=float]       # Initial policy prior (habit) over actions\n\n# Hidden State\ns[3,1,type=float]     # Current hidden state distribution\ns_prime[3,1,type=float] # Next hidden state distribution\n\n# Observation\no[3,1,type=float]     # Current observation\n\n# Policy and Control\n\u03c0[3,type=float]       # Policy (distribution over actions)\nu[1,type=int]         # Action taken\nG[1,type=float]       # Expected Free Energy (scalar or per policy)\nt[1,type=int]         # Time step",
  "Connections": "D-s\ns-A\ns-s_prime\nA-o\n(s,u)-B\nB-s_prime\nC>G\nE>\u03c0\nG>\u03c0\n\u03c0-u",
  "InitialParameterization": "# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation).\nA={\n  (1.0, 0.0, 0.0),  # obs=0; state=0,1,2\n  (0.0, 1.0, 0.0),  # obs=1\n  (0.0, 0.0, 1.0)   # obs=2\n}\n\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state.\nB={\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ), # s_next=0; actions 0,1,2\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ), # s_next=1\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )  # s_next=2\n}\n\n# C: 3 observations. Preference for observing state 2.\nC={(0.0, 0.0, 1.0)}\n\n# D: 3 states. Uniform prior.\nD={(0.33333, 0.33333, 0.33333)}\n\n# E: 3 actions. Uniform habit (no initial preference for any action).\nE={(0.33333, 0.33333, 0.33333)}",
  "Equations": "# Standard Active Inference update equations for POMDPs:\n# - State inference: qs = infer_states(o)\n# - Policy evaluation: q_pi, efe = infer_policies()\n# - Action selection: action = sample_action()",
  "Time": "Dynamic\nDiscreteTime=t\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.",
  "ActInfOntologyAnnotation": "A=LikelihoodMatrix\nB=TransitionMatrix\nC=LogPreferenceVector\nD=PriorOverHiddenStates\nE=Habit\ns=HiddenState\ns_prime=NextHiddenState\no=Observation\n\u03c0=PolicyVector # Distribution over actions\nu=Action       # Chosen action\nF=VariationalFreeEnergy\nG=ExpectedFreeEnergy\nt=Time",
  "ModelParameters": "num_hidden_states: 3  # s[3]\nnum_obs: 3           # o[3]\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)",
  "Footer": "Classic Active Inference POMDP Agent v1 - GNN Representation",
  "Signature": "NA"
}