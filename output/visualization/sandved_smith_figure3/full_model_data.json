{
  "_HeaderComments": "# GNN Example: Sandved-Smith Figure 3 - Deep Generative Model for Policy Selection\n# Format: Markdown representation of a Deep Active Inference model for meta-awareness and attentional control\n# Version: 1.0\n# This file represents the computational architecture from Sandved-Smith et al. (2021) Figure 3",
  "GNNSection": "SandvedSmithFigure3",
  "GNNVersionAndFlags": "GNN v1",
  "ModelName": "Deep Generative Model for Policy Selection with Meta-Awareness",
  "ModelAnnotation": "This model represents a deep parametric active inference architecture for modeling meta-awareness and attentional control.\nBased on Sandved-Smith et al. (2021) \"Towards a computational phenomenology of mental action: modelling meta-awareness and attentional control with deep parametric active inference.\"\n\nThe model includes:\n- Single hidden state factor relating to observable modality\n- Policy selection over cognitive states through hierarchical inference\n- Precision control for attentional modulation\n- Meta-awareness states that modulate confidence in lower-level mappings\n- Dynamic perception and action in a unified framework\n\nThis architecture enables the emergence of opacity-transparency dynamics characteristic of conscious experience and deliberate cognitive control.",
  "StateSpaceBlock": "# Likelihood matrices - A matrices (observation models)\nA[2,2,type=float]                    # Likelihood mapping from hidden states to observations P(o|s)\n\n# Transition matrices - B matrices (dynamics models)  \nB[2,2,1,type=float]                  # Transition dynamics P(s'|s,\u03c0) conditioned on policy\nB_pi[2,2,2,type=float]               # Policy-dependent transition matrices\n\n# Preference vectors - C matrices (prior preferences)\nC[2,type=float]                      # Prior preferences over observations P(o)\n\n# Prior state distributions - D matrices\nD[2,type=float]                      # Prior beliefs over initial states P(s_0)\n\n# Policy priors - E matrices\nE[2,type=float]                      # Prior beliefs over policies P(\u03c0)\n\n# Expected free energy - G\nG[2,type=float]                      # Expected free energy for each policy G_\u03c0\n\n# Hidden states\ns[2,1,type=continuous]               # Hidden state beliefs at current time\ns_prev[2,1,type=continuous]          # Hidden state beliefs at previous time  \ns_next[2,1,type=continuous]          # Hidden state beliefs at next time\ns_bar[2,1,type=continuous]           # Posterior hidden state beliefs\n\n# Observations\no[2,1,type=discrete]                 # Current observations\no_bar[2,1,type=continuous]           # Posterior observation beliefs\no_pred[2,1,type=continuous]          # Predicted observations\n\n# Policies and actions\n\u03c0[2,type=continuous]                 # Policy beliefs (posterior over policies)\n\u03c0_bar[2,type=continuous]             # Updated policy posterior\nu[1,type=int]                        # Selected action\n\n# Precision parameters\n\u03b3_A[1,type=float]                    # Precision (inverse variance) of likelihood mapping\n\u03b2_A[1,type=float]                    # Inverse precision parameter (\u03b2_A = 1/\u03b3_A)\n\u03b2_A_bar[1,type=float]                # Updated inverse precision\n\n# Free energy components\nF_\u03c0[2,type=float]                    # Variational free energy per policy\nE_\u03c0[2,type=float]                    # Complexity term per policy\n\n# Time indexing\nt[1,type=int]                        # Current time step",
  "Connections": "# Prior connections\nD > s\nE > \u03c0\n\n# Observation model connections  \n(s, \u03b3_A) > A\nA > o_pred\n(A, s_bar) > o_bar\n\n# Transition model connections\n(s, \u03c0) > B_pi\nB_pi > s_next\n(s_prev, B_pi) > s\n\n# Policy selection connections\n(G, E_\u03c0, F_\u03c0) > \u03c0_bar\n\u03c0_bar > u\n\n# Precision control connections\n\u03b3_A > A\n\u03b2_A > \u03b3_A\n\u03b2_A_bar > \u03b2_A\n\n# State inference connections\n(A, o, B_pi) > s_bar\n(s_bar, C) > F_\u03c0\nF_\u03c0 > \u03c0\n\n# Expected free energy computation\n(C, o_pred, s_bar) > G\nG > \u03c0\n\n# Temporal connections\ns > s_next\ns_prev > s\nt > (s, o, \u03c0)",
  "InitialParameterization": "# Likelihood matrix A: P(o|s)\n# Simple 2x2 mapping between states and observations\nA={\n  ((0.9, 0.1), (0.1, 0.9))  # High confidence mapping with some noise\n}\n\n# Transition matrix B: P(s'|s) for baseline (no action)\nB={\n  ((0.8, 0.2), (0.2, 0.8))  # Tendency to stay in current state\n}\n\n# Policy-dependent transitions B_\u03c0\nB_pi={\n  ((0.9, 0.1), (0.1, 0.9)),  # Policy 0: stay in current state\n  ((0.3, 0.7), (0.7, 0.3))   # Policy 1: switch states\n}\n\n# Prior preferences C\nC={(0.0, 1.0)}  # Preference for observing outcome 1\n\n# Prior state beliefs D  \nD={(0.5, 0.5)}  # Uniform prior over initial states\n\n# Prior policy beliefs E\nE={(0.5, 0.5)}  # Uniform prior over policies\n\n# Precision parameters\n\u03b3_A=2.0         # Moderate precision on likelihood mapping\n\u03b2_A=0.5         # Inverse precision (1/\u03b3_A)\n\n# Initial states\ns={(0.5, 0.5)}           # Uncertain initial state\no={(1.0, 0.0)}           # Initial observation (outcome 0)\n\u03c0={(0.5, 0.5)}           # Initial policy uncertainty\n\n# Time\nt=0",
  "Equations": "# Policy posterior (softmax of negative expected free energy):\n# \u03c0\u0304 = \u03c3(-E_\u03c0 - G_\u03c0)\n# where \u03c3 is the softmax function\n\n# State transition dynamics:\n# s_{t+1} = B_\u03c0 * s_t\n# where B_\u03c0 is the policy-dependent transition matrix\n\n# Precision relationship:\n# \u03b3_A = 1/\u03b2_A\n\n# Expected free energy per policy:\n# G_\u03c0 = \u03a3_t [o_{\u03c0,t} \u00b7 (ln(o_{\u03c0,t}) - C) - diag(A \u00b7 ln(A)) \u00b7 s\u0304_{\u03c0,t}]\n# where \u00b7 denotes element-wise multiplication\n\n# Variational free energy per policy:\n# F_\u03c0 = \u03a3_t [s\u0304_{\u03c0,t} \u00b7 (ln(s\u0304_{\u03c0,t}) - ln(A) \u00b7 o_t - 0.5 * ln(B_{t-1} * s\u0304_{\u03c0,t-1}) - 0.5 * ln(B_{t+1} * s\u0304_{\u03c0,t+1}))]\n\n# Complete policy posterior:\n# \u03c0\u0304 = \u03c3(-E_\u03c0 - G_\u03c0 - F_\u03c0)\n\n# State inference (posterior beliefs):\n# s\u0304_{\u03c0,t} = \u03c3(ln(B_{\u03c0,t-1} * s_{t-1}) + ln(\u0100 \u00b7 \u014d_t) + ln(B_{\u03c0,t} * s_{t+1}))\n\n# Precision update:\n# \u03b2\u0304_A = \u03b2_A - \u03a3_t(ln(A) \u00b7 (\u014d_t * s_t - \u0100 * s_t))\n\n# Observation posterior:\n# \u014d_t = \u03b4(o_t)\n# where \u03b4 is the Kronecker delta function",
  "Time": "Dynamic\nDiscreteTime=t\nModelTimeHorizon=Unbounded\nTemporalDepth=3  # Past, present, future dependencies",
  "ActInfOntologyAnnotation": "A=LikelihoodMapping\nB=TransitionDynamics\nB_pi=PolicyDependentTransitions\nC=PriorPreferences\nD=PriorStateBeliefs\nE=PriorPolicyBeliefs\nG=ExpectedFreeEnergy\nF_\u03c0=VariationalFreeEnergy\ns=HiddenStates\ns_bar=PosteriorHiddenStates\no=Observations\no_bar=PosteriorObservations\n\u03c0=PolicyBeliefs\n\u03c0_bar=PosteriorPolicyBeliefs\nu=SelectedAction\n\u03b3_A=LikelihoodPrecision\n\u03b2_A=InversePrecision\nt=TimeStep",
  "ModelParameters": "num_states: 2              # Binary state space\nnum_observations: 2        # Binary observation space  \nnum_policies: 2           # Two available policies\ntemporal_horizon: 3        # Past, present, future\nprecision_dynamics: true   # Enable precision learning\npolicy_selection: true     # Enable active policy selection",
  "Footer": "Deep Generative Model for Policy Selection - Sandved-Smith et al. (2021) Figure 3\nComputational Phenomenology of Mental Action and Meta-Awareness",
  "Signature": "Creator: AI Assistant for GNN\nDate: 2024-12-28\nSource: Sandved-Smith, L., Hesp, C., Mattout, J., Friston, K., Lutz, A., & Ramstead, M. J. D. (2021)\nPaper: \"Towards a computational phenomenology of mental action: modelling meta-awareness and attentional control with deep parametric active inference\"\nJournal: Neuroscience of Consciousness, 2021(1), niab018\nStatus: Research implementation of deep parametric active inference for cognitive control"
}