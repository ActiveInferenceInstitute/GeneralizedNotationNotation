STDOUT:
.............s............................ssssss........................ [ 13%]
..........................s...................s....................s.... [ 27%]
................................s.........ss.ss.......ss.ss..........FFF [ 40%]
.F...........................................s................s.......ss [ 54%]
sssss............F..EEEEE
==================================== ERRORS ====================================
_ ERROR at setup of TestFileOperationErrorScenarios.test_missing_input_directory _
file /Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py, line 112
      @pytest.mark.unit
      @pytest.mark.safe_to_fail
      def test_missing_input_directory(self, temp_directories):
E       fixture 'temp_directories' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, capture_logs, comprehensive_test_data, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, full_pipeline_environment, include_metadata_in_junit_xml, isolated_temp_dir, json_metadata, metadata, mock_dangerous_operations, mock_filesystem, mock_imports, mock_llm_provider, mock_logger, mock_mcp_tools, mock_render_module, mock_subprocess, monkeypatch, no_cover, pipeline_arguments, project_root, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, safe_filesystem, sample_gnn_file, sample_gnn_files, sample_gnn_spec, sample_markdown, sample_scala, simulate_failures, src_dir, temp_output_dir, test_config, test_data_dir, test_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py:112
_ ERROR at setup of TestFileOperationErrorScenarios.test_readonly_output_directory _
file /Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py, line 136
      @pytest.mark.unit
      @pytest.mark.safe_to_fail
      def test_readonly_output_directory(self, temp_directories):
E       fixture 'temp_directories' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, capture_logs, comprehensive_test_data, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, full_pipeline_environment, include_metadata_in_junit_xml, isolated_temp_dir, json_metadata, metadata, mock_dangerous_operations, mock_filesystem, mock_imports, mock_llm_provider, mock_logger, mock_mcp_tools, mock_render_module, mock_subprocess, monkeypatch, no_cover, pipeline_arguments, project_root, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, safe_filesystem, sample_gnn_file, sample_gnn_files, sample_gnn_spec, sample_markdown, sample_scala, simulate_failures, src_dir, temp_output_dir, test_config, test_data_dir, test_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py:136
_ ERROR at setup of TestFileOperationErrorScenarios.test_corrupted_gnn_file_handling _
file /Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py, line 168
      @pytest.mark.unit
      @pytest.mark.safe_to_fail
      def test_corrupted_gnn_file_handling(self, temp_directories):
E       fixture 'temp_directories' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, capture_logs, comprehensive_test_data, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, full_pipeline_environment, include_metadata_in_junit_xml, isolated_temp_dir, json_metadata, metadata, mock_dangerous_operations, mock_filesystem, mock_imports, mock_llm_provider, mock_logger, mock_mcp_tools, mock_render_module, mock_subprocess, monkeypatch, no_cover, pipeline_arguments, project_root, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, safe_filesystem, sample_gnn_file, sample_gnn_files, sample_gnn_spec, sample_markdown, sample_scala, simulate_failures, src_dir, temp_output_dir, test_config, test_data_dir, test_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py:168
_ ERROR at setup of TestResourceConstraintScenarios.test_large_gnn_file_handling _
file /Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py, line 196
      @pytest.mark.unit
      @pytest.mark.safe_to_fail
      def test_large_gnn_file_handling(self, temp_directories):
E       fixture 'temp_directories' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, capture_logs, comprehensive_test_data, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, full_pipeline_environment, include_metadata_in_junit_xml, isolated_temp_dir, json_metadata, metadata, mock_dangerous_operations, mock_filesystem, mock_imports, mock_llm_provider, mock_logger, mock_mcp_tools, mock_render_module, mock_subprocess, monkeypatch, no_cover, pipeline_arguments, project_root, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, safe_filesystem, sample_gnn_file, sample_gnn_files, sample_gnn_spec, sample_markdown, sample_scala, simulate_failures, src_dir, temp_output_dir, test_config, test_data_dir, test_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py:196
_ ERROR at setup of TestResourceConstraintScenarios.test_concurrent_pipeline_execution _
file /Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py, line 233
      @pytest.mark.unit
      @pytest.mark.safe_to_fail
      def test_concurrent_pipeline_execution(self, temp_directories):
E       fixture 'temp_directories' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, capture_logs, comprehensive_test_data, cov, doctest_namespace, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, full_pipeline_environment, include_metadata_in_junit_xml, isolated_temp_dir, json_metadata, metadata, mock_dangerous_operations, mock_filesystem, mock_imports, mock_llm_provider, mock_logger, mock_mcp_tools, mock_render_module, mock_subprocess, monkeypatch, no_cover, pipeline_arguments, project_root, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, safe_filesystem, sample_gnn_file, sample_gnn_files, sample_gnn_spec, sample_markdown, sample_scala, simulate_failures, src_dir, temp_output_dir, test_config, test_data_dir, test_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, worker_id
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/4d/Documents/GitHub/generalizednotationnotation/src/tests/test_pipeline_error_scenarios.py:233
=================================== FAILURES ===================================
___________________________ test_ollama_simple_chat ____________________________
src/tests/test_llm_ollama.py:82: in test_ollama_simple_chat
    assert len(result.content) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len('')
E    +    where '' = LLMResponse(content='', model_used='smollm2:135m-instruct-q4_K_S', provider='ollama', usage=None, finish_reason=None, metadata={'raw': {'model': 'smollm2:135m-instruct-q4_K_S'}}).content
----------------------------- Captured stderr call -----------------------------
2025-10-09 07:54:14,285 - llm.providers.ollama_provider - INFO - Ollama provider initialized (CLI fallback)
------------------------------ Captured log call -------------------------------
INFO     llm.providers.ollama_provider:ollama_provider.py:86 Ollama provider initialized (CLI fallback)
____________________________ test_ollama_streaming _____________________________
src/tests/test_llm_ollama.py:114: in test_ollama_streaming
    assert len(text) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len('')
----------------------------- Captured stderr call -----------------------------
2025-10-09 07:54:14,319 - llm.providers.ollama_provider - INFO - Ollama provider initialized (CLI fallback)
------------------------------ Captured log call -------------------------------
INFO     llm.providers.ollama_provider:ollama_provider.py:86 Ollama provider initialized (CLI fallback)
___________________ test_processor_uses_ollama_when_no_keys ____________________
src/tests/test_llm_ollama.py:153: in test_processor_uses_ollama_when_no_keys
    assert len(result.content) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len('')
E    +    where '' = LLMResponse(content='', model_used='smollm2:135m-instruct-q4_K_S', provider='ollama', usage=None, finish_reason=None, metadata={'raw': {'model': 'smollm2:135m-instruct-q4_K_S'}}).content
----------------------------- Captured stderr call -----------------------------
2025-10-09 07:54:14,338 - llm.providers.openai_provider - WARNING - OpenAI API key not provided
2025-10-09 07:54:14,338 - llm.llm_processor - WARNING - Failed to initialize openai provider
2025-10-09 07:54:14,338 - llm.providers.openrouter_provider - WARNING - OpenRouter API key not provided
2025-10-09 07:54:14,338 - llm.llm_processor - WARNING - Failed to initialize openrouter provider
2025-10-09 07:54:14,339 - llm.providers.perplexity_provider - WARNING - Perplexity API key not provided
2025-10-09 07:54:14,339 - llm.llm_processor - WARNING - Failed to initialize perplexity provider
2025-10-09 07:54:14,339 - llm.providers.ollama_provider - INFO - Ollama provider initialized (CLI fallback)
2025-10-09 07:54:14,339 - llm.llm_processor - INFO - Initialized ollama provider
2025-10-09 07:54:14,339 - llm.llm_processor - INFO - LLM Processor initialized with 1 providers
------------------------------ Captured log call -------------------------------
WARNING  llm.providers.openai_provider:openai_provider.py:76 OpenAI API key not provided
WARNING  llm.llm_processor:llm_processor.py:203 Failed to initialize openai provider
WARNING  llm.providers.openrouter_provider:openrouter_provider.py:116 OpenRouter API key not provided
WARNING  llm.llm_processor:llm_processor.py:203 Failed to initialize openrouter provider
WARNING  llm.providers.perplexity_provider:perplexity_provider.py:90 Perplexity API key not provided
WARNING  llm.llm_processor:llm_processor.py:203 Failed to initialize perplexity provider
INFO     llm.providers.ollama_provider:ollama_provider.py:86 Ollama provider initialized (CLI fallback)
INFO     llm.llm_processor:llm_processor.py:201 Initialized ollama provider
INFO     llm.llm_processor:llm_processor.py:211 LLM Processor initialized with 1 providers
______________ TestOllamaDetection.test_ollama_detection_logging _______________
src/tests/test_llm_ollama_integration.py:63: in test_ollama_detection_logging
    assert (
E   AssertionError: assert ('not found' in 'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n' or 'not running' in 'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n' or 'not available' in 'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n')
E    +  where 'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n' = <built-in method lower of str object at 0x95ea8ce00>()
E    +    where <built-in method lower of str object at 0x95ea8ce00> = 'INFO     test_ollama:processor.py:43 üîç Found Ollama at: /opt/homebrew/bin/ollama\nINFO     test_ollama:processor.py:8...nyllama\nINFO     test_ollama:processor.py:109 ‚ÑπÔ∏è LLM analysis will use fallback mode without live model interaction\n'.lower
E    +  and   'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n' = <built-in method lower of str object at 0x95ea8ce00>()
E    +    where <built-in method lower of str object at 0x95ea8ce00> = 'INFO     test_ollama:processor.py:43 üîç Found Ollama at: /opt/homebrew/bin/ollama\nINFO     test_ollama:processor.py:8...nyllama\nINFO     test_ollama:processor.py:109 ‚ÑπÔ∏è LLM analysis will use fallback mode without live model interaction\n'.lower
E    +  and   'info     test_ollama:processor.py:43 üîç found ollama at: /opt/homebrew/bin/ollama\ninfo     test_ollama:processor.py:8...nyllama\ninfo     test_ollama:processor.py:109 ‚ÑπÔ∏è llm analysis will use fallback mode without live model interaction\n' = <built-in method lower of str object at 0x95ea8ce00>()
E    +    where <built-in method lower of str object at 0x95ea8ce00> = 'INFO     test_ollama:processor.py:43 üîç Found Ollama at: /opt/homebrew/bin/ollama\nINFO     test_ollama:processor.py:8...nyllama\nINFO     test_ollama:processor.py:109 ‚ÑπÔ∏è LLM analysis will use fallback mode without live model interaction\n'.lower
----------------------------- Captured stderr call -----------------------------
2025-10-09 07:54:14,389 - test_ollama - INFO - üîç Found Ollama at: /opt/homebrew/bin/ollama
2025-10-09 07:54:14,403 - test_ollama - INFO - üîÑ Attempting to check Ollama serve status via API...
2025-10-09 07:54:14,403 - test_ollama - INFO - ‚ÑπÔ∏è Ollama server not responding on localhost:11434
2025-10-09 07:54:14,403 - test_ollama - WARNING - ‚ö†Ô∏è Ollama is installed but may not be running
2025-10-09 07:54:14,403 - test_ollama - INFO - üìù To start Ollama, run in a separate terminal:
2025-10-09 07:54:14,403 - test_ollama - INFO -    $ ollama serve
2025-10-09 07:54:14,403 - test_ollama - INFO - üìù To install a lightweight model for testing:
2025-10-09 07:54:14,403 - test_ollama - INFO -    $ ollama pull smollm2:135m
2025-10-09 07:54:14,404 - test_ollama - INFO -    $ ollama pull tinyllama
2025-10-09 07:54:14,404 - test_ollama - INFO - ‚ÑπÔ∏è LLM analysis will use fallback mode without live model interaction
------------------------------ Captured log call -------------------------------
INFO     test_ollama:processor.py:43 üîç Found Ollama at: /opt/homebrew/bin/ollama
INFO     test_ollama:processor.py:84 üîÑ Attempting to check Ollama serve status via API...
INFO     test_ollama:processor.py:98 ‚ÑπÔ∏è Ollama server not responding on localhost:11434
WARNING  test_ollama:processor.py:103 ‚ö†Ô∏è Ollama is installed but may not be running
INFO     test_ollama:processor.py:104 üìù To start Ollama, run in a separate terminal:
INFO     test_ollama:processor.py:105    $ ollama serve
INFO     test_ollama:processor.py:106 üìù To install a lightweight model for testing:
INFO     test_ollama:processor.py:107    $ ollama pull smollm2:135m
INFO     test_ollama:processor.py:108    $ ollama pull tinyllama
INFO     test_ollama:processor.py:109 ‚ÑπÔ∏è LLM analysis will use fallback mode without live model interaction
_____ TestDependencyErrorScenarios.test_missing_pymdp_graceful_degradation _____
src/tests/test_pipeline_error_scenarios.py:47: in test_missing_pymdp_graceful_degradation
    simulator.create_model({})
    ^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PyMDPSimulation' object has no attribute 'create_model'
----------------------------- Captured stderr call -----------------------------
2025-10-09 07:55:48,846 - execute.pymdp.pymdp_simulation - INFO - Initialized with default gridworld configuration
2025-10-09 07:55:48,847 - execute.pymdp.pymdp_simulation - INFO - Created PyMDP model: 4S, 5A, 4O
------------------------------ Captured log call -------------------------------
INFO     execute.pymdp.pymdp_simulation:pymdp_simulation.py:249 Initialized with default gridworld configuration
INFO     execute.pymdp.pymdp_simulation:pymdp_simulation.py:564 Created PyMDP model: 4S, 5A, 4O
============================= slowest 10 durations =============================
64.43s call     src/tests/test_llm_ollama_integration.py::TestLLMProcessing::test_llm_processing_without_ollama
10.29s call     src/tests/test_llm_ollama_integration.py::TestLLMProcessing::test_llm_processing_creates_outputs
7.96s call     src/tests/test_llm_ollama_integration.py::TestLLMProcessing::test_llm_processing_with_ollama
7.62s call     src/tests/test_llm_ollama_integration.py::TestLLMProcessing::test_llm_processing_model_selection
3.52s call     src/tests/test_main_orchestrator.py::TestEndToEndIntegration::test_run_pipeline_subset
0.87s call     src/tests/test_core_modules.py::TestLLMModuleComprehensive::test_llm_model_analysis
0.27s call     src/tests/test_main_orchestrator.py::TestPipelineCoordination::test_minimal_pipeline_execution
0.21s call     src/tests/test_environment_overall.py::TestEnvironmentFunctionality::test_dependency_installation
0.14s call     src/tests/test_pipeline_error_scenarios.py::TestDependencyErrorScenarios::test_matplotlib_rendering_fallbacks
0.13s call     src/tests/test_environment_overall.py::TestEnvironmentIntegration::test_environment_pipeline_integration
=========================== short test summary info ============================
FAILED src/tests/test_llm_ollama.py::test_ollama_simple_chat - AssertionError...
FAILED src/tests/test_llm_ollama.py::test_ollama_streaming - AssertionError: ...
FAILED src/tests/test_llm_ollama.py::test_processor_uses_ollama_when_no_keys
FAILED src/tests/test_llm_ollama_integration.py::TestOllamaDetection::test_ollama_detection_logging
FAILED src/tests/test_pipeline_error_scenarios.py::TestDependencyErrorScenarios::test_missing_pymdp_graceful_degradation
ERROR src/tests/test_pipeline_error_scenarios.py::TestFileOperationErrorScenarios::test_missing_input_directory
ERROR src/tests/test_pipeline_error_scenarios.py::TestFileOperationErrorScenarios::test_readonly_output_directory
ERROR src/tests/test_pipeline_error_scenarios.py::TestFileOperationErrorScenarios::test_corrupted_gnn_file_handling
ERROR src/tests/test_pipeline_error_scenarios.py::TestResourceConstraintScenarios::test_large_gnn_file_handling
ERROR src/tests/test_pipeline_error_scenarios.py::TestResourceConstraintScenarios::test_concurrent_pipeline_execution
!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 10 failures !!!!!!!!!!!!!!!!!!!!!!!!!!
5 failed, 275 passed, 28 skipped, 5 errors in 97.75s (0:01:37)


STDERR:
