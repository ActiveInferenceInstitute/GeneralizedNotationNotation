{
  "model_name": "Classic Active Inference POMDP Agent v1",
  "version": "1.0",
  "annotation": "This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.",
  "variables": [
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "a1c7f7be-7243-4a9d-8870-f58454119a69",
      "name": "A",
      "var_type": "likelihood_matrix",
      "dimensions": [
        3,
        3
      ],
      "data_type": "float",
      "description": "Likelihood mapping hidden states to observations",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "33f0001e-6053-4d18-9e46-62a5f99c9b07",
      "name": "B",
      "var_type": "transition_matrix",
      "dimensions": [
        3,
        3,
        3
      ],
      "data_type": "float",
      "description": "State transitions given previous state and action",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "db819fc4-0898-475e-b8fe-64aa1d9bc457",
      "name": "C",
      "var_type": "preference_vector",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Log-preferences over observations",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "eadb7be3-e5a1-46b8-b5ef-63cad75118d3",
      "name": "D",
      "var_type": "prior_vector",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Prior over initial hidden states",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "65c3e6a4-a1b7-4caa-bab0-2ea7ca4f0126",
      "name": "E",
      "var_type": "policy",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Initial policy prior (habit) over actions",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "9e6ca553-f3e2-45d3-8893-a465adde58c2",
      "name": "s",
      "var_type": "hidden_state",
      "dimensions": [
        3,
        1
      ],
      "data_type": "float",
      "description": "Current hidden state distribution",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "c3f2d6cf-fe5f-4530-bea3-9115c214bfc4",
      "name": "s_prime",
      "var_type": "hidden_state",
      "dimensions": [
        3,
        1
      ],
      "data_type": "float",
      "description": "Next hidden state distribution",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "78262bce-4b4e-49ac-ab5e-91fce3c9091c",
      "name": "o",
      "var_type": "observation",
      "dimensions": [
        3,
        1
      ],
      "data_type": "integer",
      "description": "Current observation (integer index)",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "bfa98287-f12e-4c27-982a-b8152f94df15",
      "name": "\u03c0",
      "var_type": "policy",
      "dimensions": [
        3
      ],
      "data_type": "float",
      "description": "Policy (distribution over actions), no planning",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "c99a56fe-8b8a-4b92-a31b-b3b2d2eab772",
      "name": "u",
      "var_type": "action",
      "dimensions": [
        1
      ],
      "data_type": "integer",
      "description": "Action taken",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "0a022505-fdd1-41f2-b6a6-c39aa5f0519b",
      "name": "G",
      "var_type": "policy",
      "dimensions": [
        1
      ],
      "data_type": "float",
      "description": "Expected Free Energy (per policy)",
      "constraints": {}
    },
    {
      "node_type": "Variable",
      "source_location": null,
      "metadata": {},
      "id": "7ebdb623-3ff5-43bf-a292-f6860e8057e8",
      "name": "t",
      "var_type": "hidden_state",
      "dimensions": [
        1
      ],
      "data_type": "integer",
      "description": "Discrete time step",
      "constraints": {}
    }
  ],
  "connections": [
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "22d39d79-bf7e-4ef8-b735-5ed0323dae9a",
      "source_variables": [
        "D"
      ],
      "target_variables": [
        "s"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "8cca3132-9e54-47db-abdc-420a21b02edb",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "A"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "d5307f92-6f4f-41f5-834a-2dd133112e33",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "s_prime"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "f75fd85d-06a6-4367-8821-fcb0ac868e8d",
      "source_variables": [
        "A"
      ],
      "target_variables": [
        "o"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "7f2f4689-3fb3-4036-9788-15d901159730",
      "source_variables": [
        "s"
      ],
      "target_variables": [
        "B"
      ],
      "connection_type": "undirected",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "07ccff6f-f487-48ad-9cef-4231b2b028e7",
      "source_variables": [
        "C"
      ],
      "target_variables": [
        "G"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "3be2a3db-9129-4b6e-bd29-2bde77712240",
      "source_variables": [
        "E"
      ],
      "target_variables": [
        "\u03c0"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "488a0e22-b7e5-41df-9dd6-ef0c2ef70b68",
      "source_variables": [
        "G"
      ],
      "target_variables": [
        "\u03c0"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "1404d0c2-3c20-4b1d-b53b-b11e27f28339",
      "source_variables": [
        "\u03c0"
      ],
      "target_variables": [
        "u"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "c0ecec06-a19e-449a-a09f-cb756991149c",
      "source_variables": [
        "B"
      ],
      "target_variables": [
        "u"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    },
    {
      "node_type": "Connection",
      "source_location": null,
      "metadata": {},
      "id": "aca9b19a-e06b-469f-bed2-560ff94a19d8",
      "source_variables": [
        "u"
      ],
      "target_variables": [
        "s_prime"
      ],
      "connection_type": "directed",
      "weight": null,
      "description": null
    }
  ],
  "parameters": [
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "0ad97f6f-00f1-4e2d-91cd-9a08ea698d6f",
      "name": "A",
      "value": [
        [
          0.9,
          0.05,
          0.05
        ],
        [
          0.05,
          0.9,
          0.05
        ],
        [
          0.05,
          0.05,
          0.9
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "4cc6ee24-e20a-45d2-9fa9-9d9769fbf166",
      "name": "B",
      "value": [
        [
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ]
        ],
        [
          [
            0.0,
            1.0,
            0.0
          ],
          [
            1.0,
            0.0,
            0.0
          ],
          [
            0.0,
            0.0,
            1.0
          ]
        ],
        [
          [
            0.0,
            0.0,
            1.0
          ],
          [
            0.0,
            1.0,
            0.0
          ],
          [
            1.0,
            0.0,
            0.0
          ]
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "00150f33-3b79-41e7-ab66-cd7386fa290a",
      "name": "C",
      "value": [
        [
          0.1,
          0.1,
          1.0
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "cb5e08c1-43b5-4158-8e8e-aafac48e3353",
      "name": "D",
      "value": [
        [
          0.33333,
          0.33333,
          0.33333
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "eb3e60e2-60ba-45df-b398-fa29711ff084",
      "name": "E",
      "value": [
        [
          0.33333,
          0.33333,
          0.33333
        ]
      ],
      "type_hint": null,
      "description": null
    },
    {
      "node_type": "Parameter",
      "source_location": null,
      "metadata": {},
      "id": "ead2d361-161c-4049-83c2-8230383963e3",
      "name": "num_actions: 3       # B actions_dim",
      "value": "3 (controlled by \u03c0)",
      "type_hint": null,
      "description": null
    }
  ],
  "equations": [],
  "time_specification": {
    "node_type": "TimeSpecification",
    "source_location": null,
    "metadata": {},
    "id": "9f082c37-d2ca-41df-812c-082bba9d08d5",
    "time_type": "Dynamic",
    "discretization": null,
    "horizon": "Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.",
    "step_size": null
  },
  "ontology_mappings": [
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "390377e4-0bf2-43a0-80bf-f25b96d47854",
      "variable_name": "A",
      "ontology_term": "LikelihoodMatrix",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "0ee95530-1152-49b0-bcec-5744a95dcf67",
      "variable_name": "B",
      "ontology_term": "TransitionMatrix",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "999f2adc-fe98-4a3d-9e42-020e839bea05",
      "variable_name": "C",
      "ontology_term": "LogPreferenceVector",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "9523b3f8-aeac-47cf-a6b9-9b9d8c0bf93b",
      "variable_name": "D",
      "ontology_term": "PriorOverHiddenStates",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "9617ba7e-4a24-4892-aaf5-f84d76e59e30",
      "variable_name": "E",
      "ontology_term": "Habit",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "e7b76de6-7bc2-4ca3-903c-173faca5ff29",
      "variable_name": "F",
      "ontology_term": "VariationalFreeEnergy",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "26139f51-d7a4-42fe-81a1-73f616c0eb2f",
      "variable_name": "G",
      "ontology_term": "ExpectedFreeEnergy",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "0a99537f-c8d3-4f66-96f9-8e60be719897",
      "variable_name": "s",
      "ontology_term": "HiddenState",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "cb58c6fe-18e9-4a33-96d0-e94e5a948328",
      "variable_name": "s_prime",
      "ontology_term": "NextHiddenState",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "2653021f-de74-4cc9-8685-c5ae9c9aa190",
      "variable_name": "o",
      "ontology_term": "Observation",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "4c1308c7-79bc-488b-89a7-ef78081f3db8",
      "variable_name": "\u03c0",
      "ontology_term": "PolicyVector # Distribution over actions",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "d79ede16-416b-417d-9f0b-ecc27eb420fc",
      "variable_name": "u",
      "ontology_term": "Action       # Chosen action",
      "description": null
    },
    {
      "node_type": "OntologyMapping",
      "source_location": null,
      "metadata": {},
      "id": "c111352c-3b88-49c2-ada3-0d7a94128687",
      "variable_name": "t",
      "ontology_term": "Time",
      "description": null
    }
  ],
  "source_format": null,
  "created_at": "2025-08-20T07:16:59.545007",
  "modified_at": "2025-08-20T07:16:59.545008",
  "checksum": null,
  "extensions": {
    "gnn_section": "ActInfPOMDP",
    "footer": "Active Inference POMDP Agent v1 - GNN Representation. \nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting.",
    "signature": "Cryptographic signature goes here"
  },
  "raw_sections": {
    "GNNSection": "ActInfPOMDP",
    "GNNVersionAndFlags": "GNN v1",
    "ModelName": "Classic Active Inference POMDP Agent v1",
    "ModelAnnotation": "This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.",
    "StateSpaceBlock": "# Likelihood matrix: A[observation_outcomes, hidden_states]\nA[3,3,type=float]   # Likelihood mapping hidden states to observations\n\n# Transition matrix: B[states_next, states_previous, actions]\nB[3,3,3,type=float]   # State transitions given previous state and action\n\n# Preference vector: C[observation_outcomes]\nC[3,type=float]       # Log-preferences over observations\n\n# Prior vector: D[states]\nD[3,type=float]       # Prior over initial hidden states\n\n# Habit vector: E[actions]\nE[3,type=float]       # Initial policy prior (habit) over actions\n\n# Hidden State\ns[3,1,type=float]     # Current hidden state distribution\ns_prime[3,1,type=float] # Next hidden state distribution\n\n# Observation\no[3,1,type=int]     # Current observation (integer index)\n\n# Policy and Control\n\u03c0[3,type=float]       # Policy (distribution over actions), no planning\nu[1,type=int]         # Action taken\nG[\u03c0,type=float]       # Expected Free Energy (per policy)\n\n# Time\nt[1,type=int]         # Discrete time step",
    "Connections": "D>s\ns-A\ns>s_prime\nA-o\ns-B\nC>G\nE>\u03c0\nG>\u03c0\n\u03c0>u\nB>u\nu>s_prime",
    "InitialParameterization": "# A: 3 observations x 3 hidden states. Identity mapping (each state deterministically produces a unique observation). Rows are observations, columns are hidden states.\nA={\n  (0.9, 0.05, 0.05),\n  (0.05, 0.9, 0.05),\n  (0.05, 0.05, 0.9)\n}\n\n# B: 3 states x 3 previous states x 3 actions. Each action deterministically moves to a state. For each slice, rows are previous states, columns are next states. Each slice is a transition matrix corresponding to a different action selection.\nB={\n  ( (1.0,0.0,0.0), (0.0,1.0,0.0), (0.0,0.0,1.0) ),\n  ( (0.0,1.0,0.0), (1.0,0.0,0.0), (0.0,0.0,1.0) ),\n  ( (0.0,0.0,1.0), (0.0,1.0,0.0), (1.0,0.0,0.0) )\n}\n\n# C: 3 observations. Preference in terms of log-probabilities over observations.\nC={(0.1, 0.1, 1.0)}\n\n# D: 3 states. Uniform prior over hidden states. Rows are hidden states, columns are prior probabilities.\nD={(0.33333, 0.33333, 0.33333)}\n\n# E: 3 actions. Uniform habit used as initial policy prior.\nE={(0.33333, 0.33333, 0.33333)}",
    "Equations": "# Standard Active Inference update equations for POMDPs:\n# - State inference using Variational Free Energy with infer_states()\n# - Policy inference using Expected Free Energy = with infer_policies()\n# - Action selection from policy posterior: action = sample_action()",
    "Time": "Time=t\nDynamic\nDiscrete\nModelTimeHorizon=Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.",
    "ActInfOntologyAnnotation": "A=LikelihoodMatrix\nB=TransitionMatrix\nC=LogPreferenceVector\nD=PriorOverHiddenStates\nE=Habit\nF=VariationalFreeEnergy\nG=ExpectedFreeEnergy\ns=HiddenState\ns_prime=NextHiddenState\no=Observation\n\u03c0=PolicyVector # Distribution over actions\nu=Action       # Chosen action\nt=Time",
    "ModelParameters": "num_hidden_states: 3  # s[3]\nnum_obs: 3           # o[3]\nnum_actions: 3       # B actions_dim=3 (controlled by \u03c0)",
    "Footer": "Active Inference POMDP Agent v1 - GNN Representation. \nCurrently there is a planning horizon of 1 step (no deep planning), no precision modulation, no hierarchical nesting.",
    "Signature": "Cryptographic signature goes here"
  }
}