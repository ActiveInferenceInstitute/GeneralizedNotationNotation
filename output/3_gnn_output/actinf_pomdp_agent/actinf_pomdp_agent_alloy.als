module Active_Inference_POMDP_Agent

// GNN Model: Active Inference POMDP Agent
// This model describes a classic Active Inference agent for a discrete POMDP:
// - One observation modality ("state_observation") with 3 possible outcomes.
// - One hidden state factor ("location") with 3 possible states.
// - The hidden state is fully controllable via 3 discrete actions.
// - The agent's preferences are encoded as log-probabilities over observations.
// - The agent has an initial policy prior (habit) encoded as log-probabilities over actions.
// Generated by AlloySerializer

// Variable signatures
sig A {
  value: Int,
  dimensions: seq Int  // [3, 3]
}

sig B {
  value: Int,
  dimensions: seq Int  // [3, 3, 3]
}

sig C {
  value: Int,
  dimensions: seq Int  // [3]
}

sig D {
  value: Int,
  dimensions: seq Int  // [3]
}

sig E {
  value: Int,
  dimensions: seq Int  // [3]
}

sig F {
  value: Int,
  dimensions: seq Int  // [1]
}

sig G {
  value: Int,
  dimensions: seq Int  // [1]
}

sig o {
  value: Int,
  dimensions: seq Int  // [3, 1]
}

sig s {
  value: Int,
  dimensions: seq Int  // [3, 1]
}

sig s_prime {
  value: Int,
  dimensions: seq Int  // [3, 1]
}

sig t {
  value: Int,
  dimensions: seq Int  // [1]
}

sig u {
  value: Int,
  dimensions: seq Int  // [1]
}

sig V_ {
  value: Int,
  dimensions: seq Int  // [3]
}

// Connection constraints
fact Connections {
  // Connection 1: D -> s
  some D and some s
  // Connection 2: s -> A
  some s and some A
  // Connection 3: s -> s_prime
  some s and some s_prime
  // Connection 4: A -> o
  some A and some o
  // Connection 5: s -> B
  some s and some B
  // Connection 6: C -> G
  some C and some G
  // Connection 7: E -> π
  some E and some V_
  // Connection 8: G -> π
  some G and some V_
  // Connection 9: π -> u
  some V_ and some u
  // Connection 10: B -> u
  some B and some u
  // Connection 11: u -> s_prime
  some u and some s_prime
}

// Model validity predicate
pred Active_Inference_POMDP_AgentValid {
  // All variables must exist
  some A
  some B
  some C
  some D
  some E
  some s
  some s_prime
  some F
  some o
  some V_
  some u
  some G
  some t
}

// Parameter constraints
fact Parameters {
  // Parameter: A = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
  // Parameter: B = [[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]]
  // Parameter: C = [[0.1, 0.1, 1.0]]
  // Parameter: D = [[0.33333, 0.33333, 0.33333]]
  // Parameter: E = [[0.33333, 0.33333, 0.33333]]
  // Parameter: num_actions: 3       # B actions_dim = 3 (controlled by π)
}


/* MODEL_DATA: {"model_name":"Active Inference POMDP Agent","version":"1.0","annotation":"This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.","variables":[{"name":"A","var_type":"likelihood_matrix","data_type":"float","dimensions":[3,3],"description":"Likelihood mapping hidden states to observations"},{"name":"B","var_type":"transition_matrix","data_type":"float","dimensions":[3,3,3],"description":"State transitions given previous state and action"},{"name":"C","var_type":"preference_vector","data_type":"float","dimensions":[3],"description":"Log-preferences over observations"},{"name":"D","var_type":"prior_vector","data_type":"float","dimensions":[3],"description":"Prior over initial hidden states"},{"name":"E","var_type":"policy","data_type":"float","dimensions":[3],"description":"Initial policy prior (habit) over actions"},{"name":"s","var_type":"hidden_state","data_type":"float","dimensions":[3,1],"description":"Current hidden state distribution"},{"name":"s_prime","var_type":"hidden_state","data_type":"float","dimensions":[3,1],"description":"Next hidden state distribution"},{"name":"F","var_type":"hidden_state","data_type":"float","dimensions":[1],"description":"Variational Free Energy for belief updating from observations"},{"name":"o","var_type":"observation","data_type":"integer","dimensions":[3,1],"description":"Current observation (integer index)"},{"name":"\u03c0","var_type":"policy","data_type":"float","dimensions":[3],"description":"Policy (distribution over actions), no planning"},{"name":"u","var_type":"action","data_type":"integer","dimensions":[1],"description":"Action taken"},{"name":"G","var_type":"policy","data_type":"float","dimensions":[1],"description":"Expected Free Energy (per policy)"},{"name":"t","var_type":"hidden_state","data_type":"integer","dimensions":[1],"description":"Discrete time step"}],"connections":[{"source_variables":["D"],"target_variables":["s"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["s"],"target_variables":["A"],"connection_type":"undirected","weight":null,"description":null},{"source_variables":["s"],"target_variables":["s_prime"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["A"],"target_variables":["o"],"connection_type":"undirected","weight":null,"description":null},{"source_variables":["s"],"target_variables":["B"],"connection_type":"undirected","weight":null,"description":null},{"source_variables":["C"],"target_variables":["G"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["E"],"target_variables":["\u03c0"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["G"],"target_variables":["\u03c0"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["\u03c0"],"target_variables":["u"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["B"],"target_variables":["u"],"connection_type":"directed","weight":null,"description":null},{"source_variables":["u"],"target_variables":["s_prime"],"connection_type":"directed","weight":null,"description":null}],"parameters":[{"name":"A","value":[[0.9,0.05,0.05],[0.05,0.9,0.05],[0.05,0.05,0.9]],"type_hint":null,"description":null},{"name":"B","value":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[0.0,0.0,1.0],[0.0,1.0,0.0],[1.0,0.0,0.0]]],"type_hint":null,"description":null},{"name":"C","value":[[0.1,0.1,1.0]],"type_hint":null,"description":null},{"name":"D","value":[[0.33333,0.33333,0.33333]],"type_hint":null,"description":null},{"name":"E","value":[[0.33333,0.33333,0.33333]],"type_hint":null,"description":null},{"name":"num_actions: 3       # B actions_dim","value":"3 (controlled by \u03c0)","type_hint":null,"description":null}],"equations":[],"time_specification":{"time_type":"Dynamic","discretization":null,"horizon":"Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.","step_size":null},"ontology_mappings":[{"variable_name":"A","ontology_term":"LikelihoodMatrix","description":null},{"variable_name":"B","ontology_term":"TransitionMatrix","description":null},{"variable_name":"C","ontology_term":"LogPreferenceVector","description":null},{"variable_name":"D","ontology_term":"PriorOverHiddenStates","description":null},{"variable_name":"E","ontology_term":"Habit","description":null},{"variable_name":"F","ontology_term":"VariationalFreeEnergy","description":null},{"variable_name":"G","ontology_term":"ExpectedFreeEnergy","description":null},{"variable_name":"s","ontology_term":"HiddenState","description":null},{"variable_name":"s_prime","ontology_term":"NextHiddenState","description":null},{"variable_name":"o","ontology_term":"Observation","description":null},{"variable_name":"\u03c0","ontology_term":"PolicyVector # Distribution over actions","description":null},{"variable_name":"u","ontology_term":"Action       # Chosen action","description":null},{"variable_name":"t","ontology_term":"Time","description":null}]} */
