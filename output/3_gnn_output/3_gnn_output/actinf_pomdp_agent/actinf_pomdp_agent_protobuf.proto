syntax = "proto3";

package classic_active_inference_pomdp_agent_v1;

// GNN Model: Classic Active Inference POMDP Agent v1
// Annotation: This model describes a classic Active Inference agent for a discrete POMDP:
- One observation modality ("state_observation") with 3 possible outcomes.
- One hidden state factor ("location") with 3 possible states.
- The hidden state is fully controllable via 3 discrete actions.
- The agent's preferences are encoded as log-probabilities over observations.
- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.
// Generated by GNN Protobuf Serializer

message Variable {{
  string name = 1;
  string var_type = 2;
  string data_type = 3;
  repeated int32 dimensions = 4;
  string description = 5;
}}

message Connection {{
  repeated string source_variables = 1;
  repeated string target_variables = 2;
  string connection_type = 3;
  string description = 4;
}}

message Parameter {{
  string name = 1;
  string value = 2;
  string param_type = 3;
}}

message Equation {{
  string equation = 1;
  string type = 2;
}}

message TimeSpecification {{
  string time_type = 1;
  int32 steps = 2;
  string description = 3;
}}

message OntologyMapping {{
  string variable_name = 1;
  string ontology_term = 2;
}}

message GNNModel {{
  string name = 1;
  string annotation = 2;
  repeated Variable variables = 3;
  repeated Connection connections = 4;
  repeated Parameter parameters = 5;
  repeated Equation equations = 6;
  TimeSpecification time_specification = 7;
  repeated OntologyMapping ontology_mappings = 8;
}}

/* MODEL_DATA: {"model_name":"Classic Active Inference POMDP Agent v1","annotation":"This model describes a classic Active Inference agent for a discrete POMDP:\n- One observation modality (\"state_observation\") with 3 possible outcomes.\n- One hidden state factor (\"location\") with 3 possible states.\n- The hidden state is fully controllable via 3 discrete actions.\n- The agent's preferences are encoded as log-probabilities over observations.\n- The agent has an initial policy prior (habit) encoded as log-probabilities over actions.","variables":[{"name":"A","var_type":"likelihood_matrix","data_type":"float","dimensions":[3,3]},{"name":"B","var_type":"transition_matrix","data_type":"float","dimensions":[3,3,3]},{"name":"C","var_type":"preference_vector","data_type":"float","dimensions":[3]},{"name":"D","var_type":"prior_vector","data_type":"float","dimensions":[3]},{"name":"E","var_type":"policy","data_type":"float","dimensions":[3]},{"name":"s","var_type":"hidden_state","data_type":"float","dimensions":[3,1]},{"name":"s_prime","var_type":"hidden_state","data_type":"float","dimensions":[3,1]},{"name":"F","var_type":"hidden_state","data_type":"float","dimensions":[1]},{"name":"o","var_type":"observation","data_type":"integer","dimensions":[3,1]},{"name":"\u03c0","var_type":"policy","data_type":"float","dimensions":[3]},{"name":"u","var_type":"action","data_type":"integer","dimensions":[1]},{"name":"G","var_type":"policy","data_type":"float","dimensions":[1]},{"name":"t","var_type":"hidden_state","data_type":"integer","dimensions":[1]}],"connections":[{"source_variables":["D"],"target_variables":["s"],"connection_type":"directed"},{"source_variables":["s"],"target_variables":["A"],"connection_type":"undirected"},{"source_variables":["s"],"target_variables":["s_prime"],"connection_type":"directed"},{"source_variables":["A"],"target_variables":["o"],"connection_type":"undirected"},{"source_variables":["s"],"target_variables":["B"],"connection_type":"undirected"},{"source_variables":["C"],"target_variables":["G"],"connection_type":"directed"},{"source_variables":["E"],"target_variables":["\u03c0"],"connection_type":"directed"},{"source_variables":["G"],"target_variables":["\u03c0"],"connection_type":"directed"},{"source_variables":["\u03c0"],"target_variables":["u"],"connection_type":"directed"},{"source_variables":["B"],"target_variables":["u"],"connection_type":"directed"},{"source_variables":["u"],"target_variables":["s_prime"],"connection_type":"directed"}],"parameters":[{"name":"A","value":[[0.9,0.05,0.05],[0.05,0.9,0.05],[0.05,0.05,0.9]],"param_type":"constant"},{"name":"B","value":[[[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]],[[0.0,1.0,0.0],[1.0,0.0,0.0],[0.0,0.0,1.0]],[[0.0,0.0,1.0],[0.0,1.0,0.0],[1.0,0.0,0.0]]],"param_type":"constant"},{"name":"C","value":[[0.1,0.1,1.0]],"param_type":"constant"},{"name":"D","value":[[0.33333,0.33333,0.33333]],"param_type":"constant"},{"name":"E","value":[[0.33333,0.33333,0.33333]],"param_type":"constant"},{"name":"num_actions: 3       # B actions_dim","value":"3 (controlled by \u03c0)","param_type":"constant"}],"equations":[],"time_specification":{"time_type":"Dynamic","discretization":null,"horizon":"Unbounded # The agent is defined for an unbounded time horizon; simulation runs may specify a finite horizon.","step_size":null},"ontology_mappings":[{"variable_name":"A","ontology_term":"LikelihoodMatrix","description":null},{"variable_name":"B","ontology_term":"TransitionMatrix","description":null},{"variable_name":"C","ontology_term":"LogPreferenceVector","description":null},{"variable_name":"D","ontology_term":"PriorOverHiddenStates","description":null},{"variable_name":"E","ontology_term":"Habit","description":null},{"variable_name":"F","ontology_term":"VariationalFreeEnergy","description":null},{"variable_name":"G","ontology_term":"ExpectedFreeEnergy","description":null},{"variable_name":"s","ontology_term":"HiddenState","description":null},{"variable_name":"s_prime","ontology_term":"NextHiddenState","description":null},{"variable_name":"o","ontology_term":"Observation","description":null},{"variable_name":"\u03c0","ontology_term":"PolicyVector # Distribution over actions","description":null},{"variable_name":"u","ontology_term":"Action       # Chosen action","description":null},{"variable_name":"t","ontology_term":"Time","description":null}]} */

// Variables:
// Variable: A (likelihood_matrix)
// Variable: B (transition_matrix)
// Variable: C (preference_vector)
// Variable: D (prior_vector)
// Variable: E (policy)
// Variable: s (hidden_state)
// Variable: s_prime (hidden_state)
// Variable: F (hidden_state)
// Variable: o (observation)
// Variable: π (policy)
// Variable: u (action)
// Variable: G (policy)
// Variable: t (hidden_state)
// Connections:
// Connection: D --directed--> s
// Connection: s --undirected--> A
// Connection: s --directed--> s_prime
// Connection: A --undirected--> o
// Connection: s --undirected--> B
// Connection: C --directed--> G
// Connection: E --directed--> π
// Connection: G --directed--> π
// Connection: π --directed--> u
// Connection: B --directed--> u
// Connection: u --directed--> s_prime
// Parameters:
// Parameter: A = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
// Parameter: B = [[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]], [[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]]
// Parameter: C = [[0.1, 0.1, 1.0]]
// Parameter: D = [[0.33333, 0.33333, 0.33333]]
// Parameter: E = [[0.33333, 0.33333, 0.33333]]
// Parameter: num_actions: 3       # B actions_dim = 3 (controlled by π)