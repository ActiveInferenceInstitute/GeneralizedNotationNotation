annotation: 'This model describes a classic Active Inference agent for a discrete
  POMDP:

  - One observation modality ("state_observation") with 3 possible outcomes.

  - One hidden state factor ("location") with 3 possible states.

  - The hidden state is fully controllable via 3 discrete actions.

  - The agent''s preferences are encoded as log-probabilities over observations.

  - The agent has an initial policy prior (habit) encoded as log-probabilities over
  actions.'
connections:
- connection_type: undirected
  description: null
  source_variables:
  - A
  target_variables:
  - o
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - B
  target_variables:
  - u
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - C
  target_variables:
  - G
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - D
  target_variables:
  - s
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - E
  target_variables:
  - π
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - G
  target_variables:
  - π
  weight: null
- connection_type: undirected
  description: null
  source_variables:
  - s
  target_variables:
  - A
  weight: null
- connection_type: undirected
  description: null
  source_variables:
  - s
  target_variables:
  - B
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - s
  target_variables:
  - s_prime
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - u
  target_variables:
  - s_prime
  weight: null
- connection_type: directed
  description: null
  source_variables:
  - π
  target_variables:
  - u
  weight: null
created_at: '2025-08-20T18:20:12.231532'
equations: []
model_name: Classic Active Inference POMDP Agent v1
modified_at: '2025-08-20T18:20:12.231536'
ontology_mappings:
- description: null
  ontology_term: LikelihoodMatrix
  variable_name: A
- description: null
  ontology_term: TransitionMatrix
  variable_name: B
- description: null
  ontology_term: LogPreferenceVector
  variable_name: C
- description: null
  ontology_term: PriorOverHiddenStates
  variable_name: D
- description: null
  ontology_term: Habit
  variable_name: E
- description: null
  ontology_term: VariationalFreeEnergy
  variable_name: F
- description: null
  ontology_term: ExpectedFreeEnergy
  variable_name: G
- description: null
  ontology_term: Observation
  variable_name: o
- description: null
  ontology_term: HiddenState
  variable_name: s
- description: null
  ontology_term: NextHiddenState
  variable_name: s_prime
- description: null
  ontology_term: Time
  variable_name: t
- description: null
  ontology_term: 'Action       # Chosen action'
  variable_name: u
- description: null
  ontology_term: 'PolicyVector # Distribution over actions'
  variable_name: π
parameters:
- description: null
  name: A
  type_hint: null
  value:
  - - 0.9
    - 0.05
    - 0.05
  - - 0.05
    - 0.9
    - 0.05
  - - 0.05
    - 0.05
    - 0.9
- description: null
  name: B
  type_hint: null
  value:
  - - - 1.0
      - 0.0
      - 0.0
    - - 0.0
      - 1.0
      - 0.0
    - - 0.0
      - 0.0
      - 1.0
  - - - 0.0
      - 1.0
      - 0.0
    - - 1.0
      - 0.0
      - 0.0
    - - 0.0
      - 0.0
      - 1.0
  - - - 0.0
      - 0.0
      - 1.0
    - - 0.0
      - 1.0
      - 0.0
    - - 1.0
      - 0.0
      - 0.0
- description: null
  name: C
  type_hint: null
  value:
  - - 0.1
    - 0.1
    - 1.0
- description: null
  name: D
  type_hint: null
  value:
  - - 0.33333
    - 0.33333
    - 0.33333
- description: null
  name: E
  type_hint: null
  value:
  - - 0.33333
    - 0.33333
    - 0.33333
- description: null
  name: 'num_actions: 3       # B actions_dim'
  type_hint: null
  value: 3 (controlled by π)
time_specification:
  discretization: null
  horizon: 'Unbounded # The agent is defined for an unbounded time horizon; simulation
    runs may specify a finite horizon.'
  step_size: null
  time_type: Dynamic
variables:
- data_type: float
  description: Likelihood mapping hidden states to observations
  dimensions:
  - 3
  - 3
  name: A
  type: likelihood_matrix
- data_type: float
  description: State transitions given previous state and action
  dimensions:
  - 3
  - 3
  - 3
  name: B
  type: transition_matrix
- data_type: float
  description: Log-preferences over observations
  dimensions:
  - 3
  name: C
  type: preference_vector
- data_type: float
  description: Prior over initial hidden states
  dimensions:
  - 3
  name: D
  type: prior_vector
- data_type: float
  description: Initial policy prior (habit) over actions
  dimensions:
  - 3
  name: E
  type: policy
- data_type: float
  description: Expected Free Energy (per policy)
  dimensions:
  - 1
  name: G
  type: policy
- data_type: integer
  description: Current observation (integer index)
  dimensions:
  - 3
  - 1
  name: o
  type: observation
- data_type: float
  description: Current hidden state distribution
  dimensions:
  - 3
  - 1
  name: s
  type: hidden_state
- data_type: float
  description: Next hidden state distribution
  dimensions:
  - 3
  - 1
  name: s_prime
  type: hidden_state
- data_type: integer
  description: Discrete time step
  dimensions:
  - 1
  name: t
  type: hidden_state
- data_type: integer
  description: Action taken
  dimensions:
  - 1
  name: u
  type: action
- data_type: float
  description: Policy (distribution over actions), no planning
  dimensions:
  - 3
  name: π
  type: policy
version: '1.0'
